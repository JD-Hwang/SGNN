{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "200802_Nested_LOSOCV_HSP_SBM_08.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "G4xBPE8jqV2l"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoC70vAoqV2M"
      },
      "source": [
        "server = \"SBM\"\n",
        "GPU = 0\n",
        "sel_cv_idx = 8\n",
        "num_workers = 4\n",
        "seed = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ULW05WDqV2Q",
        "outputId": "a1673fdb-9ee1-4183-c2b4-9dbad6079d7d"
      },
      "source": [
        "import numpy as np\n",
        "outer_cv_part = np.arange(sel_cv_idx * 2, sel_cv_idx * 2 + 2)\n",
        "print(\"Selected Fold: {}\".format(outer_cv_part))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selected Fold: [16 17]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ7eOETQqV2S"
      },
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "extr_cand = [1024]\n",
        "pred_cand = [1024]\n",
        "disc_cand = [1024]\n",
        "\n",
        "dropout_cand = [0.7]\n",
        "dropout_reg_cand = [0.95]\n",
        "batch_size_cand = [32]\n",
        "lr_cand = [5e-05]\n",
        "epochs_cand = [150]\n",
        "\n",
        "hsp_extr_cand = [0.975]\n",
        "hsp_pred_cand = [0.1, 0.5, 0.9]\n",
        "hsp_disc_cand = [0.1, 0.5, 0.9]\n",
        "\n",
        "lambda_cand = [0.02]\n",
        "l2_param_cand = [5e-03]\n",
        "\n",
        "param_cand = {\n",
        "    \"1_extr\": extr_cand, \"2_pred\": pred_cand, \"3_disc\": disc_cand, \n",
        "    \"dropout\": dropout_cand, \"dropout_reg\": dropout_reg_cand,\n",
        "    \"batch_size\": batch_size_cand, \"lr\": lr_cand, \"epochs\": epochs_cand,\n",
        "    \"lambda_\": lambda_cand, \"l2_param\": l2_param_cand,\n",
        "    \"1_hsp_extr\": hsp_extr_cand, \"2_hsp_pred\": hsp_pred_cand, \"3_hsp_disc\": hsp_disc_cand\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1keevVaqV2T"
      },
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import pickle\n",
        "import random\n",
        "import itertools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "from decimal import Decimal\n",
        "from datetime import datetime as dt\n",
        "from pytz import timezone"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klcEtgpiqV2U"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable, Function\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms, utils\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.swa_utils import AveragedModel, SWALR\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAWsPn2kqV2V"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQlqqFo9qV2W"
      },
      "source": [
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   \n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXZmxeBnqV2X"
      },
      "source": [
        "def seed_everything(seed=seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "seed_everything(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_HgJj3cqV2Y"
      },
      "source": [
        "nowtime = dt.now(timezone(\"Asia/Seoul\")); year = str(nowtime.year)[2:]\n",
        "month = '0{}'.format(nowtime.month) if nowtime.month < 10 else str(nowtime.month)\n",
        "day = '0{}'.format(nowtime.day) if nowtime.day < 10 else str(nowtime.day)\n",
        "hour = '0{}'.format(nowtime.hour) if nowtime.hour < 10 else str(nowtime.hour)\n",
        "minute = '0{}'.format(nowtime.minute) if nowtime.minute < 10 else str(nowtime.minute)\n",
        "sec = '0{}'.format(nowtime.second) if nowtime.second < 10 else str(nowtime.second)\n",
        "msec = str(nowtime.microsecond)[:2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5MnlULFqV2Z",
        "outputId": "e258dd53-42b2-4963-d1b2-4b14eba14475"
      },
      "source": [
        "save_path = \"/users/hjw/data/Nested_CV/test\"\n",
        "output_folder = \"{}/{}{}{}_{}\".format(save_path, year, month, day, server)\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "print(output_folder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/users/hjw/data/Nested_CV/test/210802_SBM\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "n5irBoS8qV2a",
        "outputId": "4c50ec0d-206d-4c33-a1dc-7cd94af48004"
      },
      "source": [
        "data = np.load(\"/users/hjw/data/ABCD/npz_files/rsfc_p_site_scanner_si_ge.npz\", allow_pickle=True)\n",
        "X = stats.zscore(data[\"X\"], axis=1)\n",
        "y = data[\"y\"]\n",
        "print(X.shape, y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(6905, 61776) (6905, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANxYydp-qV2a"
      },
      "source": [
        "p_factor_idx = 0\n",
        "site_idx = 1\n",
        "scanner_idx = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8smgcCB6qV2b"
      },
      "source": [
        "y = np.array(y, dtype=np.float)\n",
        "y[:, site_idx] = y[:, site_idx].astype(np.int)\n",
        "y[:, scanner_idx] = y[:, scanner_idx].astype(np.int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "CNyGJTqxqV2b",
        "outputId": "870b41f5-0669-4a5d-fe57-ece6e9731580"
      },
      "source": [
        "# Spliting subject indices for leave-one-site-out validation set from two sites\n",
        "y_df = pd.DataFrame(y, columns=[\"p-factor\", \"site\", \"scanner\"])\n",
        "site_unq = np.unique(y[:, site_idx])\n",
        "data_idx = np.arange(y.shape[0])\n",
        "\n",
        "outer_train_folds_idx = []\n",
        "outer_test_folds_idx = []\n",
        "inner_folds_idx = []\n",
        "\n",
        "split_seed = 0\n",
        "n_outer_repeat = len(site_unq)\n",
        "n_inner_repeat = 3\n",
        "\n",
        "# Outer loop\n",
        "for n_outer, outer_test_site in enumerate(site_unq):\n",
        "    outer_train_idx = np.where(y[:, site_idx] != outer_test_site)[0]\n",
        "    outer_test_idx = np.where(y[:, site_idx] == outer_test_site)[0]\n",
        "    outer_train_folds_idx.append(outer_train_idx)\n",
        "    outer_test_folds_idx.append(outer_test_idx)\n",
        "    \n",
        "    outer_test_df = y_df.iloc[outer_test_idx]\n",
        "    outer_train_df = y_df.iloc[outer_train_idx]\n",
        "    valid_0_df = outer_train_df[outer_train_df[\"scanner\"] == 0]\n",
        "    valid_1_df = outer_train_df[outer_train_df[\"scanner\"] == 1]\n",
        "    valid_0_site_unq = pd.Series(np.unique(valid_0_df[\"site\"]))\n",
        "    valid_1_site_unq = pd.Series(np.unique(valid_1_df[\"site\"]))\n",
        "\n",
        "    inner_train_folds_idx = []\n",
        "    inner_valid_folds_idx = []\n",
        "    \n",
        "    inner_valid_site_0_list = valid_0_site_unq.sample(\n",
        "        n=n_inner_repeat, replace=False, random_state=split_seed)\n",
        "    inner_valid_site_1_list = valid_1_site_unq.sample(\n",
        "        n=n_inner_repeat, replace=False, random_state=split_seed)\n",
        "    \n",
        "    # Inner loop\n",
        "    for n_inner in range(n_inner_repeat):\n",
        "        inner_valid_site_0 = inner_valid_site_0_list.values[n_inner]\n",
        "        inner_valid_site_1 = inner_valid_site_1_list.values[n_inner]\n",
        "        inner_valid_site = [inner_valid_site_0, inner_valid_site_1]\n",
        "        inner_valid_cond_0 = (outer_train_df[\"site\"] == inner_valid_site_0)\n",
        "        inner_valid_cond_1 = (outer_train_df[\"site\"] == inner_valid_site_1)\n",
        "        inner_valid_df = outer_train_df[inner_valid_cond_0 | inner_valid_cond_1]\n",
        "\n",
        "        inner_train_idx = np.setdiff1d(\n",
        "            outer_train_df.index.values, inner_valid_df.index.values)\n",
        "        inner_valid_idx = inner_valid_df.index.values\n",
        "        print(\"[{}/{}] inner fold: train: {}, valid: {}\".\n",
        "              format(n_inner + 1, n_inner_repeat, len(inner_train_idx), len(inner_valid_idx)),\n",
        "              end=\", \")\n",
        "        print(\"valid site: {}, {}\".format(int(inner_valid_site[0]), int(inner_valid_site[1])))\n",
        "        inner_train_folds_idx.append(inner_train_idx)\n",
        "        inner_valid_folds_idx.append(inner_valid_idx)\n",
        "        \n",
        "    inner_folds_idx.append([inner_train_folds_idx, inner_valid_folds_idx])\n",
        "    \n",
        "    outer_test_scnr_label = np.unique(outer_test_df[\"scanner\"])\n",
        "    outer_train_scnr_label = np.unique(outer_train_df[\"scanner\"])\n",
        "    inner_valid_scnr_label = np.unique(inner_valid_df[\"scanner\"])\n",
        "    \n",
        "    print(\"[{}/{}] outer fold: train: {}, test: {}\"\n",
        "          .format(n_outer + 1, len(site_unq), len(outer_train_idx), len(outer_test_idx)), \n",
        "          end=\" --> \")\n",
        "    print(\"outer test site: {}\\n\".format(int(outer_test_site)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1/3] inner fold: train: 6054, valid: 563, valid site: 10, 12\n",
            "[2/3] inner fold: train: 5631, valid: 986, valid site: 4, 21\n",
            "[3/3] inner fold: train: 5998, valid: 619, valid site: 8, 9\n",
            "[1/18] outer fold: train: 6617, test: 288 --> outer test site: 2\n",
            "\n",
            "[1/3] inner fold: train: 5801, valid: 563, valid site: 10, 12\n",
            "[2/3] inner fold: train: 5378, valid: 986, valid site: 4, 21\n",
            "[3/3] inner fold: train: 5745, valid: 619, valid site: 8, 9\n",
            "[2/18] outer fold: train: 6364, test: 541 --> outer test site: 3\n",
            "\n",
            "[1/3] inner fold: train: 5613, valid: 788, valid site: 13, 11\n",
            "[2/3] inner fold: train: 5753, valid: 648, valid site: 18, 20\n",
            "[3/3] inner fold: train: 5735, valid: 666, valid site: 10, 7\n",
            "[3/18] outer fold: train: 6401, test: 504 --> outer test site: 4\n",
            "\n",
            "[1/3] inner fold: train: 6045, valid: 563, valid site: 10, 12\n",
            "[2/3] inner fold: train: 5622, valid: 986, valid site: 4, 21\n",
            "[3/3] inner fold: train: 5989, valid: 619, valid site: 8, 9\n",
            "[4/18] outer fold: train: 6608, test: 297 --> outer test site: 5\n",
            "\n",
            "[1/3] inner fold: train: 5871, valid: 563, valid site: 10, 12\n",
            "[2/3] inner fold: train: 5448, valid: 986, valid site: 4, 21\n",
            "[3/3] inner fold: train: 5815, valid: 619, valid site: 8, 9\n",
            "[5/18] outer fold: train: 6434, test: 471 --> outer test site: 6\n",
            "\n",
            "[1/3] inner fold: train: 6097, valid: 563, valid site: 10, 12\n",
            "[2/3] inner fold: train: 5674, valid: 986, valid site: 4, 21\n",
            "[3/3] inner fold: train: 6041, valid: 619, valid site: 8, 9\n",
            "[6/18] outer fold: train: 6660, test: 245 --> outer test site: 7\n",
            "\n",
            "[1/3] inner fold: train: 5889, valid: 788, valid site: 13, 11\n",
            "[2/3] inner fold: train: 6029, valid: 648, valid site: 18, 20\n",
            "[3/3] inner fold: train: 6011, valid: 666, valid site: 10, 7\n",
            "[7/18] outer fold: train: 6677, test: 228 --> outer test site: 8\n",
            "\n",
            "[1/3] inner fold: train: 5951, valid: 563, valid site: 10, 12\n",
            "[2/3] inner fold: train: 5528, valid: 986, valid site: 4, 21\n",
            "[3/3] inner fold: train: 6041, valid: 473, valid site: 8, 7\n",
            "[8/18] outer fold: train: 6514, test: 391 --> outer test site: 9\n",
            "\n",
            "[1/3] inner fold: train: 5696, valid: 788, valid site: 13, 11\n",
            "[2/3] inner fold: train: 5836, valid: 648, valid site: 18, 20\n",
            "[3/3] inner fold: train: 6011, valid: 473, valid site: 8, 7\n",
            "[9/18] outer fold: train: 6484, test: 421 --> outer test site: 10\n",
            "\n",
            "[1/3] inner fold: train: 5984, valid: 563, valid site: 10, 12\n",
            "[2/3] inner fold: train: 5561, valid: 986, valid site: 4, 21\n",
            "[3/3] inner fold: train: 6074, valid: 473, valid site: 8, 7\n",
            "[10/18] outer fold: train: 6547, test: 358 --> outer test site: 11\n",
            "\n",
            "[1/3] inner fold: train: 5984, valid: 779, valid site: 10, 11\n",
            "[2/3] inner fold: train: 5777, valid: 986, valid site: 4, 21\n",
            "[3/3] inner fold: train: 6290, valid: 473, valid site: 8, 7\n",
            "[11/18] outer fold: train: 6763, test: 142 --> outer test site: 12\n",
            "\n",
            "[1/3] inner fold: train: 5696, valid: 779, valid site: 10, 11\n",
            "[2/3] inner fold: train: 5827, valid: 648, valid site: 18, 20\n",
            "[3/3] inner fold: train: 6002, valid: 473, valid site: 8, 7\n",
            "[12/18] outer fold: train: 6475, test: 430 --> outer test site: 13\n",
            "\n",
            "[1/3] inner fold: train: 5783, valid: 779, valid site: 10, 11\n",
            "[2/3] inner fold: train: 5576, valid: 986, valid site: 4, 21\n",
            "[3/3] inner fold: train: 6089, valid: 473, valid site: 8, 7\n",
            "[13/18] outer fold: train: 6562, test: 343 --> outer test site: 14\n",
            "\n",
            "[1/3] inner fold: train: 5786, valid: 779, valid site: 10, 11\n",
            "[2/3] inner fold: train: 5579, valid: 986, valid site: 4, 21\n",
            "[3/3] inner fold: train: 6092, valid: 473, valid site: 8, 7\n",
            "[14/18] outer fold: train: 6565, test: 340 --> outer test site: 15\n",
            "\n",
            "[1/3] inner fold: train: 5350, valid: 779, valid site: 10, 11\n",
            "[2/3] inner fold: train: 5143, valid: 986, valid site: 4, 21\n",
            "[3/3] inner fold: train: 5656, valid: 473, valid site: 8, 7\n",
            "[15/18] outer fold: train: 6129, test: 776 --> outer test site: 16\n",
            "\n",
            "[1/3] inner fold: train: 5879, valid: 779, valid site: 10, 11\n",
            "[2/3] inner fold: train: 5827, valid: 831, valid site: 13, 20\n",
            "[3/3] inner fold: train: 6185, valid: 473, valid site: 8, 7\n",
            "[16/18] outer fold: train: 6658, test: 247 --> outer test site: 18\n",
            "\n",
            "[1/3] inner fold: train: 5725, valid: 779, valid site: 10, 11\n",
            "[2/3] inner fold: train: 5518, valid: 986, valid site: 4, 21\n",
            "[3/3] inner fold: train: 6031, valid: 473, valid site: 8, 7\n",
            "[17/18] outer fold: train: 6504, test: 401 --> outer test site: 20\n",
            "\n",
            "[1/3] inner fold: train: 5644, valid: 779, valid site: 10, 11\n",
            "[2/3] inner fold: train: 5518, valid: 905, valid site: 4, 20\n",
            "[3/3] inner fold: train: 5950, valid: 473, valid site: 8, 7\n",
            "[18/18] outer fold: train: 6423, test: 482 --> outer test site: 21\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VSpcib0qV2c"
      },
      "source": [
        "mode = \"max\"\n",
        "lr_patience = 5\n",
        "min_lr = 1e-08\n",
        "lr_factor = 0.25\n",
        "\n",
        "swa_lr = 5e-03\n",
        "momentum = 0.90\n",
        "l1_param = 0\n",
        "early_stopping_patience = 150\n",
        "\n",
        "input_dim = 61776\n",
        "n_classes = len(np.unique(y[:, scanner_idx]))\n",
        "output_reg_dim = 1\n",
        "output_clf_dim = n_classes\n",
        "\n",
        "wsc_flag = [1, 1, 1]\n",
        "beta_lr = [1e-04, 1e-03, 1e-03]\n",
        "max_beta = [1e-02, 5e-02, 5e-02]\n",
        "n_wsc = wsc_flag.count(1)\n",
        "\n",
        "outer_n_splits = n_outer_repeat\n",
        "inner_n_splits = n_inner_repeat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0vCf2N6qV2d"
      },
      "source": [
        "# Training dataset\n",
        "class train_dataset(Dataset): \n",
        "    def __init__(self, X_train, y_train):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X_train)\n",
        "    \n",
        "    def __getitem__(self, idx): \n",
        "        X_train = torch.from_numpy(self.X_train[idx]).type(torch.FloatTensor)\n",
        "        y_train = torch.from_numpy(self.y_train[idx]).type(torch.FloatTensor)\n",
        "\n",
        "        return X_train, y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7uc19tLqV2d"
      },
      "source": [
        "# Test dataset\n",
        "class valid_dataset(Dataset): \n",
        "    def __init__(self, X_valid, y_valid):\n",
        "        self.X_valid = X_valid\n",
        "        self.y_valid = y_valid\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X_valid)\n",
        "    \n",
        "    def __getitem__(self, idx): \n",
        "        X_valid = torch.from_numpy(self.X_valid[idx]).type(torch.FloatTensor)\n",
        "        y_valid = torch.from_numpy(self.y_valid[idx]).type(torch.FloatTensor)\n",
        "        \n",
        "        return X_valid, y_valid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq0i_2U3qV2d"
      },
      "source": [
        "# Test dataset\n",
        "class test_dataset(Dataset): \n",
        "    def __init__(self, X_test, y_test):\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X_test)\n",
        "    \n",
        "    def __getitem__(self, idx): \n",
        "        X_test = torch.from_numpy(self.X_test[idx]).type(torch.FloatTensor)\n",
        "        y_test = torch.from_numpy(self.y_test[idx]).type(torch.FloatTensor)\n",
        "        \n",
        "        return X_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70BynnVaqV2e"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\n",
        "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Q0ZCRlvqV2e"
      },
      "source": [
        "class GradientReversalFunction(Function):\n",
        "    \"\"\"\n",
        "    Gradient Reversal Layer from:\n",
        "    Unsupervised Domain Adaptation by Backpropagation (Ganin & Lempitsky, 2015)\n",
        "    Forward pass is the identity function. In the backward pass,\n",
        "    the upstream gradients are multiplied by -lambda (i.e. gradient is reversed)\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, lambda_):\n",
        "        ctx.lambda_ = lambda_\n",
        "        return x.clone()\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grads):\n",
        "        lambda_ = ctx.lambda_\n",
        "        lambda_ = grads.new_tensor(lambda_)\n",
        "        dx = -lambda_ * grads\n",
        "        return dx, None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4T1GISXlqV2e"
      },
      "source": [
        "class GradientReversal(torch.nn.Module):\n",
        "    def __init__(self, lambda_=0.0):\n",
        "        super(GradientReversal, self).__init__()\n",
        "        self.lambda_ = lambda_\n",
        "\n",
        "    def forward(self, x):\n",
        "        return GradientReversalFunction.apply(x, self.lambda_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3Ee6hv4qV2e"
      },
      "source": [
        "class DNN(nn.Module):\n",
        "    def __init__(self, extr_hidden, disc_hidden, pred_hidden, \n",
        "                 dropout_rate, dropout_reg, lambda_, act_func_name):\n",
        "        super(DNN, self).__init__()\n",
        "        self.ext_1 = nn.Linear(input_dim, extr_hidden)\n",
        "        self.ext_bn_1 = nn.BatchNorm1d(extr_hidden)\n",
        "        \n",
        "        self.reg_1 = nn.Linear(extr_hidden, pred_hidden)\n",
        "        self.reg_bn_1 = nn.BatchNorm1d(pred_hidden)\n",
        "        self.reg_2 = nn.Linear(pred_hidden, output_reg_dim)\n",
        "        \n",
        "        self.clf_1 = nn.Linear(extr_hidden, disc_hidden)\n",
        "        self.clf_bn_1 = nn.BatchNorm1d(disc_hidden)\n",
        "        self.clf_2 = nn.Linear(disc_hidden, output_clf_dim)\n",
        "\n",
        "        self.GradientReversal = GradientReversal(lambda_)\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "        self.dropout_reg = nn.Dropout(p=dropout_reg)\n",
        "        self.act_func = get_activation_function(act_func_name)\n",
        "        self.weights_init()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        feature = self.ext_1(x)\n",
        "        feature = self.ext_bn_1(feature)\n",
        "        feature = self.act_func(feature)\n",
        "        feature = self.dropout(feature)\n",
        "        \n",
        "        x_reg = self.reg_1(feature)\n",
        "        x_reg = self.reg_bn_1(x_reg)\n",
        "        x_reg = self.act_func(x_reg)\n",
        "        x_reg = self.dropout_reg(x_reg)\n",
        "        x_reg = self.reg_2(x_reg)\n",
        "        \n",
        "        x_clf = self.GradientReversal(feature)\n",
        "        x_clf = self.clf_1(x_clf)\n",
        "        x_clf = self.clf_bn_1(x_clf)\n",
        "        x_clf = self.act_func(x_clf)\n",
        "        # x_clf = self.dropout(x_clf)\n",
        "        x_clf = self.clf_2(x_clf)\n",
        "        \n",
        "        return x_reg, x_clf\n",
        "    \n",
        "    def weights_init(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "                nn.init.normal_(m.bias, std=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NP_tYXtqV2f"
      },
      "source": [
        "def get_optimizer(model, opt_name, learning_rate=None, l2_param=None):\n",
        "    lower_opt_name = opt_name.lower()\n",
        "    if lower_opt_name == 'momentum':\n",
        "        return optim.SGD(model.parameters(), lr=learning_rate, \n",
        "                         momentum=momentum, weight_decay=l2_param)\n",
        "    elif lower_opt_name == 'nag':\n",
        "        return optim.SGD(model.parameters(), lr=learning_rate, \n",
        "                         momentum=momentum, weight_decay=l2_param, nesterov=True)\n",
        "    elif lower_opt_name == 'adam':\n",
        "        return optim.Adam(model.parameters(), lr=learning_rate, \n",
        "                          weight_decay=l2_param)\n",
        "    else:\n",
        "        sys.exit(\"Illegal arguement for optimizer type\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBHPU8ktqV2f"
      },
      "source": [
        "def get_activation_function(act_func_name):\n",
        "    act_func_name = act_func_name.lower()\n",
        "    if act_func_name == 'relu':\n",
        "        return nn.ReLU()\n",
        "    elif act_func_name == 'prelu':\n",
        "        return nn.PReLU()\n",
        "    elif act_func_name == 'elu':\n",
        "        return nn.ELU()\n",
        "    elif act_func_name == 'silu':\n",
        "        return nn.SiLU()\n",
        "    elif act_func_name == 'leakyrelu':\n",
        "        return nn.LeakyReLU()\n",
        "    elif act_func_name == 'tanh':\n",
        "        return nn.Tanh()\n",
        "    else:\n",
        "        sys.exit(\"Illegal arguement for activation function type\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQVi3dweqV2g"
      },
      "source": [
        "def init_hsp(n_wsc, epochs):\n",
        "    hsp_val = np.zeros(n_wsc)\n",
        "    beta_val = hsp_val.copy()\n",
        "    hsp_list = np.zeros((n_wsc, epochs))\n",
        "    beta_list = np.zeros((n_wsc, epochs))\n",
        "    \n",
        "    return hsp_val, beta_val, hsp_list, beta_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9QKG_HjqV2g"
      },
      "source": [
        "# Weight sparsity control with Hoyer's sparsness (Layer wise)\n",
        "def calc_hsp(w, beta, max_beta, beta_lr, tg_hsp):\n",
        "    \n",
        "    # Get value of weight\n",
        "    [dim, n_nodes] = w.shape\n",
        "    num_elements = dim * n_nodes\n",
        "    norm_ratio = torch.norm(w, 1) / torch.norm(w, 2)\n",
        "\n",
        "    # Calculate hoyer's sparsity level\n",
        "    num = np.sqrt(num_elements) - norm_ratio.item()\n",
        "    den = np.sqrt(num_elements) - 1\n",
        "    hsp = num / den\n",
        "\n",
        "    # Update beta\n",
        "    beta = beta + beta_lr * np.sign(tg_hsp - hsp)\n",
        "    \n",
        "    # Trim value\n",
        "    beta = -max_beta if beta < -max_beta else beta\n",
        "    beta = max_beta if beta > max_beta else beta\n",
        "\n",
        "    return [hsp, beta]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiOF-J2KqV2g"
      },
      "source": [
        "def l1_penalty(model, epoch, hsp_val, beta_val, hsp_list, beta_list, tg_hsp):\n",
        "    l1_reg = None\n",
        "    layer_idx = 0\n",
        "    wsc_idx = 0\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"weight\" in name and \"bn\" not in name:\n",
        "            if \"ext\" in name or \"reg_1\" in name or \"clf_1\" in name:\n",
        "                temp_w = param\n",
        "                \n",
        "                if wsc_flag[layer_idx] != 0:\n",
        "                    hsp_val[wsc_idx], beta_val[wsc_idx] = calc_hsp(\n",
        "                        temp_w, beta_val[wsc_idx], max_beta[wsc_idx], \n",
        "                        beta_lr[wsc_idx], tg_hsp[wsc_idx]\n",
        "                    )\n",
        "                    hsp_list[wsc_idx, epoch - 1] = hsp_val[wsc_idx]\n",
        "                    beta_list[wsc_idx, epoch - 1] = beta_val[wsc_idx]\n",
        "                    layer_reg = torch.norm(temp_w, 1) * beta_val[wsc_idx]\n",
        "                    wsc_idx += 1\n",
        "                else:\n",
        "                    layer_reg = torch.norm(temp_w, 1).item() * l1_param\n",
        "\n",
        "                if l1_reg is None:\n",
        "                    l1_reg = layer_reg\n",
        "                else:\n",
        "                    l1_reg = l1_reg + layer_reg\n",
        "                layer_idx += 1\n",
        "        \n",
        "    return l1_reg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqb_Z6eLqV2g"
      },
      "source": [
        "def pearsonr(x, y):\n",
        "    x_mean = torch.mean(x)\n",
        "    y_mean = torch.mean(y)\n",
        "    xx = x.sub(x_mean)\n",
        "    yy = y.sub(y_mean)\n",
        "    num = xx.dot(yy)\n",
        "    den = torch.norm(xx, 2) * torch.norm(yy, 2)\n",
        "    corr = num / den\n",
        "    return corr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4DWJ3VfqV2g"
      },
      "source": [
        "def train(model, epoch, train_loader, optimizer, criterion_clf, criterion_reg, \n",
        "          hsp_val, beta_val, hsp_list, beta_list, tg_hsp):\n",
        "    model.train()\n",
        "    reg_loss = 0\n",
        "    clf_loss = 0\n",
        "    clf_acc = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    y_train_true = []\n",
        "    y_train_pred = []\n",
        "    \n",
        "    for batch_idx, (input, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        input, target = input.to(DEVICE), target.to(DEVICE)\n",
        "        output_reg, output_clf = model(input)\n",
        "        target_clf = target[:, scanner_idx].long().view(-1)\n",
        "        target_reg = target[:, p_factor_idx].view(-1, 1)\n",
        "        running_clf_loss = criterion_clf(output_clf, target_clf)\n",
        "        running_reg_loss = criterion_reg(output_reg, target_reg)\n",
        "        l1_term = l1_penalty(model, epoch, hsp_val, beta_val, hsp_list, beta_list, tg_hsp)\n",
        "        running_loss = running_clf_loss + running_reg_loss + l1_term\n",
        "        cost = running_loss\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "        clf_loss += running_clf_loss.item()\n",
        "        reg_loss += running_reg_loss.item()\n",
        "        total += output_reg.size(0)\n",
        "        _, pred = torch.max(output_clf.data, 1)\n",
        "        correct += (pred.view(-1, 1) == target).sum().item()\n",
        "        true_batch = torch.flatten(target_reg.detach())\n",
        "        pred_batch = torch.flatten(output_reg.detach())\n",
        "        y_train_true.append(true_batch)\n",
        "        y_train_pred.append(pred_batch)\n",
        "        \n",
        "    reg_loss /= total\n",
        "    clf_loss /= total\n",
        "    clf_acc = 100 * correct / total\n",
        "    y_train_true = torch.flatten(torch.stack(y_train_true))\n",
        "    y_train_pred = torch.flatten(torch.stack(y_train_pred))\n",
        "    train_corr = pearsonr(y_train_true, y_train_pred)\n",
        "    return clf_loss, reg_loss, clf_acc, train_corr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Kbh_dHcqV2h"
      },
      "source": [
        "def valid(model, epoch, valid_loader, criterion_clf, criterion_reg):\n",
        "    model.eval()\n",
        "    reg_loss = 0\n",
        "    clf_loss = 0\n",
        "    clf_acc = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    y_valid_true = []\n",
        "    y_valid_pred = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for input, target in valid_loader:\n",
        "            input, target = input.to(DEVICE), target.to(DEVICE)\n",
        "            output_reg, output_clf = model(input)\n",
        "            target_clf = target[:, scanner_idx].long().view(-1)\n",
        "            target_reg = target[:, p_factor_idx].view(-1, 1)\n",
        "            running_clf_loss = criterion_clf(output_clf, target_clf)\n",
        "            running_reg_loss = criterion_reg(output_reg, target_reg)\n",
        "            clf_loss += running_clf_loss.item()\n",
        "            reg_loss += running_reg_loss.item()\n",
        "            total += output_reg.size(0)\n",
        "            _, pred = torch.max(output_clf.data, 1)\n",
        "            correct += (pred.view(-1, 1) == target).sum().item()\n",
        "            true_batch = torch.flatten(target_reg.detach())\n",
        "            pred_batch = torch.flatten(output_reg.detach())\n",
        "            y_valid_true.append(true_batch)\n",
        "            y_valid_pred.append(pred_batch)\n",
        "\n",
        "    clf_acc = 100 * correct / total\n",
        "    y_valid_true = torch.flatten(torch.stack(y_valid_true))\n",
        "    y_valid_pred = torch.flatten(torch.stack(y_valid_pred))\n",
        "    valid_corr = pearsonr(y_valid_true, y_valid_pred)\n",
        "    return clf_loss, reg_loss, clf_acc, valid_corr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnbjM8NbqV2h"
      },
      "source": [
        "def test(model, epoch, test_loader, criterion_clf, criterion_reg):\n",
        "    model.eval()\n",
        "    reg_loss = 0\n",
        "    total = 0\n",
        "    y_test_true = []\n",
        "    y_test_pred = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for input, target in test_loader:\n",
        "            input, target = input.to(DEVICE), target.to(DEVICE)\n",
        "            output_reg, output_clf = model(input)\n",
        "            target_reg = target[:, p_factor_idx].view(-1, 1)\n",
        "            running_reg_loss = criterion_reg(output_reg, target_reg)\n",
        "            reg_loss += running_reg_loss.item()\n",
        "            total += output_reg.size(0)\n",
        "            true_batch = torch.flatten(target_reg.detach())\n",
        "            pred_batch = torch.flatten(output_reg.detach())\n",
        "            y_test_true.append(true_batch)\n",
        "            y_test_pred.append(pred_batch)\n",
        "\n",
        "    y_test_true = torch.flatten(torch.stack(y_test_true))\n",
        "    y_test_pred = torch.flatten(torch.stack(y_test_pred))\n",
        "    test_corr = pearsonr(y_test_true, y_test_pred)\n",
        "    return reg_loss, test_corr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xe43KLejqV2h"
      },
      "source": [
        "class early_stopping_func:\n",
        "    def __init__(self, patience=5, verbose=False, delta=0, path=None):\n",
        "        self.patience = patience\n",
        "        self.verbose = verbose\n",
        "        self.counter = 0\n",
        "        self.best_epoch = 0\n",
        "        self.best_corr = 0\n",
        "        self.early_stop = False\n",
        "        self.valid_corr_max = -np.Inf\n",
        "        self.delta = delta\n",
        "        self.path = path\n",
        "    \n",
        "    def __call__(self, valid_loss, model, epoch, train_corr, valid_corr, test_corr):\n",
        "        if self.best_corr is None:\n",
        "            self.best_corr = valid_corr\n",
        "            self.best_corr_list = [train_corr, valid_corr, test_corr]\n",
        "            self.save_checkpoint(valid_loss, model, epoch)\n",
        "        elif valid_corr < self.best_corr + self.delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "                print(\"Early Stopping! Best Model at Epoch {}\"\n",
        "                      .format(self.best_epoch), end=\", \")\n",
        "                print(\"valid corr: {:.4f}, test corr: {:.4f}\"\n",
        "                      .format(self.best_corr_list[1], self.best_corr_list[2]))\n",
        "        else:\n",
        "            self.best_corr = valid_corr\n",
        "            self.best_corr_list = [train_corr, valid_corr, test_corr]\n",
        "            self.save_checkpoint(self.best_corr, model, epoch)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, best_corr, model, epoch):\n",
        "        if self.verbose:\n",
        "            print(\"Validation Corr Increased! ({:.4f} --> {:.4f}), Saving the Model!\"\n",
        "                  .format(self.valid_corr_max, best_corr))\n",
        "        # torch.save(model.state_dict(), self.path + \"/early_stopped_model.pt\")\n",
        "        self.valid_corr_max = best_corr\n",
        "        self.best_epoch = epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4Su1ASAqV2i"
      },
      "source": [
        "def plot_learning_curves(\n",
        "    save_dir, epochs, train_loss, valid_loss,  \n",
        "    train_corr, valid_corr, train_acc, valid_acc, lr,\n",
        "    plot_hsp_list, plot_beta_list, tg_hsp):\n",
        "    \n",
        "    sns.set(style=\"dark\", font_scale=2)\n",
        "    fig, ax = plt.subplots(2, 3, figsize=(28, 10))\n",
        "    ax = ax.flat\n",
        "    lw = 3.5\n",
        "    last_epoch = epochs\n",
        "    \n",
        "    train_loss, valid_loss = np.array(train_loss), np.array(valid_loss)\n",
        "    \n",
        "    ax[0].plot(train_loss[:last_epoch, 0], label='train disc loss', lw=lw, color=\"r\")\n",
        "    ax[0].legend()\n",
        "    ax[0].set_title(\"Discriminator Loss Plot\", pad=20)\n",
        "\n",
        "    ax[1].plot(train_loss[:last_epoch, 1], label='train pred loss', lw=lw, color=\"r\")\n",
        "    ax[1].plot(valid_loss[:last_epoch, 1], label='valid pred loss', lw=lw, color=\"g\")\n",
        "    ax[1].legend()\n",
        "    ax[1].set_title(\"Predictor Loss Plot\", pad=20)\n",
        "\n",
        "    ax[2].plot(lr[:last_epoch], label='learning rate', lw=lw, color=\"k\")\n",
        "    ax[2].legend()\n",
        "    ax[2].set_title(\"Learning Rate Plot\", pad=20)\n",
        "\n",
        "    ax[3].plot(train_corr[:last_epoch], label='train corr', lw=lw, color=\"r\")\n",
        "    ax[3].plot(valid_corr[:last_epoch], label='valid corr', lw=lw, color=\"g\")\n",
        "    ax[3].legend()\n",
        "    ax[3].set_title(\"Correlation Plot ($r$={:.4f})\".format(valid_corr[-1]), pad=20)\n",
        "\n",
        "    plot_hsp_list, plot_beta_list = np.array(plot_hsp_list).T, np.array(plot_beta_list).T\n",
        "    \n",
        "    for idx, n_layer in enumerate(indices):\n",
        "        ax[4].plot(plot_hsp_list[idx], label='layer{}'.format(n_layer), lw=lw)\n",
        "        ax[5].plot(plot_beta_list[idx], \n",
        "                   label='layer{}'.format(n_layer), lw=lw)\n",
        "        ax[4].legend(); ax[5].legend()\n",
        "        ax[4].set_title(\"HSP plot [{:.3f}/{:.3f}]\"\n",
        "                        .format(plot_hsp_list[0, -1], tg_hsp[0][0]), pad=20)\n",
        "        ax[5].set_title(\"Beta plot\", pad=20)\n",
        "    \n",
        "    fig.tight_layout()\n",
        "    fig.savefig(\"{}/Learning_curves.png\".format(save_dir))\n",
        "    \n",
        "    plt.close(fig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVkPGWxTqV2i"
      },
      "source": [
        "print_epoch = 50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYyS5GaEqV2i"
      },
      "source": [
        "def run_inner_fold(output_save_dir=None, cur_tg_hsp=None):\n",
        "    inner_cv = []\n",
        "    \n",
        "    for n_inner_cv in range(inner_n_splits):\n",
        "        \n",
        "        print(\"\\n===================================\", end=\" \")\n",
        "        print(\"Inner Fold [{}/{}]\".format(n_inner_cv + 1, inner_n_splits), end=\" \")\n",
        "        print(\"===================================\")\n",
        "\n",
        "        inner_start_fold_time = time.time()\n",
        "        inner_save_dir = \"{}/Inner_fold_{}\".format(output_save_dir, n_inner_cv + 1)\n",
        "        os.makedirs(inner_save_dir, exist_ok=True)\n",
        "\n",
        "        inner_train_idx = inner_train_folds_idx[n_inner_cv]\n",
        "        inner_valid_idx = inner_valid_folds_idx[n_inner_cv]\n",
        "\n",
        "        X_train, y_train = X[inner_train_idx], y[inner_train_idx]\n",
        "        X_valid, y_valid = X[inner_valid_idx], y[inner_valid_idx]\n",
        "\n",
        "        inner_train_dataset = train_dataset(X_train, y_train)\n",
        "        inner_valid_dataset = train_dataset(X_valid, y_valid)\n",
        "\n",
        "        inner_train_loader = DataLoader(\n",
        "            inner_train_dataset, batch_size=batch_size, pin_memory=True,\n",
        "            shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "        inner_valid_loader = DataLoader(\n",
        "            inner_valid_dataset, batch_size=len(y_valid), pin_memory=True,\n",
        "            shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "\n",
        "        # Assign model\n",
        "        model = DNN(\n",
        "            extr_hidden, disc_hidden, pred_hidden, dropout_rate, dropout_reg, lambda_, act_func_name\n",
        "        ).to(DEVICE)\n",
        "        optimizer = get_optimizer(model, optimizer_name, learning_rate, l2_param)\n",
        "        scheduler = ReduceLROnPlateau(\n",
        "            optimizer, mode=mode, patience=lr_patience, min_lr=min_lr, factor=lr_factor\n",
        "        )\n",
        "        criterion_clf = nn.CrossEntropyLoss()\n",
        "        criterion_reg = nn.MSELoss(reduction=\"mean\")\n",
        "\n",
        "        # list to save learning parameters\n",
        "        inner_train_loss = []\n",
        "        inner_valid_loss = []\n",
        "        inner_train_corr = []\n",
        "        inner_valid_corr = []\n",
        "        inner_train_acc = []\n",
        "        inner_valid_acc = []\n",
        "        inner_lr = []\n",
        "        inner_hsp_list = []\n",
        "        inner_beta_list = []\n",
        "\n",
        "        hsp_val, beta_val, hsp_list, beta_list = init_hsp(n_wsc, epochs)\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            train_clf_loss, train_reg_loss, train_acc, train_corr = train(\n",
        "                model, epoch, inner_train_loader, \n",
        "                optimizer, criterion_clf, criterion_reg, \n",
        "                hsp_val, beta_val, hsp_list, beta_list, cur_tg_hsp\n",
        "            )\n",
        "            valid_clf_loss, valid_reg_loss, valid_acc, valid_corr = valid(\n",
        "                model, epoch, inner_valid_loader, criterion_clf, criterion_reg\n",
        "            )\n",
        "\n",
        "            scheduler.step(hsp_val[0])\n",
        "            lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "            inner_train_loss.append([train_clf_loss, train_reg_loss])\n",
        "            inner_train_corr.append(train_corr)\n",
        "            inner_train_acc.append(train_acc)\n",
        "            inner_valid_loss.append([valid_clf_loss, valid_reg_loss])\n",
        "            inner_valid_corr.append(valid_corr)\n",
        "            inner_valid_acc.append(valid_acc)\n",
        "            inner_lr.append(lr)\n",
        "            inner_hsp_list.append(list(hsp_val))\n",
        "            inner_beta_list.append(list(beta_val))\n",
        "\n",
        "            if epoch % print_epoch == 0:\n",
        "                print(\"\\nEpoch [{:d}/{:d}]\".format(epoch, epochs), end=\" \")\n",
        "                print(\"Train corr: {:.4f}, Valid corr: {:.4f}\".format(train_corr, valid_corr))\n",
        "                for i in range(len(wsc_flag)):\n",
        "                    if wsc_flag[i] != 0:\n",
        "                        print(\"Layer {:d}: [{:.4f}/{:.4f}]\".\n",
        "                              format(i + 1, hsp_val[i], cur_tg_hsp[i][0]), end=\" \")\n",
        "                # print(\"\\nCurrent learning rate: {:.2e}\".format(Decimal(str(lr))))\n",
        "\n",
        "            plot_learning_curves(\n",
        "                inner_save_dir, epochs, inner_train_loss, inner_valid_loss,\n",
        "                inner_train_corr, inner_valid_corr, \n",
        "                inner_train_acc, inner_valid_acc, \n",
        "                inner_lr, inner_hsp_list, inner_beta_list, cur_tg_hsp\n",
        "            )\n",
        "        print(\"\\nInner Fold [{}/{}] train corr: {:.4f}, valid corr: {:.4f}\"\n",
        "              .format(n_inner_cv + 1, inner_n_splits, train_corr, valid_corr))\n",
        "        \"\"\"\n",
        "        torch.save(model.state_dict(),\n",
        "                   inner_save_dir + \"/model_fold_\" + str(n_outer_cv + 1) + \".pt\")\n",
        "        \"\"\"\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        inner_tot_time = (time.time() - inner_start_fold_time) / 60\n",
        "        print(\"Execution Time for Fold: {:.2f} mins\".format(inner_tot_time))\n",
        "        inner_cv.append([train_corr.detach().cpu().numpy(), valid_corr.detach().cpu().numpy()])\n",
        "            \n",
        "    inner_cv_df = pd.DataFrame(np.array(inner_cv), columns=[\"train\", \"valid\"])\n",
        "    avg_train_corr = inner_cv_df[\"train\"].mean()\n",
        "    avg_valid_corr = inner_cv_df[\"valid\"].mean()\n",
        "    inner_cv_df.to_csv(\"{}/inner_cv.csv\".format(output_save_dir))\n",
        "\n",
        "    return avg_train_corr, avg_valid_corr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJHZioi9qV2k"
      },
      "source": [
        "def run_outer_fold(n_outer_cv=0, outer_save_dir=None, sel_tg_hsp=None):\n",
        "\n",
        "    # Outer fold\n",
        "    print(\"\\n===================================\", end=\" \")\n",
        "    print(\"Outer Fold [{}/{}]\".format(n_outer_cv + 1, outer_n_splits), end=\" \")\n",
        "    print(\"===================================\")\n",
        "    \n",
        "    outer_start_fold_time = time.time()\n",
        "    outer_train_idx = outer_train_folds_idx[n_outer_cv]\n",
        "    outer_test_idx = outer_test_folds_idx[n_outer_cv]\n",
        "\n",
        "    X_train, y_train = X[outer_train_idx], y[outer_train_idx]\n",
        "    X_test, y_test = X[outer_test_idx], y[outer_test_idx]\n",
        "    \n",
        "    outer_train_dataset = train_dataset(X_train, y_train)\n",
        "    outer_test_dataset = test_dataset(X_test, y_test)\n",
        "    \n",
        "    outer_train_loader = DataLoader(\n",
        "        outer_train_dataset, batch_size=batch_size, pin_memory=True,\n",
        "        shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "    outer_test_loader = DataLoader(\n",
        "        outer_test_dataset, batch_size=len(y_test), pin_memory=True,\n",
        "        shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "        \n",
        "    # Assign model \n",
        "    model = DNN(\n",
        "        extr_hidden, disc_hidden, pred_hidden, dropout_rate, dropout_reg, lambda_, act_func_name\n",
        "    ).to(DEVICE)\n",
        "    optimizer = get_optimizer(model, optimizer_name, learning_rate, l2_param)\n",
        "    scheduler = ReduceLROnPlateau(\n",
        "        optimizer, mode=mode, patience=lr_patience, min_lr=min_lr, factor=lr_factor\n",
        "    )\n",
        "    criterion_clf = nn.CrossEntropyLoss()\n",
        "    criterion_reg = nn.MSELoss(reduction=\"mean\")\n",
        "              \n",
        "    # list to save learning parameters\n",
        "    outer_train_loss = []\n",
        "    outer_test_loss = []\n",
        "    outer_train_corr = []\n",
        "    outer_test_corr = []\n",
        "    outer_train_acc = []\n",
        "    outer_test_acc = []\n",
        "    outer_lr = []\n",
        "    outer_hsp_list = []\n",
        "    outer_beta_list = []\n",
        "\n",
        "    hsp_val, beta_val, hsp_list, beta_list = init_hsp(n_wsc, epochs)\n",
        "        \n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_clf_loss, train_reg_loss, train_acc, train_corr = train(\n",
        "            model, epoch, outer_train_loader, \n",
        "            optimizer, criterion_clf, criterion_reg, \n",
        "            hsp_val, beta_val, hsp_list, beta_list, sel_tg_hsp\n",
        "        )\n",
        "        test_reg_loss, test_corr = test(\n",
        "            model, epoch, outer_test_loader, criterion_clf, criterion_reg\n",
        "        )\n",
        "\n",
        "        scheduler.step(hsp_val[0])\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "        \n",
        "        outer_train_loss.append([train_clf_loss, train_reg_loss])\n",
        "        outer_train_corr.append(train_corr)\n",
        "        outer_train_acc.append(train_acc)\n",
        "        outer_test_loss.append([[], test_reg_loss])\n",
        "        outer_test_corr.append(test_corr)\n",
        "        outer_test_acc.append([])\n",
        "        outer_lr.append(lr)\n",
        "        outer_hsp_list.append(list(hsp_val))\n",
        "        outer_beta_list.append(list(beta_val))\n",
        "\n",
        "        if epoch % print_epoch == 0:\n",
        "            print(\"\\nEpoch [{:d}/{:d}]\".format(epoch, epochs), end=\" \")\n",
        "            print(\"Train corr: {:.4f}, Test corr: {:.4f}\"\n",
        "                  .format(train_corr, test_corr))\n",
        "            for i in range(len(wsc_flag)):\n",
        "                if wsc_flag[i] != 0:\n",
        "                    print(\"Layer {:d}: [{:.4f}/{:.4f}]\".\n",
        "                          format( i + 1, hsp_val[i], sel_tg_hsp[i][0]), end=\" \")\n",
        "            # print(\"\\nCurrent learning rate: {:.2e}\".format(Decimal(str(lr))))\n",
        "\n",
        "        plot_learning_curves(\n",
        "            outer_save_dir, epochs, outer_train_loss, outer_test_loss,  \n",
        "            outer_train_corr, outer_test_corr, \n",
        "            outer_train_acc, outer_test_acc, \n",
        "            outer_lr, outer_hsp_list, outer_beta_list, sel_tg_hsp\n",
        "        )\n",
        "    \n",
        "    torch.save(model.state_dict(), \n",
        "               outer_save_dir + \"/model_fold_\" + str(n_outer_cv + 1) + \".pt\")\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    train_corr = train_corr.detach().cpu().numpy()\n",
        "    test_corr = test_corr.detach().cpu().numpy()\n",
        "\n",
        "    outer_tot_time = time.time() - outer_start_fold_time\n",
        "    print(\"\\nExecution Time for Fold: {:.2f} mins\".format(outer_tot_time / 60))\n",
        "    \n",
        "    return train_corr, test_corr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vlCwu4aqV2k"
      },
      "source": [
        "param_grid = list(ParameterGrid(param_cand))\n",
        "\n",
        "temp_param = param_grid[0]\n",
        "act_func_name = \"elu\"\n",
        "optimizer_name = \"nag\"\n",
        "\n",
        "extr_hidden = temp_param[\"1_extr\"]\n",
        "pred_hidden = temp_param[\"2_pred\"]\n",
        "disc_hidden = temp_param[\"3_disc\"]\n",
        "\n",
        "dropout_rate = temp_param[\"dropout\"]\n",
        "dropout_reg = temp_param[\"dropout_reg\"]\n",
        "\n",
        "batch_size = temp_param[\"batch_size\"]\n",
        "learning_rate = temp_param[\"lr\"]\n",
        "epochs = temp_param[\"epochs\"]\n",
        "\n",
        "l2_param = temp_param[\"l2_param\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GNrNfEtqV2k"
      },
      "source": [
        "code_start_time = time.time()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "l6M9mlweqV2l",
        "outputId": "e8f8d676-b64f-4f3d-9407-5bac68ba6d9d"
      },
      "source": [
        "print(output_folder)\n",
        "\n",
        "outer_cv = []\n",
        "\n",
        "for n_outer_cv in outer_cv_part:\n",
        "    print(\"\\n===================================\", end=\" \")\n",
        "    print(\"Outer Fold [{}/{}]\".format(n_outer_cv + 1, outer_n_splits), end=\" \")\n",
        "    print(\"===================================\")\n",
        "\n",
        "    outer_save_dir = \"{}/Outer_fold_{}\".format(output_folder, n_outer_cv + 1)\n",
        "    os.makedirs(outer_save_dir, exist_ok=True)\n",
        "\n",
        "    inner_train_folds_idx = inner_folds_idx[n_outer_cv][0]\n",
        "    inner_valid_folds_idx = inner_folds_idx[n_outer_cv][1]\n",
        "    \n",
        "    inner_cv = []\n",
        "    \n",
        "    # Inner Fold\n",
        "    for param_idx, cur_param in enumerate(param_grid):\n",
        "        print(\"\\n===================================\", end=\" \")\n",
        "        print(\"Param Cand [{}/{}]\".format(param_idx + 1, len(param_grid)), end=\" \")\n",
        "        print(\"===================================\")\n",
        "\n",
        "        hsp_cand_1 = [cur_param[\"1_hsp_extr\"]]\n",
        "        hsp_cand_2 = [cur_param[\"2_hsp_pred\"]]\n",
        "        hsp_cand_3 = [cur_param[\"3_hsp_disc\"]]\n",
        "\n",
        "        indices = [i + 1 for i, x in enumerate(wsc_flag) if x == 1]\n",
        "        hsp_cand_list = list(itertools.product(hsp_cand_1, hsp_cand_2, hsp_cand_3))\n",
        "        hsp_cand_list = [list(i) for i in hsp_cand_list]\n",
        "        hsp_cand = [hsp_cand_1, hsp_cand_2, hsp_cand_3]\n",
        "        cur_tg_hsp = hsp_cand\n",
        "        \n",
        "        lambda_ = temp_param[\"lambda_\"]\n",
        "        print(\"Param:\", end=\" \")\n",
        "        for i, param in enumerate(cur_param):\n",
        "            if \"hsp\" in param or \"lambda\" in param: \n",
        "                print(\"{}: {}\".format(param, cur_param[param]), end=\" \")\n",
        "        print(\"\")\n",
        "        \n",
        "        cur_param_name = \"hsp_{}_{}_{}\".format(\n",
        "            cur_tg_hsp[0][0], cur_tg_hsp[1][0], cur_tg_hsp[2][0]\n",
        "        ) \n",
        "        param_save_dir = \"{}/{}\".format(outer_save_dir, cur_param_name)\n",
        "        os.makedirs(param_save_dir, exist_ok=True)\n",
        "\n",
        "        inner_train_corr, inner_valid_corr = run_inner_fold(param_save_dir, cur_tg_hsp)\n",
        "        inner_cv.append([inner_train_corr, inner_valid_corr])\n",
        "        \n",
        "        print(\"\\nParam Cand: [{}/{}] train corr: {:.4f}, valid corr: {:.4f}\"\n",
        "              .format(param_idx + 1, len(param_grid), inner_train_corr, inner_valid_corr))\n",
        "    \n",
        "    # Selecting hyperparameter\n",
        "    inner_valid_cv = np.array(inner_cv)[:, 1]\n",
        "    sel_idx = np.argmax(inner_valid_cv)\n",
        "    sel_param = param_grid[sel_idx]\n",
        "    sel_hsp = []\n",
        "    print(\"Selected param:\", end=\" \")\n",
        "    for x in sel_param:\n",
        "        if \"hsp\" in x: \n",
        "            print(\"{}\".format(sel_param[x]), end=\" \")\n",
        "            sel_hsp.append(sel_param[x])\n",
        "    \n",
        "    # Outer Fold\n",
        "    hsp_cand_1 = [sel_param[\"1_hsp_extr\"]]\n",
        "    hsp_cand_2 = [sel_param[\"2_hsp_pred\"]]\n",
        "    hsp_cand_3 = [sel_param[\"3_hsp_disc\"]]\n",
        "\n",
        "    indices = [i + 1 for i, x in enumerate(wsc_flag) if x == 1]\n",
        "    hsp_cand_list = list(itertools.product(hsp_cand_1, hsp_cand_2, hsp_cand_3))\n",
        "    hsp_cand_list = [list(i) for i in hsp_cand_list]\n",
        "    hsp_cand = [hsp_cand_1, hsp_cand_2, hsp_cand_3]\n",
        "    sel_tg_hsp = hsp_cand\n",
        "\n",
        "    lambda_ = sel_param[\"lambda_\"]\n",
        "\n",
        "    outer_train_corr, outer_test_corr = run_outer_fold(n_outer_cv, outer_save_dir, sel_tg_hsp)\n",
        "    outer_cv.append([sel_hsp, outer_train_corr, outer_test_corr])\n",
        "    \n",
        "    print(\"\\nOuter Fold [{}/{}]: train corr: {:.4f}, valid corr: {:.4f}\"\n",
        "          .format(n_outer_cv + 1, outer_n_splits, outer_train_corr, outer_test_corr))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/users/hjw/data/Nested_CV/test/210802_SBM\n",
            "\n",
            "=================================== Outer Fold [17/18] ===================================\n",
            "\n",
            "=================================== Param Cand [1/9] ===================================\n",
            "Param: 1_hsp_extr: 0.975 2_hsp_pred: 0.1 3_hsp_disc: 0.1 lambda_: 0.02 \n",
            "\n",
            "=================================== Inner Fold [1/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1624, Valid corr: 0.1467\n",
            "Layer 1: [0.8872/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [100/150] Train corr: 0.3362, Valid corr: 0.1434\n",
            "Layer 1: [0.9748/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [150/150] Train corr: 0.3498, Valid corr: 0.1452\n",
            "Layer 1: [0.9749/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.1000/0.1000] \n",
            "Inner Fold [1/3] train corr: 0.3498, valid corr: 0.1452\n",
            "Execution Time for Fold: 21.70 mins\n",
            "\n",
            "=================================== Inner Fold [2/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1587, Valid corr: 0.1287\n",
            "Layer 1: [0.9147/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [100/150] Train corr: 0.2529, Valid corr: 0.1364\n",
            "Layer 1: [0.9747/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [150/150] Train corr: 0.3009, Valid corr: 0.1323\n",
            "Layer 1: [0.9752/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.1000/0.1000] \n",
            "Inner Fold [2/3] train corr: 0.3009, valid corr: 0.1323\n",
            "Execution Time for Fold: 22.61 mins\n",
            "\n",
            "=================================== Inner Fold [3/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1666, Valid corr: 0.0954\n",
            "Layer 1: [0.8825/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [100/150] Train corr: 0.3165, Valid corr: 0.1355\n",
            "Layer 1: [0.9740/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [150/150] Train corr: 0.3282, Valid corr: 0.1328\n",
            "Layer 1: [0.9745/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.1000/0.1000] \n",
            "Inner Fold [3/3] train corr: 0.3282, valid corr: 0.1328\n",
            "Execution Time for Fold: 23.93 mins\n",
            "\n",
            "Param Cand: [1/9] train corr: 0.3263, valid corr: 0.1368\n",
            "\n",
            "=================================== Param Cand [2/9] ===================================\n",
            "Param: 1_hsp_extr: 0.975 2_hsp_pred: 0.1 3_hsp_disc: 0.5 lambda_: 0.02 \n",
            "\n",
            "=================================== Inner Fold [1/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.2011, Valid corr: 0.1391\n",
            "Layer 1: [0.9519/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [100/150] Train corr: 0.2201, Valid corr: 0.1413\n",
            "Layer 1: [0.9746/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [150/150] Train corr: 0.2899, Valid corr: 0.1443\n",
            "Layer 1: [0.9749/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.5000/0.5000] \n",
            "Inner Fold [1/3] train corr: 0.2899, valid corr: 0.1443\n",
            "Execution Time for Fold: 25.12 mins\n",
            "\n",
            "=================================== Inner Fold [2/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1711, Valid corr: 0.1297\n",
            "Layer 1: [0.9139/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.5002/0.5000] \n",
            "Epoch [100/150] Train corr: 0.2826, Valid corr: 0.1138\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [150/150] Train corr: 0.3316, Valid corr: 0.1084\n",
            "Layer 1: [0.9752/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.5000/0.5000] \n",
            "Inner Fold [2/3] train corr: 0.3316, valid corr: 0.1084\n",
            "Execution Time for Fold: 26.05 mins\n",
            "\n",
            "=================================== Inner Fold [3/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.2274, Valid corr: 0.1319\n",
            "Layer 1: [0.9305/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.5001/0.5000] \n",
            "Epoch [100/150] Train corr: 0.3153, Valid corr: 0.1496\n",
            "Layer 1: [0.9748/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [150/150] Train corr: 0.3440, Valid corr: 0.1545\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.5000/0.5000] \n",
            "Inner Fold [3/3] train corr: 0.3440, valid corr: 0.1545\n",
            "Execution Time for Fold: 27.27 mins\n",
            "\n",
            "Param Cand: [2/9] train corr: 0.3218, valid corr: 0.1358\n",
            "\n",
            "=================================== Param Cand [3/9] ===================================\n",
            "Param: 1_hsp_extr: 0.975 2_hsp_pred: 0.1 3_hsp_disc: 0.9 lambda_: 0.02 \n",
            "\n",
            "=================================== Inner Fold [1/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.2006, Valid corr: 0.1229\n",
            "Layer 1: [0.9539/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.8853/0.9000] \n",
            "Epoch [100/150] Train corr: 0.2159, Valid corr: 0.1271\n",
            "Layer 1: [0.9749/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.8863/0.9000] \n",
            "Epoch [150/150] Train corr: 0.2701, Valid corr: 0.1273\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.8865/0.9000] \n",
            "Inner Fold [1/3] train corr: 0.2701, valid corr: 0.1273\n",
            "Execution Time for Fold: 28.31 mins\n",
            "\n",
            "=================================== Inner Fold [2/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1987, Valid corr: 0.1241\n",
            "Layer 1: [0.9396/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.8866/0.9000] \n",
            "Epoch [100/150] Train corr: 0.2561, Valid corr: 0.1147\n",
            "Layer 1: [0.9752/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.8879/0.9000] \n",
            "Epoch [150/150] Train corr: 0.3090, Valid corr: 0.1137\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.8881/0.9000] \n",
            "Inner Fold [2/3] train corr: 0.3090, valid corr: 0.1137\n",
            "Execution Time for Fold: 29.30 mins\n",
            "\n",
            "=================================== Inner Fold [3/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.2174, Valid corr: 0.1175\n",
            "Layer 1: [0.9315/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.9005/0.9000] \n",
            "Epoch [100/150] Train corr: 0.3255, Valid corr: 0.1368\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.9000/0.9000] \n",
            "Epoch [150/150] Train corr: 0.3696, Valid corr: 0.1339\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.8999/0.9000] \n",
            "Inner Fold [3/3] train corr: 0.3696, valid corr: 0.1339\n",
            "Execution Time for Fold: 30.79 mins\n",
            "\n",
            "Param Cand: [3/9] train corr: 0.3162, valid corr: 0.1250\n",
            "\n",
            "=================================== Param Cand [4/9] ===================================\n",
            "Param: 1_hsp_extr: 0.975 2_hsp_pred: 0.5 3_hsp_disc: 0.1 lambda_: 0.02 \n",
            "\n",
            "=================================== Inner Fold [1/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1603, Valid corr: 0.1100\n",
            "Layer 1: [0.9225/0.9750] Layer 2: [0.4999/0.5000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [100/150] Train corr: 0.2417, Valid corr: 0.1373\n",
            "Layer 1: [0.9753/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [150/150] Train corr: 0.2943, Valid corr: 0.1384\n",
            "Layer 1: [0.9748/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.1000/0.1000] \n",
            "Inner Fold [1/3] train corr: 0.2943, valid corr: 0.1384\n",
            "Execution Time for Fold: 31.50 mins\n",
            "\n",
            "=================================== Inner Fold [2/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1511, Valid corr: 0.1017\n",
            "Layer 1: [0.9153/0.9750] Layer 2: [0.5003/0.5000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [100/150] Train corr: 0.2346, Valid corr: 0.1299\n",
            "Layer 1: [0.9753/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [150/150] Train corr: 0.3035, Valid corr: 0.1254\n",
            "Layer 1: [0.9752/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.1000/0.1000] \n",
            "Inner Fold [2/3] train corr: 0.3035, valid corr: 0.1254\n",
            "Execution Time for Fold: 32.68 mins\n",
            "\n",
            "=================================== Inner Fold [3/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.2070, Valid corr: 0.1164\n",
            "Layer 1: [0.9426/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [100/150] Train corr: 0.2603, Valid corr: 0.1362\n",
            "Layer 1: [0.9753/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [150/150] Train corr: 0.3076, Valid corr: 0.1372\n",
            "Layer 1: [0.9748/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.1000/0.1000] \n",
            "Inner Fold [3/3] train corr: 0.3076, valid corr: 0.1372\n",
            "Execution Time for Fold: 34.02 mins\n",
            "\n",
            "Param Cand: [4/9] train corr: 0.3018, valid corr: 0.1337\n",
            "\n",
            "=================================== Param Cand [5/9] ===================================\n",
            "Param: 1_hsp_extr: 0.975 2_hsp_pred: 0.5 3_hsp_disc: 0.5 lambda_: 0.02 \n",
            "\n",
            "=================================== Inner Fold [1/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1707, Valid corr: 0.1194\n",
            "Layer 1: [0.9459/0.9750] Layer 2: [0.4999/0.5000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [100/150] Train corr: 0.2359, Valid corr: 0.1223\n",
            "Layer 1: [0.9747/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [150/150] Train corr: 0.2790, Valid corr: 0.1304\n",
            "Layer 1: [0.9749/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.5000/0.5000] \n",
            "Inner Fold [1/3] train corr: 0.2790, valid corr: 0.1304\n",
            "Execution Time for Fold: 35.01 mins\n",
            "\n",
            "=================================== Inner Fold [2/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1596, Valid corr: 0.1204\n",
            "Layer 1: [0.9128/0.9750] Layer 2: [0.4996/0.5000] Layer 3: [0.5005/0.5000] \n",
            "Epoch [100/150] Train corr: 0.2542, Valid corr: 0.1074\n",
            "Layer 1: [0.9752/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [150/150] Train corr: 0.3387, Valid corr: 0.1086\n",
            "Layer 1: [0.9752/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.5000/0.5000] \n",
            "Inner Fold [2/3] train corr: 0.3387, valid corr: 0.1086\n",
            "Execution Time for Fold: 36.01 mins\n",
            "\n",
            "=================================== Inner Fold [3/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.2323, Valid corr: 0.1297\n",
            "Layer 1: [0.9342/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [100/150] Train corr: 0.3294, Valid corr: 0.1482\n",
            "Layer 1: [0.9749/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [150/150] Train corr: 0.3668, Valid corr: 0.1489\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.5000/0.5000] \n",
            "Inner Fold [3/3] train corr: 0.3668, valid corr: 0.1489\n",
            "Execution Time for Fold: 37.66 mins\n",
            "\n",
            "Param Cand: [5/9] train corr: 0.3282, valid corr: 0.1293\n",
            "\n",
            "=================================== Param Cand [6/9] ===================================\n",
            "Param: 1_hsp_extr: 0.975 2_hsp_pred: 0.5 3_hsp_disc: 0.9 lambda_: 0.02 \n",
            "\n",
            "=================================== Inner Fold [1/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.2274, Valid corr: 0.1423\n",
            "Layer 1: [0.9261/0.9750] Layer 2: [0.4999/0.5000] Layer 3: [0.8999/0.9000] \n",
            "Epoch [100/150] Train corr: 0.3022, Valid corr: 0.1405\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.9000/0.9000] \n",
            "Epoch [150/150] Train corr: 0.3583, Valid corr: 0.1480\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.9000/0.9000] \n",
            "Inner Fold [1/3] train corr: 0.3583, valid corr: 0.1480\n",
            "Execution Time for Fold: 38.50 mins\n",
            "\n",
            "=================================== Inner Fold [2/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1621, Valid corr: 0.1140\n",
            "Layer 1: [0.9136/0.9750] Layer 2: [0.5001/0.5000] Layer 3: [0.8906/0.9000] \n",
            "Epoch [100/150] Train corr: 0.2672, Valid corr: 0.1174\n",
            "Layer 1: [0.9748/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.8920/0.9000] \n",
            "Epoch [150/150] Train corr: 0.3338, Valid corr: 0.1163\n",
            "Layer 1: [0.9748/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.8922/0.9000] \n",
            "Inner Fold [2/3] train corr: 0.3338, valid corr: 0.1163\n",
            "Execution Time for Fold: 39.51 mins\n",
            "\n",
            "=================================== Inner Fold [3/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1331, Valid corr: 0.0993\n",
            "Layer 1: [0.9169/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.8906/0.9000] \n",
            "Epoch [100/150] Train corr: 0.2580, Valid corr: 0.1077\n",
            "Layer 1: [0.9753/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.8930/0.9000] \n",
            "Epoch [150/150] Train corr: 0.3068, Valid corr: 0.1151\n",
            "Layer 1: [0.9749/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.8932/0.9000] \n",
            "Inner Fold [3/3] train corr: 0.3068, valid corr: 0.1151\n",
            "Execution Time for Fold: 40.83 mins\n",
            "\n",
            "Param Cand: [6/9] train corr: 0.3330, valid corr: 0.1265\n",
            "\n",
            "=================================== Param Cand [7/9] ===================================\n",
            "Param: 1_hsp_extr: 0.975 2_hsp_pred: 0.9 3_hsp_disc: 0.1 lambda_: 0.02 \n",
            "\n",
            "=================================== Inner Fold [1/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1989, Valid corr: 0.1401\n",
            "Layer 1: [0.9389/0.9750] Layer 2: [0.9001/0.9000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [100/150] Train corr: 0.3041, Valid corr: 0.1542\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.9000/0.9000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [150/150] Train corr: 0.3567, Valid corr: 0.1548\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.8999/0.9000] Layer 3: [0.1000/0.1000] \n",
            "Inner Fold [1/3] train corr: 0.3567, valid corr: 0.1548\n",
            "Execution Time for Fold: 41.68 mins\n",
            "\n",
            "=================================== Inner Fold [2/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.2173, Valid corr: 0.1161\n",
            "Layer 1: [0.9508/0.9750] Layer 2: [0.8884/0.9000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [100/150] Train corr: 0.2981, Valid corr: 0.1190\n",
            "Layer 1: [0.9752/0.9750] Layer 2: [0.8895/0.9000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [150/150] Train corr: 0.3293, Valid corr: 0.1179\n",
            "Layer 1: [0.9749/0.9750] Layer 2: [0.8897/0.9000] Layer 3: [0.1000/0.1000] \n",
            "Inner Fold [2/3] train corr: 0.3293, valid corr: 0.1179\n",
            "Execution Time for Fold: 42.89 mins\n",
            "\n",
            "=================================== Inner Fold [3/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1588, Valid corr: 0.1081\n",
            "Layer 1: [0.8861/0.9750] Layer 2: [0.9000/0.9000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [100/150] Train corr: 0.3400, Valid corr: 0.1296\n",
            "Layer 1: [0.9749/0.9750] Layer 2: [0.9001/0.9000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [150/150] Train corr: 0.3732, Valid corr: 0.1365\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.9000/0.9000] Layer 3: [0.1000/0.1000] \n",
            "Inner Fold [3/3] train corr: 0.3732, valid corr: 0.1365\n",
            "Execution Time for Fold: 44.64 mins\n",
            "\n",
            "Param Cand: [7/9] train corr: 0.3531, valid corr: 0.1364\n",
            "\n",
            "=================================== Param Cand [8/9] ===================================\n",
            "Param: 1_hsp_extr: 0.975 2_hsp_pred: 0.9 3_hsp_disc: 0.5 lambda_: 0.02 \n",
            "\n",
            "=================================== Inner Fold [1/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1837, Valid corr: 0.1459\n",
            "Layer 1: [0.9316/0.9750] Layer 2: [0.8990/0.9000] Layer 3: [0.5001/0.5000] \n",
            "Epoch [100/150] Train corr: 0.2989, Valid corr: 0.1548\n",
            "Layer 1: [0.9749/0.9750] Layer 2: [0.8999/0.9000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [150/150] Train corr: 0.3678, Valid corr: 0.1541\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.9000/0.9000] Layer 3: [0.5000/0.5000] \n",
            "Inner Fold [1/3] train corr: 0.3678, valid corr: 0.1541\n",
            "Execution Time for Fold: 45.33 mins\n",
            "\n",
            "=================================== Inner Fold [2/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1611, Valid corr: 0.1065\n",
            "Layer 1: [0.8895/0.9750] Layer 2: [0.9004/0.9000] Layer 3: [0.5002/0.5000] \n",
            "Epoch [100/150] Train corr: 0.3646, Valid corr: 0.1193\n",
            "Layer 1: [0.9747/0.9750] Layer 2: [0.8999/0.9000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [150/150] Train corr: 0.4092, Valid corr: 0.1194\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.9000/0.9000] Layer 3: [0.5000/0.5000] \n",
            "Inner Fold [2/3] train corr: 0.4092, valid corr: 0.1194\n",
            "Execution Time for Fold: 46.74 mins\n",
            "\n",
            "=================================== Inner Fold [3/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1471, Valid corr: 0.1226\n",
            "Layer 1: [0.8911/0.9750] Layer 2: [0.8991/0.9000] Layer 3: [0.4996/0.5000] \n",
            "Epoch [100/150] Train corr: 0.3625, Valid corr: 0.1497\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.9000/0.9000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [150/150] Train corr: 0.3959, Valid corr: 0.1525\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.9000/0.9000] Layer 3: [0.5000/0.5000] \n",
            "Inner Fold [3/3] train corr: 0.3959, valid corr: 0.1525\n",
            "Execution Time for Fold: 48.68 mins\n",
            "\n",
            "Param Cand: [8/9] train corr: 0.3909, valid corr: 0.1420\n",
            "\n",
            "=================================== Param Cand [9/9] ===================================\n",
            "Param: 1_hsp_extr: 0.975 2_hsp_pred: 0.9 3_hsp_disc: 0.9 lambda_: 0.02 \n",
            "\n",
            "=================================== Inner Fold [1/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.2394, Valid corr: 0.1433\n",
            "Layer 1: [0.9390/0.9750] Layer 2: [0.8999/0.9000] Layer 3: [0.9000/0.9000] \n",
            "Epoch [100/150] Train corr: 0.3362, Valid corr: 0.1381\n",
            "Layer 1: [0.9752/0.9750] Layer 2: [0.8999/0.9000] Layer 3: [0.9000/0.9000] \n",
            "Epoch [150/150] Train corr: 0.3778, Valid corr: 0.1429\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.8999/0.9000] Layer 3: [0.8999/0.9000] \n",
            "Inner Fold [1/3] train corr: 0.3778, valid corr: 0.1429\n",
            "Execution Time for Fold: 53.20 mins\n",
            "\n",
            "=================================== Inner Fold [2/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1712, Valid corr: 0.1141\n",
            "Layer 1: [0.9191/0.9750] Layer 2: [0.8901/0.9000] Layer 3: [0.8888/0.9000] \n",
            "Epoch [100/150] Train corr: 0.2745, Valid corr: 0.1177\n",
            "Layer 1: [0.9748/0.9750] Layer 2: [0.8915/0.9000] Layer 3: [0.8904/0.9000] \n",
            "Epoch [150/150] Train corr: 0.3421, Valid corr: 0.1203\n",
            "Layer 1: [0.9752/0.9750] Layer 2: [0.8916/0.9000] Layer 3: [0.8906/0.9000] \n",
            "Inner Fold [2/3] train corr: 0.3421, valid corr: 0.1203\n",
            "Execution Time for Fold: 54.60 mins\n",
            "\n",
            "=================================== Inner Fold [3/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1575, Valid corr: 0.1183\n",
            "Layer 1: [0.9063/0.9750] Layer 2: [0.8993/0.9000] Layer 3: [0.8976/0.9000] \n",
            "Epoch [100/150] Train corr: 0.3121, Valid corr: 0.1393\n",
            "Layer 1: [0.9748/0.9750] Layer 2: [0.9000/0.9000] Layer 3: [0.8999/0.9000] \n",
            "Epoch [150/150] Train corr: 0.3816, Valid corr: 0.1395\n",
            "Layer 1: [0.9749/0.9750] Layer 2: [0.9000/0.9000] Layer 3: [0.9000/0.9000] \n",
            "Inner Fold [3/3] train corr: 0.3816, valid corr: 0.1395\n",
            "Execution Time for Fold: 56.06 mins\n",
            "\n",
            "Param Cand: [9/9] train corr: 0.3671, valid corr: 0.1342\n",
            "Selected param: 0.975 0.9 0.5 \n",
            "=================================== Outer Fold [17/18] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.2099, Test corr: 0.1542\n",
            "Layer 1: [0.9282/0.9750] Layer 2: [0.8991/0.9000] Layer 3: [0.5001/0.5000] \n",
            "Epoch [100/150] Train corr: 0.3241, Test corr: 0.1628\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.9000/0.9000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [150/150] Train corr: 0.3339, Test corr: 0.1640\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.8999/0.9000] Layer 3: [0.5000/0.5000] \n",
            "Execution Time for Fold: 58.11 mins\n",
            "\n",
            "Outer Fold [17/18]: train corr: 0.3339, valid corr: 0.1640\n",
            "\n",
            "=================================== Outer Fold [18/18] ===================================\n",
            "\n",
            "=================================== Param Cand [1/9] ===================================\n",
            "Param: 1_hsp_extr: 0.975 2_hsp_pred: 0.1 3_hsp_disc: 0.1 lambda_: 0.02 \n",
            "\n",
            "=================================== Inner Fold [1/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.2177, Valid corr: 0.1322\n",
            "Layer 1: [0.9372/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [100/150] Train corr: 0.3186, Valid corr: 0.1265\n",
            "Layer 1: [0.9748/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [150/150] Train corr: 0.3544, Valid corr: 0.1309\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.1000/0.1000] \n",
            "Inner Fold [1/3] train corr: 0.3544, valid corr: 0.1309\n",
            "Execution Time for Fold: 55.68 mins\n",
            "\n",
            "=================================== Inner Fold [2/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1424, Valid corr: 0.1470\n",
            "Layer 1: [0.9110/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [100/150] Train corr: 0.2829, Valid corr: 0.1434\n",
            "Layer 1: [0.9752/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [150/150] Train corr: 0.3211, Valid corr: 0.1426\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.1000/0.1000] \n",
            "Inner Fold [2/3] train corr: 0.3211, valid corr: 0.1426\n",
            "Execution Time for Fold: 56.53 mins\n",
            "\n",
            "=================================== Inner Fold [3/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1760, Valid corr: 0.1119\n",
            "Layer 1: [0.8930/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [100/150] Train corr: 0.3129, Valid corr: 0.1352\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [150/150] Train corr: 0.3563, Valid corr: 0.1397\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.1000/0.1000] \n",
            "Inner Fold [3/3] train corr: 0.3563, valid corr: 0.1397\n",
            "Execution Time for Fold: 62.44 mins\n",
            "\n",
            "Param Cand: [1/9] train corr: 0.3440, valid corr: 0.1377\n",
            "\n",
            "=================================== Param Cand [2/9] ===================================\n",
            "Param: 1_hsp_extr: 0.975 2_hsp_pred: 0.1 3_hsp_disc: 0.5 lambda_: 0.02 \n",
            "\n",
            "=================================== Inner Fold [1/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1538, Valid corr: 0.1176\n",
            "Layer 1: [0.9109/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.4995/0.5000] \n",
            "Epoch [100/150] Train corr: 0.2796, Valid corr: 0.1397\n",
            "Layer 1: [0.9747/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [150/150] Train corr: 0.3219, Valid corr: 0.1417\n",
            "Layer 1: [0.9752/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.5000/0.5000] \n",
            "Inner Fold [1/3] train corr: 0.3219, valid corr: 0.1417\n",
            "Execution Time for Fold: 59.35 mins\n",
            "\n",
            "=================================== Inner Fold [2/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1708, Valid corr: 0.1237\n",
            "Layer 1: [0.9148/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.4995/0.5000] \n",
            "Epoch [100/150] Train corr: 0.2677, Valid corr: 0.1428\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [150/150] Train corr: 0.3357, Valid corr: 0.1448\n",
            "Layer 1: [0.9748/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.5000/0.5000] \n",
            "Inner Fold [2/3] train corr: 0.3357, valid corr: 0.1448\n",
            "Execution Time for Fold: 63.58 mins\n",
            "\n",
            "=================================== Inner Fold [3/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.2283, Valid corr: 0.1127\n",
            "Layer 1: [0.9301/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.5001/0.5000] \n",
            "Epoch [100/150] Train corr: 0.3005, Valid corr: 0.1276\n",
            "Layer 1: [0.9749/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [150/150] Train corr: 0.3460, Valid corr: 0.1330\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.5000/0.5000] \n",
            "Inner Fold [3/3] train corr: 0.3460, valid corr: 0.1330\n",
            "Execution Time for Fold: 63.80 mins\n",
            "\n",
            "Param Cand: [2/9] train corr: 0.3346, valid corr: 0.1399\n",
            "\n",
            "=================================== Param Cand [3/9] ===================================\n",
            "Param: 1_hsp_extr: 0.975 2_hsp_pred: 0.1 3_hsp_disc: 0.9 lambda_: 0.02 \n",
            "\n",
            "=================================== Inner Fold [1/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1665, Valid corr: 0.1281\n",
            "Layer 1: [0.8969/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.8995/0.9000] \n",
            "Epoch [100/150] Train corr: 0.3093, Valid corr: 0.1276\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.9000/0.9000] \n",
            "Epoch [150/150] Train corr: 0.3758, Valid corr: 0.1301\n",
            "Layer 1: [0.9748/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.9000/0.9000] \n",
            "Inner Fold [1/3] train corr: 0.3758, valid corr: 0.1301\n",
            "Execution Time for Fold: 63.11 mins\n",
            "\n",
            "=================================== Inner Fold [2/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.2072, Valid corr: 0.1345\n",
            "Layer 1: [0.9392/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.8887/0.9000] \n",
            "Epoch [100/150] Train corr: 0.2629, Valid corr: 0.1392\n",
            "Layer 1: [0.9748/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.8900/0.9000] \n",
            "Epoch [150/150] Train corr: 0.3106, Valid corr: 0.1449\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.8901/0.9000] \n",
            "Inner Fold [2/3] train corr: 0.3106, valid corr: 0.1449\n",
            "Execution Time for Fold: 65.11 mins\n",
            "\n",
            "=================================== Inner Fold [3/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1507, Valid corr: 0.1000\n",
            "Layer 1: [0.8993/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.8986/0.9000] \n",
            "Epoch [100/150] Train corr: 0.3080, Valid corr: 0.1366\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.8999/0.9000] \n",
            "Epoch [150/150] Train corr: 0.3487, Valid corr: 0.1391\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.1000/0.1000] Layer 3: [0.9000/0.9000] \n",
            "Inner Fold [3/3] train corr: 0.3487, valid corr: 0.1391\n",
            "Execution Time for Fold: 66.82 mins\n",
            "\n",
            "Param Cand: [3/9] train corr: 0.3450, valid corr: 0.1380\n",
            "\n",
            "=================================== Param Cand [4/9] ===================================\n",
            "Param: 1_hsp_extr: 0.975 2_hsp_pred: 0.5 3_hsp_disc: 0.1 lambda_: 0.02 \n",
            "\n",
            "=================================== Inner Fold [1/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1526, Valid corr: 0.1285\n",
            "Layer 1: [0.9076/0.9750] Layer 2: [0.4995/0.5000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [100/150] Train corr: 0.2793, Valid corr: 0.1338\n",
            "Layer 1: [0.9752/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [150/150] Train corr: 0.3211, Valid corr: 0.1338\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.1000/0.1000] \n",
            "Inner Fold [1/3] train corr: 0.3211, valid corr: 0.1338\n",
            "Execution Time for Fold: 67.61 mins\n",
            "\n",
            "=================================== Inner Fold [2/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1603, Valid corr: 0.1173\n",
            "Layer 1: [0.9135/0.9750] Layer 2: [0.5005/0.5000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [100/150] Train corr: 0.2831, Valid corr: 0.1420\n",
            "Layer 1: [0.9747/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [150/150] Train corr: 0.3281, Valid corr: 0.1408\n",
            "Layer 1: [0.9749/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.1000/0.1000] \n",
            "Inner Fold [2/3] train corr: 0.3281, valid corr: 0.1408\n",
            "Execution Time for Fold: 67.29 mins\n",
            "\n",
            "=================================== Inner Fold [3/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1578, Valid corr: 0.1281\n",
            "Layer 1: [0.8938/0.9750] Layer 2: [0.4999/0.5000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [100/150] Train corr: 0.3194, Valid corr: 0.1332\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [150/150] Train corr: 0.3665, Valid corr: 0.1368\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.1000/0.1000] \n",
            "Inner Fold [3/3] train corr: 0.3665, valid corr: 0.1368\n",
            "Execution Time for Fold: 69.11 mins\n",
            "\n",
            "Param Cand: [4/9] train corr: 0.3385, valid corr: 0.1371\n",
            "\n",
            "=================================== Param Cand [5/9] ===================================\n",
            "Param: 1_hsp_extr: 0.975 2_hsp_pred: 0.5 3_hsp_disc: 0.5 lambda_: 0.02 \n",
            "\n",
            "=================================== Inner Fold [1/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1724, Valid corr: 0.1267\n",
            "Layer 1: [0.8955/0.9750] Layer 2: [0.4997/0.5000] Layer 3: [0.4996/0.5000] \n",
            "Epoch [100/150] Train corr: 0.3341, Valid corr: 0.1457\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [150/150] Train corr: 0.3832, Valid corr: 0.1468\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.5000/0.5000] \n",
            "Inner Fold [1/3] train corr: 0.3832, valid corr: 0.1468\n",
            "Execution Time for Fold: 68.72 mins\n",
            "\n",
            "=================================== Inner Fold [2/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1588, Valid corr: 0.1303\n",
            "Layer 1: [0.9083/0.9750] Layer 2: [0.5002/0.5000] Layer 3: [0.4995/0.5000] \n",
            "Epoch [100/150] Train corr: 0.3036, Valid corr: 0.1414\n",
            "Layer 1: [0.9748/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [150/150] Train corr: 0.3435, Valid corr: 0.1449\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.5000/0.5000] \n",
            "Inner Fold [2/3] train corr: 0.3435, valid corr: 0.1449\n",
            "Execution Time for Fold: 69.01 mins\n",
            "\n",
            "=================================== Inner Fold [3/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1993, Valid corr: 0.1062\n",
            "Layer 1: [0.9406/0.9750] Layer 2: [0.4999/0.5000] Layer 3: [0.4999/0.5000] \n",
            "Epoch [100/150] Train corr: 0.2486, Valid corr: 0.1188\n",
            "Layer 1: [0.9752/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.5000/0.5000] \n",
            "Epoch [150/150] Train corr: 0.3090, Valid corr: 0.1235\n",
            "Layer 1: [0.9749/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.5000/0.5000] \n",
            "Inner Fold [3/3] train corr: 0.3090, valid corr: 0.1235\n",
            "Execution Time for Fold: 68.74 mins\n",
            "\n",
            "Param Cand: [5/9] train corr: 0.3452, valid corr: 0.1384\n",
            "\n",
            "=================================== Param Cand [6/9] ===================================\n",
            "Param: 1_hsp_extr: 0.975 2_hsp_pred: 0.5 3_hsp_disc: 0.9 lambda_: 0.02 \n",
            "\n",
            "=================================== Inner Fold [1/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.2384, Valid corr: 0.1371\n",
            "Layer 1: [0.9394/0.9750] Layer 2: [0.4999/0.5000] Layer 3: [0.8962/0.9000] \n",
            "Epoch [100/150] Train corr: 0.2899, Valid corr: 0.1473\n",
            "Layer 1: [0.9748/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.8974/0.9000] \n",
            "Epoch [150/150] Train corr: 0.3440, Valid corr: 0.1520\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.8976/0.9000] \n",
            "Inner Fold [1/3] train corr: 0.3440, valid corr: 0.1520\n",
            "Execution Time for Fold: 68.36 mins\n",
            "\n",
            "=================================== Inner Fold [2/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1680, Valid corr: 0.1373\n",
            "Layer 1: [0.9113/0.9750] Layer 2: [0.5004/0.5000] Layer 3: [0.8906/0.9000] \n",
            "Epoch [100/150] Train corr: 0.2759, Valid corr: 0.1388\n",
            "Layer 1: [0.9749/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.8924/0.9000] \n",
            "Epoch [150/150] Train corr: 0.3449, Valid corr: 0.1405\n",
            "Layer 1: [0.9748/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.8926/0.9000] \n",
            "Inner Fold [2/3] train corr: 0.3449, valid corr: 0.1405\n",
            "Execution Time for Fold: 69.07 mins\n",
            "\n",
            "=================================== Inner Fold [3/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.2159, Valid corr: 0.1384\n",
            "Layer 1: [0.9296/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.9000/0.9000] \n",
            "Epoch [100/150] Train corr: 0.2926, Valid corr: 0.1417\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.9000/0.9000] \n",
            "Epoch [150/150] Train corr: 0.3386, Valid corr: 0.1465\n",
            "Layer 1: [0.9748/0.9750] Layer 2: [0.5000/0.5000] Layer 3: [0.9000/0.9000] \n",
            "Inner Fold [3/3] train corr: 0.3386, valid corr: 0.1465\n",
            "Execution Time for Fold: 68.95 mins\n",
            "\n",
            "Param Cand: [6/9] train corr: 0.3425, valid corr: 0.1463\n",
            "\n",
            "=================================== Param Cand [7/9] ===================================\n",
            "Param: 1_hsp_extr: 0.975 2_hsp_pred: 0.9 3_hsp_disc: 0.1 lambda_: 0.02 \n",
            "\n",
            "=================================== Inner Fold [1/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1619, Valid corr: 0.1201\n",
            "Layer 1: [0.9138/0.9750] Layer 2: [0.8928/0.9000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [100/150] Train corr: 0.2736, Valid corr: 0.1321\n",
            "Layer 1: [0.9750/0.9750] Layer 2: [0.8943/0.9000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [150/150] Train corr: 0.3342, Valid corr: 0.1369\n",
            "Layer 1: [0.9748/0.9750] Layer 2: [0.8944/0.9000] Layer 3: [0.1000/0.1000] \n",
            "Inner Fold [1/3] train corr: 0.3342, valid corr: 0.1369\n",
            "Execution Time for Fold: 70.51 mins\n",
            "\n",
            "=================================== Inner Fold [2/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1864, Valid corr: 0.1460\n",
            "Layer 1: [0.9423/0.9750] Layer 2: [0.8890/0.9000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [100/150] Train corr: 0.2641, Valid corr: 0.1571\n",
            "Layer 1: [0.9749/0.9750] Layer 2: [0.8901/0.9000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [150/150] Train corr: 0.3201, Valid corr: 0.1567\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.8903/0.9000] Layer 3: [0.1000/0.1000] \n",
            "Inner Fold [2/3] train corr: 0.3201, valid corr: 0.1567\n",
            "Execution Time for Fold: 77.14 mins\n",
            "\n",
            "=================================== Inner Fold [3/3] ===================================\n",
            "\n",
            "Epoch [50/150] Train corr: 0.1530, Valid corr: 0.0929\n",
            "Layer 1: [0.9018/0.9750] Layer 2: [0.9001/0.9000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [100/150] Train corr: 0.3366, Valid corr: 0.1287\n",
            "Layer 1: [0.9751/0.9750] Layer 2: [0.8999/0.9000] Layer 3: [0.1000/0.1000] \n",
            "Epoch [150/150] Train corr: 0.3840, Valid corr: 0.1313\n",
            "Layer 1: [0.9749/0.9750] Layer 2: [0.8999/0.9000] Layer 3: [0.1000/0.1000] \n",
            "Inner Fold [3/3] train corr: 0.3840, valid corr: 0.1313\n",
            "Execution Time for Fold: 74.82 mins\n",
            "\n",
            "Param Cand: [7/9] train corr: 0.3461, valid corr: 0.1416\n",
            "\n",
            "=================================== Param Cand [8/9] ===================================\n",
            "Param: 1_hsp_extr: 0.975 2_hsp_pred: 0.9 3_hsp_disc: 0.5 lambda_: 0.02 \n",
            "\n",
            "=================================== Inner Fold [1/3] ===================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "[Errno 12] Cannot allocate memory",
          "traceback": [
            "\u001b[0;31m-----------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-3cf11e5db395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_save_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0minner_train_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_valid_corr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_inner_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_save_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_tg_hsp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0minner_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minner_train_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_valid_corr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-e52daefeb305>\u001b[0m in \u001b[0;36mrun_inner_fold\u001b[0;34m(output_save_dir, cur_tg_hsp)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minner_train_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mhsp_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhsp_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcur_tg_hsp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             )\n\u001b[1;32m     60\u001b[0m             valid_clf_loss, valid_reg_loss, valid_acc, valid_corr = valid(\n",
            "\u001b[0;32m<ipython-input-29-659f45ddeaa7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epoch, train_loader, optimizer, criterion_clf, criterion_reg, hsp_val, beta_val, hsp_list, beta_list, tg_hsp)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/envs/tf-gpu/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/envs/tf-gpu/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/envs/tf-gpu/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    912\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/envs/tf-gpu/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/envs/tf-gpu/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/envs/tf-gpu/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/envs/tf-gpu/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush_std_streams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/envs/tf-gpu/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0oYBNweqV2l"
      },
      "source": [
        "code_tot_time = time.time() - code_start_time \n",
        "print(\"Execution Time for the training: {:.2f} hours\".format(code_tot_time / 60 / 60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4xBPE8jqV2l"
      },
      "source": [
        "# May be 40 hours --> two days..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjqHpI8BqV2l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}