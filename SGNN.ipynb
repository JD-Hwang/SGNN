{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scanner-Generalization Neural Networks (SGNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from decimal import Decimal\n",
    "from datetime import datetime as dt\n",
    "from pytz import timezone\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable, Function\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning GPU for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Controling seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed parameters for leave-one-site-out cross-validation (LOSOCV; $n_{test} = 18$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_outer_cv = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
    "n_inner_valid = 5\n",
    "output_folder = \"SGNN\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = np.load(\"RSFC_p_factor_ABCD.npz\")\n",
    "X = data[\"X\"]\n",
    "y = data[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_factor_idx = 0\n",
    "site_idx = 1\n",
    "scanner_idx = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing LOSOCV by spliting data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spliting subject indices for leave-one-site-out validation set from two sites\n",
    "site_unq = np.unique(y[:, site_idx])\n",
    "data_idx = np.arange(y.shape[0])\n",
    "\n",
    "outer_train_folds_idx = []\n",
    "outer_test_folds_idx = []\n",
    "inner_folds_idx = []\n",
    "\n",
    "split_seed = 1000\n",
    "n_outer_repeat = len(site_unq)\n",
    "n_inner_repeat_list = []\n",
    "\n",
    "# Outer loop\n",
    "for n_outer, outer_test_site in enumerate(site_unq):\n",
    "    outer_train_idx = np.where(y[:, site_idx] != outer_test_site)[0]\n",
    "    outer_test_idx = np.where(y[:, site_idx] == outer_test_site)[0]\n",
    "    outer_train_folds_idx.append(outer_train_idx)\n",
    "    outer_test_folds_idx.append(outer_test_idx)\n",
    "    \n",
    "    outer_test_df = y_df.iloc[outer_test_idx]\n",
    "    outer_train_df = y_df.iloc[outer_train_idx]\n",
    "    valid_0_df = outer_train_df[outer_train_df[\"scanner\"] == 0]\n",
    "    valid_1_df = outer_train_df[outer_train_df[\"scanner\"] == 1]\n",
    "    valid_0_site_unq = pd.Series(np.unique(valid_0_df[\"site\"]))\n",
    "    valid_1_site_unq = pd.Series(np.unique(valid_1_df[\"site\"]))\n",
    "\n",
    "    inner_train_folds_idx = []\n",
    "    inner_valid_folds_idx = []\n",
    "    \n",
    "    sel_val_seed = n_outer + split_seed\n",
    "    inner_valid_site_list = list(\n",
    "        itertools.product(valid_0_site_unq.values, valid_1_site_unq.values)\n",
    "    )\n",
    "    if n_inner_valid is None:\n",
    "        n_inner_repeat = len(inner_valid_site_list) # Full combination\n",
    "    else:\n",
    "        n_inner_repeat = n_inner_valid\n",
    "    n_inner_repeat_list.append(n_inner_repeat)\n",
    "    \n",
    "    inner_valid_site_df = pd.DataFrame(inner_valid_site_list, columns=[\"SI\", \"GE\"]).sample(\n",
    "        n=n_inner_repeat, replace=False, random_state=sel_val_seed\n",
    "    )    \n",
    "    # Inner loop\n",
    "    for n_inner in range(n_inner_repeat):\n",
    "        inner_valid_site_0 = inner_valid_site_df[\"SI\"].values[n_inner]\n",
    "        inner_valid_site_1 = inner_valid_site_df[\"GE\"].values[n_inner]\n",
    "        inner_valid_site = [inner_valid_site_0, inner_valid_site_1]\n",
    "        inner_valid_cond_0 = (outer_train_df[\"site\"] == inner_valid_site_0)\n",
    "        inner_valid_cond_1 = (outer_train_df[\"site\"] == inner_valid_site_1)\n",
    "        inner_valid_df = outer_train_df[inner_valid_cond_0 | inner_valid_cond_1]\n",
    "\n",
    "        inner_train_idx = np.setdiff1d(\n",
    "            outer_train_df.index.values, inner_valid_df.index.values)\n",
    "        inner_valid_idx = inner_valid_df.index.values\n",
    "        print(\"[{}/{}] inner fold: train: {}, valid: {}\".\n",
    "              format(n_inner + 1, n_inner_repeat, len(inner_train_idx), len(inner_valid_idx)),\n",
    "              end=\", \")\n",
    "        print(\"valid site: {}, {}\".format(int(inner_valid_site[0]), int(inner_valid_site[1])))\n",
    "        inner_train_folds_idx.append(inner_train_idx)\n",
    "        inner_valid_folds_idx.append(inner_valid_idx)\n",
    "        \n",
    "    inner_folds_idx.append([inner_train_folds_idx, inner_valid_folds_idx])\n",
    "    \n",
    "    outer_test_scnr_label = np.unique(outer_test_df[\"scanner\"])\n",
    "    outer_train_scnr_label = np.unique(outer_train_df[\"scanner\"])\n",
    "    inner_valid_scnr_label = np.unique(inner_valid_df[\"scanner\"])\n",
    "    \n",
    "    print(\"[{}/{}] outer fold: train: {}, test: {}\"\n",
    "          .format(n_outer + 1, len(site_unq), len(outer_train_idx), len(outer_test_idx)), \n",
    "          end=\" --> \")\n",
    "    print(\"outer test site: {}\\n\".format(int(outer_test_site)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"max\"\n",
    "lr_patience = 5\n",
    "min_lr = 1e-08\n",
    "lr_alpha = -1.5\n",
    "lr_beta = 1.7\n",
    "\n",
    "swa_lr = 5e-03\n",
    "momentum = 0.90\n",
    "l1_param = 0\n",
    "early_stopping_patience = 150\n",
    "\n",
    "input_dim = 61776\n",
    "n_classes = len(np.unique(y[:, scanner_idx]))\n",
    "output_prd_dim = 1\n",
    "output_dsc_dim = n_classes\n",
    "\n",
    "wsc_flag = [1, 1, 1]\n",
    "beta_lr = [1e-04, 1e-03, 1e-03]\n",
    "max_beta = [1e-02, 5e-02, 5e-02]\n",
    "n_wsc = wsc_flag.count(1)\n",
    "\n",
    "outer_n_splits = n_outer_repeat\n",
    "inner_n_splits = n_inner_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "class TrainDataset(Dataset): \n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        X_train = torch.from_numpy(self.X_train[idx]).type(torch.FloatTensor)\n",
    "        y_train = torch.from_numpy(self.y_train[idx]).type(torch.FloatTensor)\n",
    "\n",
    "        return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "class ValidDataset(Dataset): \n",
    "    def __init__(self, X_valid, y_valid):\n",
    "        self.X_valid = X_valid\n",
    "        self.y_valid = y_valid\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_valid)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        X_valid = torch.from_numpy(self.X_valid[idx]).type(torch.FloatTensor)\n",
    "        y_valid = torch.from_numpy(self.y_valid[idx]).type(torch.FloatTensor)\n",
    "        \n",
    "        return X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "class TestDataset(Dataset): \n",
    "    def __init__(self, X_test, y_test):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_test)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        X_test = torch.from_numpy(self.X_test[idx]).type(torch.FloatTensor)\n",
    "        y_test = torch.from_numpy(self.y_test[idx]).type(torch.FloatTensor)\n",
    "        \n",
    "        return X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for gradient reversal layer (Ganin et al., 2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradRevFunc(Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.clone()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grads):\n",
    "        lambda_ = ctx.lambda_\n",
    "        lambda_ = grads.new_tensor(lambda_)\n",
    "        dx = lambda_ * grads.neg()\n",
    "        return dx, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradRev(torch.nn.Module):\n",
    "    def __init__(self, lambda_=0.0):\n",
    "        super(GradRev, self).__init__()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradRevFunc.apply(x, self.lambda_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining SGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNN(nn.Module):\n",
    "    def __init__(self, fe_hidden, prd_hidden, dsc_hidden,\n",
    "                 dropout_fe, dropout_prd, dropout_dsc, act_func_name, lambda_):\n",
    "        super(SGNN, self).__init__()\n",
    "        self.fe_1 = nn.Linear(input_dim, fe_hidden)\n",
    "        self.fe_bn_1 = nn.BatchNorm1d(fe_hidden)\n",
    "        \n",
    "        self.prd_1 = nn.Linear(fe_hidden, prd_hidden)\n",
    "        self.prd_bn_1 = nn.BatchNorm1d(prd_hidden)\n",
    "        self.prd_2 = nn.Linear(prd_hidden, output_prd_dim)\n",
    "        \n",
    "        self.dsc_1 = nn.Linear(fe_hidden, dsc_hidden)\n",
    "        self.dsc_bn_1 = nn.BatchNorm1d(dsc_hidden)\n",
    "        self.dsc_2 = nn.Linear(dsc_hidden, output_dsc_dim)\n",
    "\n",
    "        self.dropout_fe = nn.Dropout(p=dropout_fe)\n",
    "        self.dropout_prd = nn.Dropout(p=dropout_prd)\n",
    "        self.dropout_dsc = nn.Dropout(p=dropout_dsc)\n",
    "        \n",
    "        self.act_func = get_act_func(act_func_name)\n",
    "        self.GradRev = GradRev(lambda_)\n",
    "        self.weights_init()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_ftr = self.fe_1(x)\n",
    "        x_ftr = self.fe_bn_1(x_ftr)\n",
    "        x_ftr = self.act_func(x_ftr)\n",
    "        x_ftr = self.dropout_fe(x_ftr)\n",
    "        \n",
    "        x_prd = self.prd_1(x_ftr)\n",
    "        x_prd = self.prd_bn_1(x_prd)\n",
    "        x_prd = self.act_func(x_prd)\n",
    "        x_prd = self.dropout_prd(x_prd)\n",
    "        x_prd = self.prd_2(x_prd)\n",
    "        \n",
    "        x_rev = self.GradRev(x_ftr)\n",
    "        x_dsc = self.dsc_1(x_rev)\n",
    "        x_dsc = self.dsc_bn_1(x_dsc)\n",
    "        x_dsc = self.act_func(x_dsc)\n",
    "        x_dsc = self.dropout_dsc(x_dsc)\n",
    "        x_dsc = self.dsc_2(x_dsc)\n",
    "        \n",
    "        return x_prd, x_dsc\n",
    "    \n",
    "    def weights_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "                nn.init.normal_(m.bias, std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, opt_name, learning_rate=None, l2_param=None):\n",
    "    lower_opt_name = opt_name.lower()\n",
    "    if lower_opt_name == 'momentum':\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate, \n",
    "                         momentum=momentum, weight_decay=l2_param)\n",
    "    elif lower_opt_name == 'nag':\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate, \n",
    "                         momentum=momentum, weight_decay=l2_param, nesterov=True)\n",
    "    elif lower_opt_name == 'adam':\n",
    "        return optim.Adam(model.parameters(), lr=learning_rate, \n",
    "                          weight_decay=l2_param)\n",
    "    else:\n",
    "        sys.exit(\"Illegal arguement for optimizer type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_act_func(act_func_name):\n",
    "    act_func_name = act_func_name.lower()\n",
    "    if act_func_name == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif act_func_name == 'prelu':\n",
    "        return nn.PReLU()\n",
    "    elif act_func_name == 'elu':\n",
    "        return nn.ELU()\n",
    "    elif act_func_name == 'silu':\n",
    "        return nn.SiLU()\n",
    "    elif act_func_name == 'leakyrelu':\n",
    "        return nn.LeakyReLU()\n",
    "    elif act_func_name == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        sys.exit(\"Illegal arguement for activation function type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for weight sparsity control with Hoyer's sparsness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hsp(n_wsc, epochs):\n",
    "    hsp_val = torch.zeros(n_wsc)\n",
    "    beta_val = torch.clone(hsp_val)\n",
    "    hsp_list = torch.zeros((n_wsc, epochs))\n",
    "    beta_list = torch.zeros((n_wsc, epochs))\n",
    "    \n",
    "    return hsp_val, beta_val, hsp_list, beta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight sparsity control with Hoyer's sparsness (Layer wise)\n",
    "def calc_hsp(w, beta, max_beta, beta_lr, tg_hsp):\n",
    "    \n",
    "    # Get value of weight\n",
    "    [dim, n_nodes] = w.shape\n",
    "    num_elements = dim * n_nodes\n",
    "    norm_ratio = torch.norm(w.detach(), 1) / torch.norm(w.detach(), 2)\n",
    "\n",
    "    # Calculate hoyer's sparsity level\n",
    "    num = math.sqrt(num_elements) - norm_ratio\n",
    "    den = math.sqrt(num_elements) - 1\n",
    "    hsp = torch.tensor(num / den).to(device)\n",
    "\n",
    "    # Update beta\n",
    "    beta = beta.clone() + beta_lr * torch.sign(torch.tensor(tg_hsp).to(device) - hsp)\n",
    "    \n",
    "    # Trim value\n",
    "    beta = 0 if beta < 0 else beta\n",
    "    beta = max_beta if beta > max_beta else beta\n",
    "\n",
    "    return [hsp, beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_l1(model, epoch, hsp_val, beta_val, hsp_list, beta_list, tg_hsp):\n",
    "    l1_reg = None\n",
    "    layer_idx = 0\n",
    "    wsc_idx = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name and \"bn\" not in name:\n",
    "            if \"fe\" in name or \"prd_1\" in name or \"dsc_1\" in name:\n",
    "                temp_w = param\n",
    "                \n",
    "                if wsc_flag[layer_idx] != 0:\n",
    "                    hsp_val[wsc_idx], beta_val[wsc_idx] = calc_hsp(\n",
    "                        temp_w, beta_val[wsc_idx], max_beta[wsc_idx], \n",
    "                        beta_lr[wsc_idx], tg_hsp[wsc_idx]\n",
    "                    )\n",
    "                    hsp_list[wsc_idx, epoch - 1] = hsp_val[wsc_idx]\n",
    "                    beta_list[wsc_idx, epoch - 1] = beta_val[wsc_idx]\n",
    "                    layer_reg = torch.norm(temp_w, 1) * beta_val[wsc_idx].clone()\n",
    "                    wsc_idx += 1\n",
    "                else:\n",
    "                    layer_reg = torch.norm(temp_w, 1) * l1_param\n",
    "\n",
    "                if l1_reg is None:\n",
    "                    l1_reg = layer_reg\n",
    "                else:\n",
    "                    l1_reg = l1_reg + layer_reg\n",
    "                layer_idx += 1\n",
    "        \n",
    "    return l1_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_pearsonr(x, y):\n",
    "    x_mean = torch.mean(x)\n",
    "    y_mean = torch.mean(y)\n",
    "    xx = x.sub(x_mean)\n",
    "    yy = y.sub(y_mean)\n",
    "    num = xx.dot(yy)\n",
    "    den = torch.norm(xx, 2) * torch.norm(yy, 2)\n",
    "    corr = num / den\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mae(x, y):\n",
    "    return torch.abs(x - y).mean().data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, train_loader, optimizer, criterion_prd, criterion_dsc, \n",
    "          hsp_val, beta_val, hsp_list, beta_list, tg_hsp, lambda_, X_train, y_train):\n",
    "    model.train()\n",
    "    prd_loss = 0\n",
    "    dsc_loss = 0\n",
    "    dsc_acc = 0\n",
    "    cost = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    y_train_true = []\n",
    "    y_train_pred = []\n",
    "    \n",
    "    for batch_idx, (input, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        input, target = input.to(device), target.to(device)\n",
    "        output_prd, output_dsc = model(input)\n",
    "        \n",
    "        # 1. p-factor predictor loss\n",
    "        target_prd = target[:, p_factor_idx].view(-1, 1)\n",
    "        running_prd_loss = criterion_prd(output_prd, target_prd)\n",
    "        l1_norm = calc_l1(model, epoch, hsp_val, beta_val, hsp_list, beta_list, tg_hsp)\n",
    "\n",
    "        if epoch > pretrain_epoch:\n",
    "            # Undersampling for scanner-generalization\n",
    "            target_dsc = target[:, scanner_idx].long().view(-1)\n",
    "            scnr_smp = target[:, scanner_idx].detach().cpu().numpy()\n",
    "            n_minor = (scnr_smp == 0).sum()\n",
    "            n_major = len(scnr_smp) - n_minor\n",
    "            if n_minor != 0 and n_major != 0:\n",
    "                minor_idx = np.where(scnr_smp == 0)[0]\n",
    "                major_idx = np.where(scnr_smp != 0)[0]\n",
    "                major_smp_idx = np.random.choice(major_idx, size=n_minor, replace=True)\n",
    "                smp_idx = np.concatenate((minor_idx.astype(np.int), major_smp_idx.astype(np.int)))\n",
    "                running_dsc_loss = criterion_dsc(output_dsc[smp_idx], target_dsc[smp_idx])\n",
    "                dsc_loss += running_dsc_loss.detach()\n",
    "            else:\n",
    "                running_dsc_loss = 0\n",
    "                dsc_loss += 0\n",
    "\n",
    "            # Total Loss\n",
    "            running_loss = running_dsc_loss + running_prd_loss + l1_norm.clone()\n",
    "            \n",
    "        else:\n",
    "            running_loss = running_prd_loss + l1_norm.clone()\n",
    "\n",
    "        cost = running_loss\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        prd_loss += running_prd_loss.detach()\n",
    "        total += output_prd.size(0)\n",
    "        true_batch = torch.flatten(target_prd.detach())\n",
    "        pred_batch = torch.flatten(output_prd.detach())\n",
    "        y_train_true.append(true_batch)\n",
    "        y_train_pred.append(pred_batch)\n",
    "        \n",
    "    X_train = torch.from_numpy(X_train).type(torch.FloatTensor).to(device)\n",
    "    _, output_dsc = model(X_train)\n",
    "    _, scnr_pred = torch.max(output_dsc.data, 1)\n",
    "    scnr_pred = scnr_pred.detach().cpu().numpy().ravel()\n",
    "    scnr_true = y_train[:, scanner_idx].ravel()\n",
    "    dsc_acc = balanced_accuracy_score(scnr_true, scnr_pred)\n",
    "    \n",
    "    prd_loss /= total\n",
    "    dsc_loss /= total\n",
    "    y_train_true = torch.flatten(torch.stack(y_train_true))\n",
    "    y_train_pred = torch.flatten(torch.stack(y_train_pred))\n",
    "    train_corr = calc_pearsonr(y_train_true, y_train_pred)\n",
    "    train_mae = calc_mae(y_train_true, y_train_pred)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return prd_loss, dsc_loss, dsc_acc, train_corr, train_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, epoch, valid_loader, criterion_prd, criterion_dsc, X_valid, y_valid):\n",
    "    model.eval()\n",
    "    prd_loss = 0\n",
    "    dsc_loss = 0\n",
    "    dsc_acc = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_valid_true = []\n",
    "    y_valid_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input, target in valid_loader:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            output_prd, output_dsc = model(input)\n",
    "            target_dsc = target[:, scanner_idx].long().view(-1)\n",
    "            target_prd = target[:, p_factor_idx].view(-1, 1)\n",
    "            running_dsc_loss = criterion_dsc(output_dsc, target_dsc)\n",
    "            running_prd_loss = criterion_prd(output_prd, target_prd)\n",
    "            dsc_loss += running_dsc_loss.detach()\n",
    "            prd_loss += running_prd_loss.detach()\n",
    "            total += output_prd.size(0)\n",
    "            _, pred = torch.max(output_dsc.data, 1)\n",
    "            correct += (pred.view(-1, 1) == target).sum().detach()\n",
    "            true_batch = torch.flatten(target_prd.detach())\n",
    "            pred_batch = torch.flatten(output_prd.detach())\n",
    "            y_valid_true.append(true_batch)\n",
    "            y_valid_pred.append(pred_batch)\n",
    "\n",
    "    X_valid = torch.from_numpy(X_valid).type(torch.FloatTensor).to(device)\n",
    "    _, output_dsc = model(X_valid)\n",
    "    _, scnr_pred = torch.max(output_dsc.data, 1)\n",
    "    scnr_pred = scnr_pred.detach().cpu().numpy().ravel()\n",
    "    scnr_true = y_valid[:, scanner_idx].ravel()\n",
    "    dsc_acc = balanced_accuracy_score(scnr_true, scnr_pred)\n",
    "\n",
    "    y_valid_true = torch.flatten(torch.stack(y_valid_true))\n",
    "    y_valid_pred = torch.flatten(torch.stack(y_valid_pred))\n",
    "    valid_corr = calc_pearsonr(y_valid_true, y_valid_pred)\n",
    "    valid_mae = calc_mae(y_valid_true, y_valid_pred)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return prd_loss, dsc_loss, dsc_acc, valid_corr, valid_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch, test_loader, criterion_prd, criterion_dsc, X_test, y_test):\n",
    "    model.eval()\n",
    "    prd_loss = 0\n",
    "    total = 0\n",
    "    y_test_true = []\n",
    "    y_test_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input, target in test_loader:\n",
    "            input, target = input.to(device), target.to(device)\n",
    "            output_prd, output_clf = model(input)\n",
    "            target_prd = target[:, p_factor_idx].view(-1, 1)\n",
    "            running_prd_loss = criterion_prd(output_prd, target_prd)\n",
    "            prd_loss += running_prd_loss.detach()\n",
    "            total += output_prd.size(0)\n",
    "            true_batch = torch.flatten(target_prd.detach())\n",
    "            pred_batch = torch.flatten(output_prd.detach())\n",
    "            y_test_true.append(true_batch)\n",
    "            y_test_pred.append(pred_batch)\n",
    "\n",
    "    X_test = torch.from_numpy(X_test).type(torch.FloatTensor).to(device)\n",
    "    _, output_dsc = model(X_test)\n",
    "    _, scnr_pred = torch.max(output_dsc.data, 1)\n",
    "    scnr_pred = scnr_pred.detach().cpu().numpy().ravel()\n",
    "    scnr_true = y_test[:, scanner_idx].ravel()\n",
    "    dsc_acc = balanced_accuracy_score(scnr_true, scnr_pred)\n",
    "\n",
    "    y_test_true = torch.flatten(torch.stack(y_test_true))\n",
    "    y_test_pred = torch.flatten(torch.stack(y_test_pred))\n",
    "    test_corr = calc_pearsonr(y_test_true, y_test_pred)\n",
    "    test_mae = calc_mae(y_test_true, y_test_pred)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return prd_loss, test_corr, test_mae, dsc_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(\n",
    "    save_dir, epochs, train_loss, valid_loss,  \n",
    "    train_corr, valid_corr, train_acc, valid_acc, lr,\n",
    "    plot_hsp_list, plot_beta_list, tg_hsp):\n",
    "    \n",
    "    sns.set(style=\"whitegrid\", font_scale=2)\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(28, 10))\n",
    "    ax = ax.flat\n",
    "    lw = 3.5\n",
    "    last_epoch = epochs\n",
    "    \n",
    "    train_loss, valid_loss = np.array(train_loss), np.array(valid_loss)\n",
    "    \n",
    "    ax[0].plot(train_loss[:last_epoch, 0], label='train prd loss', lw=lw, color=\"r\")\n",
    "    ax[0].plot(valid_loss[:last_epoch, 0], label='valid prd loss', lw=lw, color=\"g\")\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title(\"Predictor Loss Plot\", pad=20)\n",
    "\n",
    "    ax[1].plot(train_loss[:last_epoch, 1], label='train dsc loss', lw=lw, color=\"r\")\n",
    "    ax[1].legend()\n",
    "    ax[1].set_title(\"Discriminator Loss Plot\", pad=20)\n",
    "\n",
    "    ax[2].plot(train_acc[:last_epoch], label='train dsc acc', lw=lw, color=\"r\")\n",
    "    ax[2].legend()\n",
    "    ax[2].set_title(\"Discriminator Accuracy Plot\", pad=20)\n",
    "\n",
    "    ax[3].plot(train_corr[:last_epoch], label='train corr', lw=lw, color=\"r\")\n",
    "    ax[3].plot(valid_corr[:last_epoch], label='valid corr', lw=lw, color=\"g\")\n",
    "    ax[3].legend()\n",
    "    ax[3].set_title(\"Correlation Plot ($r$={:.4f})\".format(valid_corr[-1]), pad=20)\n",
    "\n",
    "    plot_hsp_list, plot_beta_list = np.array(plot_hsp_list).T, np.array(plot_beta_list).T\n",
    "    \n",
    "    for idx, n_layer in enumerate(indices):\n",
    "        ax[4].plot(plot_hsp_list[idx], label='layer{}'.format(n_layer), lw=lw)\n",
    "        ax[5].plot(plot_beta_list[idx], \n",
    "                   label='layer{}'.format(n_layer), lw=lw)\n",
    "        ax[4].legend(); ax[5].legend()\n",
    "        ax[4].set_title(\"HSP plot [{:.3f}/{:.3f}]\"\n",
    "                        .format(plot_hsp_list[0, -1], tg_hsp[0][0]), pad=20)\n",
    "        ax[5].set_title(\"Beta plot\", pad=20)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"{}/Learning_curves.png\".format(save_dir))\n",
    "    \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inner_fold(\n",
    "    n_outer_cv, output_save_dir=None, cur_tg_hsp=None, cur_lambda=None):\n",
    "    inner_cv_list = []\n",
    "    \n",
    "    inner_train_folds_idx = inner_folds_idx[n_outer_cv][0]\n",
    "    inner_valid_folds_idx = inner_folds_idx[n_outer_cv][1]    \n",
    "    \n",
    "    for n_inner_cv in range(inner_n_splits):\n",
    "        \n",
    "        print(\"\\n===================================\", end=\" \")\n",
    "        print(\"Inner Fold [{}/{}]\".format(n_inner_cv + 1, inner_n_splits), end=\" \")\n",
    "        print(\"===================================\")\n",
    "\n",
    "        inner_start_fold_time = time.time()\n",
    "        inner_save_dir = \"{}/Inner_fold_{}\".format(output_save_dir, n_inner_cv + 1)\n",
    "        os.makedirs(inner_save_dir, exist_ok=True)\n",
    "\n",
    "        inner_train_idx = inner_train_folds_idx[n_inner_cv]\n",
    "        inner_valid_idx = inner_valid_folds_idx[n_inner_cv]\n",
    "\n",
    "        X_train, y_train = X[inner_train_idx], y[inner_train_idx]\n",
    "        X_valid, y_valid = X[inner_valid_idx], y[inner_valid_idx]\n",
    "\n",
    "        X_train = stats.zscore(X_train, axis=1)\n",
    "        X_valid = stats.zscore(X_valid, axis=1)\n",
    "\n",
    "        inner_train_dataset = TrainDataset(X_train, y_train)\n",
    "        inner_valid_dataset = ValidDataset(X_valid, y_valid)\n",
    "\n",
    "        inner_train_loader = DataLoader(\n",
    "            inner_train_dataset, batch_size=batch_size, pin_memory=True,\n",
    "            shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "        inner_valid_loader = DataLoader(\n",
    "            inner_valid_dataset, batch_size=len(y_valid), pin_memory=True,\n",
    "            shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "\n",
    "        # Assign model\n",
    "        model = SGNN(\n",
    "            fe_hidden, dsc_hidden, prd_hidden, \n",
    "            dropout_fe, dropout_prd, dropout_dsc, act_func_name, cur_lambda\n",
    "        ).to(device)\n",
    "        optimizer = get_optimizer(model, optimizer_name, learning_rate, l2_param)\n",
    "        lr_factor = lr_alpha * cur_tg_hsp[0][0] + lr_beta\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer, mode=mode, patience=lr_patience, min_lr=min_lr, factor=lr_factor\n",
    "        )\n",
    "        criterion_prd = nn.MSELoss()\n",
    "        criterion_dsc = nn.CrossEntropyLoss()\n",
    "\n",
    "        # list to save learning parameters\n",
    "        inner_train_loss = []\n",
    "        inner_valid_loss = []\n",
    "        inner_train_corr = []\n",
    "        inner_valid_corr = []\n",
    "        inner_train_acc = []\n",
    "        inner_valid_acc = []\n",
    "        inner_lr = []\n",
    "        inner_hsp_list = []\n",
    "        inner_beta_list = []\n",
    "\n",
    "        hsp_val, beta_val, hsp_list, beta_list = init_hsp(n_wsc, epochs)\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            train_prd_loss, train_dsc_loss, train_acc, train_corr, train_mae = train(\n",
    "                model, epoch, inner_train_loader, \n",
    "                optimizer, criterion_prd, criterion_dsc, \n",
    "                hsp_val, beta_val, hsp_list, beta_list, cur_tg_hsp, cur_lambda,\n",
    "                X_train, y_train\n",
    "            )\n",
    "            valid_prd_loss, valid_dsc_loss, valid_acc, valid_corr, valid_mae = valid(\n",
    "                model, epoch, inner_valid_loader, criterion_prd, criterion_dsc,\n",
    "                X_valid, y_valid\n",
    "            )\n",
    "\n",
    "            scheduler.step(hsp_val[0])\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            inner_train_loss.append([train_prd_loss, train_dsc_loss])\n",
    "            inner_train_corr.append(train_corr)\n",
    "            inner_train_acc.append(train_acc)\n",
    "            inner_valid_loss.append([valid_prd_loss, valid_dsc_loss])\n",
    "            inner_valid_corr.append(valid_corr)\n",
    "            inner_valid_acc.append(valid_acc)\n",
    "            inner_lr.append(lr)\n",
    "            inner_hsp_list.append(list(hsp_val.clone()))\n",
    "            inner_beta_list.append(list(beta_val.clone()))\n",
    "\n",
    "            if epoch % print_epoch == 0:\n",
    "                print(\"\\nEpoch [{:d}/{:d}]\".format(epoch, epochs), end=\" \")\n",
    "                print(\"Train corr: {:.4f}, Valid corr: {:.4f}, Train loss: {:.4f}, Valid loss: {:.4f}\"\n",
    "                      .format(train_corr, valid_corr, train_prd_loss, valid_prd_loss))\n",
    "                for i in range(len(wsc_flag)):\n",
    "                    if wsc_flag[i] != 0:\n",
    "                        print(\"Layer {:d}: [{:.4f}/{:.4f}]\".\n",
    "                              format(i + 1, hsp_val[i], cur_tg_hsp[i][0]), end=\" \")\n",
    "                # print(\"\\nCurrent learning rate: {:.2e}\".format(Decimal(str(lr))))\n",
    "                print(\"Train acc: {:.2f}\".format(train_acc))\n",
    "\n",
    "                plot_learning_curves(\n",
    "                    inner_save_dir, epochs, inner_train_loss, inner_valid_loss,\n",
    "                    inner_train_corr, inner_valid_corr, \n",
    "                    inner_train_acc, inner_valid_acc, \n",
    "                    inner_lr, inner_hsp_list, inner_beta_list, cur_tg_hsp\n",
    "                )\n",
    "        \n",
    "        train_prd_loss, train_dsc_loss, train_acc, train_corr, train_mae = valid(\n",
    "            model, epoch, inner_train_loader, criterion_prd, criterion_dsc, X_train, y_train\n",
    "        )\n",
    "        print(\"\\nInner Fold [{}/{}] train corr: {:.4f}, valid corr: {:.4f}\"\n",
    "              .format(n_inner_cv + 1, inner_n_splits, train_corr, valid_corr))\n",
    "        \n",
    "        if n_outer_cv == 0:\n",
    "            torch.save(\n",
    "                model.state_dict(), inner_save_dir + \"/inner_model_fold_\" + str(n_inner_cv + 1) + \".pt\")\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        inner_tot_time = (time.time() - inner_start_fold_time) / 60\n",
    "        print(\"Execution Time for Fold: {:.2f} mins\".format(inner_tot_time))\n",
    "        inner_cv_list.append(\n",
    "            [train_corr.cpu().numpy(), valid_corr.cpu().numpy(),\n",
    "             train_mae.cpu().numpy(), valid_mae.cpu().numpy(),\n",
    "             train_acc, valid_acc]\n",
    "        )\n",
    "            \n",
    "    inner_cv_df = pd.DataFrame(\n",
    "        np.array(inner_cv_list), \n",
    "        columns=[\"train_corr\", \"valid_corr\", \"train_mae\", \"valid_mae\", \"train_acc\", \"valid_acc\"]\n",
    "    )\n",
    "    \n",
    "    avg_train_corr = inner_cv_df[\"train_corr\"].mean()\n",
    "    avg_valid_corr = inner_cv_df[\"valid_corr\"].mean()\n",
    "    avg_train_mae = inner_cv_df[\"train_mae\"].mean()\n",
    "    avg_valid_mae = inner_cv_df[\"valid_mae\"].mean()\n",
    "    \n",
    "    inner_cv_df.to_csv(\"{}/inner_cv.csv\".format(output_save_dir))\n",
    "\n",
    "    return avg_train_corr, avg_valid_corr, avg_train_mae, avg_valid_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_outer_fold(n_outer_cv=0, outer_save_dir=None, sel_tg_hsp=None, sel_lambda=None):\n",
    "    \n",
    "    outer_cv_list = []\n",
    "    \n",
    "    # Outer fold\n",
    "    print(\"\\n===================================\", end=\" \")\n",
    "    print(\"Outer Fold [{}/{}]\".format(n_outer_cv + 1, outer_n_splits), end=\" \")\n",
    "    print(\"===================================\")\n",
    "    \n",
    "    outer_start_fold_time = time.time()\n",
    "    outer_train_idx = outer_train_folds_idx[n_outer_cv]\n",
    "    outer_test_idx = outer_test_folds_idx[n_outer_cv]\n",
    "\n",
    "    X_train, y_train = X[outer_train_idx], y[outer_train_idx]\n",
    "    X_test, y_test = X[outer_test_idx], y[outer_test_idx]\n",
    "    \n",
    "    X_train = stats.zscore(X_train, axis=1)\n",
    "    X_test = stats.zscore(X_test, axis=1)\n",
    "        \n",
    "    outer_train_dataset = TrainDataset(X_train, y_train)\n",
    "    outer_test_dataset = TestDataset(X_test, y_test)\n",
    "    \n",
    "    outer_train_loader = DataLoader(\n",
    "        outer_train_dataset, batch_size=batch_size, pin_memory=True,\n",
    "        shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "    outer_test_loader = DataLoader(\n",
    "        outer_test_dataset, batch_size=len(y_test), pin_memory=True,\n",
    "        shuffle=True, num_workers=num_workers, drop_last=True)\n",
    "        \n",
    "    # Assign model \n",
    "    model = SGNN(\n",
    "        fe_hidden, dsc_hidden, prd_hidden, \n",
    "        dropout_fe, dropout_prd, dropout_dsc, act_func_name, sel_lambda\n",
    "    ).to(device)\n",
    "    optimizer = get_optimizer(model, optimizer_name, learning_rate, l2_param)\n",
    "    lr_factor = lr_alpha * sel_tg_hsp[0][0] + lr_beta\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode=mode, patience=lr_patience, min_lr=min_lr, factor=lr_factor\n",
    "    )\n",
    "    criterion_prd = nn.MSELoss()\n",
    "    criterion_dsc = nn.CrossEntropyLoss()\n",
    "              \n",
    "    # list to save learning parameters\n",
    "    outer_train_loss = []\n",
    "    outer_test_loss = []\n",
    "    outer_train_corr = []\n",
    "    outer_test_corr = []\n",
    "    outer_train_acc = []\n",
    "    outer_test_acc = []\n",
    "    outer_lr = []\n",
    "    outer_hsp_list = []\n",
    "    outer_beta_list = []\n",
    "\n",
    "    hsp_val, beta_val, hsp_list, beta_list = init_hsp(n_wsc, epochs)\n",
    "        \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_prd_loss, train_dsc_loss, train_acc, train_corr, train_mae = train(\n",
    "            model, epoch, outer_train_loader, \n",
    "            optimizer, criterion_prd, criterion_dsc, \n",
    "            hsp_val, beta_val, hsp_list, beta_list, sel_tg_hsp, sel_lambda,\n",
    "            X_train, y_train\n",
    "        )\n",
    "        test_prd_loss, test_corr, test_mae, test_acc = test(\n",
    "            model, epoch, outer_test_loader, criterion_prd, criterion_dsc, X_test, y_test\n",
    "        )\n",
    "\n",
    "        scheduler.step(hsp_val[0])\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        outer_train_loss.append([train_prd_loss, train_dsc_loss])\n",
    "        outer_train_corr.append(train_corr)\n",
    "        outer_train_acc.append(train_acc)\n",
    "        outer_test_loss.append([test_prd_loss, []])\n",
    "        outer_test_corr.append(test_corr)\n",
    "        outer_test_acc.append(test_acc)\n",
    "        outer_lr.append(lr)\n",
    "        outer_hsp_list.append(list(hsp_val.clone()))\n",
    "        outer_beta_list.append(list(beta_val.clone()))\n",
    "\n",
    "        if epoch % print_epoch == 0:\n",
    "            print(\"\\nEpoch [{:d}/{:d}]\".format(epoch, epochs), end=\" \")\n",
    "            print(\"Train corr: {:.4f}, Test corr: {:.4f}, Train loss: {:.4f}, Test loss: {:.4f}\"\n",
    "                  .format(train_corr, test_corr, train_prd_loss, test_prd_loss))\n",
    "            for i in range(len(wsc_flag)):\n",
    "                if wsc_flag[i] != 0:\n",
    "                    print(\"Layer {:d}: [{:.4f}/{:.4f}]\".\n",
    "                          format( i + 1, hsp_val[i], sel_tg_hsp[i][0]), end=\" \")\n",
    "            print(\"Train acc: {:.2f}\".format(train_acc))\n",
    "\n",
    "            plot_learning_curves(\n",
    "                outer_save_dir, epochs, outer_train_loss, outer_test_loss,  \n",
    "                outer_train_corr, outer_test_corr, \n",
    "                outer_train_acc, outer_test_acc, \n",
    "                outer_lr, outer_hsp_list, outer_beta_list, sel_tg_hsp\n",
    "            )\n",
    "    \n",
    "    train_prd_loss, train_corr, train_mae, train_acc = test(\n",
    "        model, epoch, outer_train_loader, criterion_prd, criterion_dsc, X_train, y_train\n",
    "    )\n",
    "    torch.save(model.state_dict(), \n",
    "               outer_save_dir + \"/model_fold_\" + str(n_outer_cv + 1) + \".pt\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    train_corr = train_corr.cpu().numpy()\n",
    "    test_corr = test_corr.cpu().numpy()\n",
    "    train_mae = train_mae.cpu().numpy()\n",
    "    test_mae = test_mae.cpu().numpy()\n",
    "\n",
    "    outer_cv_list.append([train_corr, test_corr, train_mae, test_mae, train_acc, test_acc])\n",
    "            \n",
    "    outer_cv_df = pd.DataFrame(\n",
    "        np.array(outer_cv_list), \n",
    "        columns=[\"train_corr\", \"valid_corr\", \"train_mae\", \"valid_mae\", \"train_acc\", \"valid_acc\"]\n",
    "    )\n",
    "    outer_cv_df.to_csv(\"{}/outer_cv.csv\".format(outer_save_dir))\n",
    "\n",
    "    outer_tot_time = time.time() - outer_start_fold_time\n",
    "    print(\"\\nExecution Time for Fold: {:.2f} mins\".format(outer_tot_time / 60))\n",
    "    \n",
    "    return train_corr, test_corr, train_mae, test_mae, outer_train_acc, outer_train_corr, outer_hsp_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter for SGNN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_func_name = \"elu\"\n",
    "optimizer_name = \"nag\"\n",
    "\n",
    "fe_hidden = 1024\n",
    "pp_hidden = 1024\n",
    "scd_hidden = 1024\n",
    "\n",
    "dropout_fe = 0.9\n",
    "dropout_pp = 0.9\n",
    "dropout_scd = 0.9\n",
    "\n",
    "batch_size = 32\n",
    "learning_rate = 5e-05\n",
    "epochs = 150\n",
    "pretrain_epoch = 40\n",
    "\n",
    "l2_param = 5e-02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparemter for optimizaiton with Nested CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsp_fe_cand = [0.99, 0.98, 0.975, 0.9, 0.8, 0.5]\n",
    "lambda_cand = [0, 0.002, 0.005, 0.01, 0.02]\n",
    "\n",
    "param_cand = {\"lambda_\": lambda_cand, \"hsp_fe\": hsp_fe_cand}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_epoch = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training SGNN with LOSOCV framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outer_cv = []\n",
    "\n",
    "param_grid = list(ParameterGrid(param_cand))\n",
    "\n",
    "for n_outer_cv in outer_cv_part:\n",
    "    print(\"\\n===================================\", end=\" \")\n",
    "    print(\"Outer Fold [{}/{}]\".format(n_outer_cv + 1, outer_n_splits), end=\" \")\n",
    "    print(\"===================================\")\n",
    "\n",
    "    outer_save_dir = \"{}/Outer_fold_{}\".format(output_folder, n_outer_cv + 1)\n",
    "    model_file = \"model_fold_{}.pt\".format(n_outer_cv + 1)\n",
    "    model_path = os.path.join(outer_save_dir, model_file)\n",
    "    \n",
    "    os.makedirs(outer_save_dir, exist_ok=True)\n",
    "    \n",
    "    inner_cv = []\n",
    "    temp_inner_cv_corr = []\n",
    "    temp_inner_cv_mae = []\n",
    "\n",
    "    # Inner Fold\n",
    "    for param_idx, cur_param in enumerate(param_grid):\n",
    "        print(\"\\n===================================\", end=\" \")\n",
    "        print(\"Param Cand [{}/{}]\".format(param_idx + 1, len(param_grid)), end=\" \")\n",
    "        print(\"===================================\")\n",
    "\n",
    "        hsp_cand_1 = [cur_param[\"1_hsp_fe\"]]\n",
    "        hsp_cand_2 = [cur_param[\"2_hsp_prd\"]]\n",
    "        hsp_cand_3 = [cur_param[\"3_hsp_dsc\"]]\n",
    "\n",
    "        indices = [i + 1 for i, x in enumerate(wsc_flag) if x == 1]\n",
    "        hsp_cand_list = list(itertools.product(hsp_cand_1, hsp_cand_2, hsp_cand_3))\n",
    "        hsp_cand_list = [list(i) for i in hsp_cand_list]\n",
    "        hsp_cand = [hsp_cand_1, hsp_cand_2, hsp_cand_3]\n",
    "\n",
    "        cur_tg_hsp = hsp_cand\n",
    "        cur_lambda = cur_param[\"lambda_\"]\n",
    "\n",
    "        print(\"Param:\", end=\" \")\n",
    "        for i, param in enumerate(cur_param):\n",
    "            if \"hsp\" in param or \"lambda\" in param: \n",
    "                print(\"{}: {}\".format(param, cur_param[param]), end=\" \")\n",
    "        print(\"\")\n",
    "        \n",
    "        cur_param_name = \"FE_{}_LMD_{}\".format(\n",
    "            cur_tg_hsp[0][0], cur_lambda) \n",
    "        param_save_dir = \"{}/{}\".format(outer_save_dir, cur_param_name)\n",
    "        \n",
    "        final_inner_cv_df_path = os.path.join(param_save_dir, \"inner_cv.csv\")\n",
    "        \n",
    "        if os.path.exists(final_inner_cv_df_path):\n",
    "            final_inner_cv_df = pd.read_csv(final_inner_cv_df_path, index_col=0)\n",
    "            temp_inner_cv_corr.append(final_inner_cv_df[\"valid_corr\"].mean())\n",
    "            temp_inner_cv_mae.append(final_inner_cv_df[\"valid_mae\"].mean())\n",
    "            print(\"Training Inner CV Done\")\n",
    "        else:\n",
    "            os.makedirs(param_save_dir, exist_ok=True)\n",
    "            inner_train_corr, inner_valid_corr, inner_train_mae, inner_valid_mae = run_inner_fold(\n",
    "                n_outer_cv, param_save_dir, cur_tg_hsp, cur_lambda\n",
    "            )\n",
    "            temp_inner_cv_corr.append([inner_valid_corr])\n",
    "            temp_inner_cv_mae.append([inner_valid_mae])\n",
    "\n",
    "            print(\"\\nParam Cand: [{}/{}] train corr: {:.4f}, valid corr: {:.4f}\"\n",
    "                  .format(param_idx + 1, len(param_grid), inner_train_corr, inner_valid_corr))\n",
    "\n",
    "    sel_idx = np.argmin(temp_inner_cv_mae)\n",
    "    sel_param = param_grid[sel_idx]\n",
    "    sel_hsp = []\n",
    "    sel_lambda = sel_param[\"lambda_\"]\n",
    "    print(\"Selected param:\", end=\" \")\n",
    "    for x in sel_param:\n",
    "        if \"hsp\" in x or \"lambda\" in x: \n",
    "            print(\"{}: {}\".format(x, sel_param[x]), end=\" \")\n",
    "            sel_hsp.append(sel_param[x])\n",
    "    \n",
    "    # Outer Fold\n",
    "    hsp_cand = [sel_param[\"1_hsp_fe\"]]\n",
    "\n",
    "    indices = [i + 1 for i, x in enumerate(wsc_flag) if x == 1]\n",
    "    hsp_cand_list = list(itertools.product(hsp_cand))\n",
    "    hsp_cand_list = [list(i) for i in hsp_cand_list]\n",
    "    hsp_cand = [hsp_cand_1, hsp_cand_2, hsp_cand_3]\n",
    "    sel_tg_hsp = hsp_cand\n",
    "\n",
    "    (outer_train_corr, outer_test_corr, outer_train_mae, \n",
    "    outer_test_mae, outer_train_acc, outer_test_acc, outer_hsp_list) = run_outer_fold(\n",
    "        n_outer_cv, outer_save_dir, sel_tg_hsp, sel_lambda\n",
    "    )\n",
    "    outer_cv.append([sel_hsp, outer_train_corr, outer_test_corr])\n",
    "    \n",
    "    print(\"\\nOuter Fold [{}/{}]: train corr: {:.4f}, valid corr: {:.4f}\"\n",
    "          .format(n_outer_cv + 1, outer_n_splits, outer_train_corr, outer_test_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_tot_time = time.time() - code_start_time \n",
    "print(\"Execution Time for the training: {:.2f} hours\".format(code_tot_time / 60 / 60))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
