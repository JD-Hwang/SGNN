{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU = 7\n",
    "num_workers = 4\n",
    "seed = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsp_extr_cand = [0.9]\n",
    "hsp_pred_cand = [0.7, 0.5, 0.3]\n",
    "hsp_disc_cand = [0.7, 0.5, 0.3]\n",
    "\n",
    "wsc_flag = [1, 1, 1]\n",
    "beta_lr = [1e-04, 1e-03, 1e-03]\n",
    "max_beta = [1e-02, 5e-02, 5e-02]\n",
    "n_wsc = wsc_flag.count(1)\n",
    "\n",
    "hsp_cand_1 = [cur_param[\"1_hsp_extr\"]]\n",
    "hsp_cand_2 = [cur_param[\"2_hsp_pred\"]]\n",
    "hsp_cand_3 = [cur_param[\"3_hsp_disc\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "from decimal import Decimal\n",
    "from datetime import datetime as dt\n",
    "from pytz import timezone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable, Function\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, utils\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nowtime = dt.now(timezone(\"Asia/Seoul\")); year = str(nowtime.year)\n",
    "month = '0{}'.format(nowtime.month) if nowtime.month < 10 else str(nowtime.month)\n",
    "day = '0{}'.format(nowtime.day) if nowtime.day < 10 else str(nowtime.day)\n",
    "hour = '0{}'.format(nowtime.hour) if nowtime.hour < 10 else str(nowtime.hour)\n",
    "minute = '0{}'.format(nowtime.minute) if nowtime.minute < 10 else str(nowtime.minute)\n",
    "sec = '0{}'.format(nowtime.second) if nowtime.second < 10 else str(nowtime.second)\n",
    "msec = str(nowtime.microsecond)[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/hjw/data/sann/optuna/20210702_18290451/\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/users/hjw/data/sann/optuna\"\n",
    "output_folder = \"{}/{}{}{}_{}{}{}{}/\".format(\n",
    "    save_path, year, month, day, hour, minute, sec, msec\n",
    ")\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "print(output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6905, 61776) (6905, 3)\n"
     ]
    }
   ],
   "source": [
    "data = np.load(\"/users/hjw/data/ABCD/npz_files/rsfc_p_site_scanner_si_ge.npz\", allow_pickle=True)\n",
    "X = stats.zscore(data[\"X\"], axis=1)\n",
    "y = data[\"y\"]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_factor_idx = 0\n",
    "site_idx = 1\n",
    "scanner_idx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(y, dtype=np.float)\n",
    "y[:, site_idx] = y[:, site_idx].astype(np.int)\n",
    "y[:, scanner_idx] = y[:, scanner_idx].astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spliting subject indices for leave-one-site-out with validation set\n",
    "y_df = pd.DataFrame(y, columns=[\"p-factor\", \"site\", \"scanner\"])\n",
    "y_arr = np.array(y_df)\n",
    "site_unq = np.unique(y_arr[:, site_idx])\n",
    "data_idx = np.arange(y_arr.shape[0])\n",
    "\n",
    "train_folds_idx = []\n",
    "valid_folds_idx = []\n",
    "test_folds_idx = []\n",
    "\n",
    "split_seed = 0\n",
    "valid_ratio = 0.10\n",
    "\n",
    "for i, site in enumerate(site_unq):\n",
    "    temp_train_idx = np.where(y_arr[:, site_idx] != site)[0]\n",
    "    temp_test_idx = np.where(y_arr[:, site_idx] == site)[0]\n",
    "    \n",
    "    temp_test_df = y_df.iloc[temp_test_idx]\n",
    "    test_scnr_label = np.unique(temp_test_df[\"scanner\"])\n",
    "    \n",
    "    temp_train_df = y_df.iloc[temp_train_idx]\n",
    "    temp_n_valid = np.int(len(temp_train_df) * valid_ratio)\n",
    "    temp_valid_df = temp_train_df[temp_train_df[\"scanner\"] == test_scnr_label[0]]\n",
    "    temp_valid_df = temp_valid_df.sample(n=temp_n_valid, replace=False, random_state=split_seed)\n",
    "\n",
    "    temp_train_idx = np.setdiff1d(temp_train_df.index.values, temp_valid_df.index.values)\n",
    "    temp_valid_idx = temp_valid_df.index.values\n",
    "    train_folds_idx.append(temp_train_idx)\n",
    "    valid_folds_idx.append(temp_valid_idx)\n",
    "    test_folds_idx.append(temp_test_idx)\n",
    "    \n",
    "    train_scnr_label = np.unique(temp_train_df[\"scanner\"])\n",
    "    valid_scnr_label = np.unique(temp_valid_df[\"scanner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"max\"\n",
    "lr_patience = 5\n",
    "min_lr = 1e-12\n",
    "lr_factor = 0.25\n",
    "\n",
    "swa_lr = 5e-03\n",
    "\n",
    "momentum = 0.90\n",
    "\n",
    "l1_param = 0\n",
    "l2_param = 1e-03\n",
    "\n",
    "early_stopping_patience = 150\n",
    "\n",
    "input_dim = 61776\n",
    "n_classes = len(np.unique(y[:, scanner_idx]))\n",
    "output_reg_dim = 1\n",
    "output_clf_dim = n_classes\n",
    "\n",
    "outer_n_splits = len(train_folds_idx)\n",
    "inner_n_splits = outer_n_splits - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9], [0.3], [0.5]]\n"
     ]
    }
   ],
   "source": [
    "indices = [i + 1 for i, x in enumerate(wsc_flag) if x == 1]\n",
    "hsp_cand_list = list(itertools.product(hsp_cand_1, hsp_cand_2, hsp_cand_2))\n",
    "hsp_cand_list = [list(i) for i in hsp_cand_list]\n",
    "hsp_cand = [hsp_cand_1, hsp_cand_2, hsp_cand_3]\n",
    "tg_hsp = hsp_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "class train_dataset(Dataset): \n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        X_train = torch.from_numpy(self.X_train[idx]).type(torch.FloatTensor)\n",
    "        y_train = torch.from_numpy(self.y_train[idx]).type(torch.FloatTensor)\n",
    "\n",
    "        return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "class valid_dataset(Dataset): \n",
    "    def __init__(self, X_valid, y_valid):\n",
    "        self.X_valid = X_valid\n",
    "        self.y_valid = y_valid\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_valid)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        X_valid = torch.from_numpy(self.X_valid[idx]).type(torch.FloatTensor)\n",
    "        y_valid = torch.from_numpy(self.y_valid[idx]).type(torch.FloatTensor)\n",
    "        \n",
    "        return X_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataset\n",
    "class test_dataset(Dataset): \n",
    "    def __init__(self, X_test, y_test):\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X_test)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        X_test = torch.from_numpy(self.X_test[idx]).type(torch.FloatTensor)\n",
    "        y_test = torch.from_numpy(self.y_test[idx]).type(torch.FloatTensor)\n",
    "        \n",
    "        return X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversalFunction(Function):\n",
    "    \"\"\"\n",
    "    Gradient Reversal Layer from:\n",
    "    Unsupervised Domain Adaptation by Backpropagation (Ganin & Lempitsky, 2015)\n",
    "    Forward pass is the identity function. In the backward pass,\n",
    "    the upstream gradients are multiplied by -lambda (i.e. gradient is reversed)\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.clone()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grads):\n",
    "        lambda_ = ctx.lambda_\n",
    "        lambda_ = grads.new_tensor(lambda_)\n",
    "        dx = -lambda_ * grads\n",
    "        return dx, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientReversal(torch.nn.Module):\n",
    "    def __init__(self, lambda_=0.0):\n",
    "        super(GradientReversal, self).__init__()\n",
    "        self.lambda_ = lambda_\n",
    "\n",
    "    def forward(self, x):\n",
    "        return GradientReversalFunction.apply(x, self.lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self, extr_hidden, disc_hidden, pred_hidden, \n",
    "                 dropout_rate, dropout_reg, lambda_, act_func_name):\n",
    "        super(DNN, self).__init__()\n",
    "        self.ext_1 = nn.Linear(input_dim, extr_hidden)\n",
    "        self.ext_bn_1 = nn.BatchNorm1d(extr_hidden)\n",
    "        \n",
    "        self.reg_1 = nn.Linear(extr_hidden, pred_hidden)\n",
    "        self.reg_bn_1 = nn.BatchNorm1d(pred_hidden)\n",
    "        self.reg_2 = nn.Linear(pred_hidden, output_reg_dim)\n",
    "        \n",
    "        self.clf_1 = nn.Linear(extr_hidden, disc_hidden)\n",
    "        self.clf_bn_1 = nn.BatchNorm1d(disc_hidden)\n",
    "        self.clf_2 = nn.Linear(disc_hidden, output_clf_dim)\n",
    "\n",
    "        self.GradientReversal = GradientReversal(lambda_)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.dropout_reg = nn.Dropout(p=dropout_reg)\n",
    "        self.act_func = get_activation_function(act_func_name)\n",
    "        self.weights_init()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feature = self.ext_1(x)\n",
    "        feature = self.ext_bn_1(feature)\n",
    "        feature = self.act_func(feature)\n",
    "        feature = self.dropout(feature)\n",
    "        \n",
    "        x_reg = self.reg_1(feature)\n",
    "        x_reg = self.reg_bn_1(x_reg)\n",
    "        x_reg = self.act_func(x_reg)\n",
    "        x_reg = self.dropout_reg(x_reg)\n",
    "        x_reg = self.reg_2(x_reg)\n",
    "        \n",
    "        x_clf = self.GradientReversal(feature)\n",
    "        x_clf = self.clf_1(x_clf)\n",
    "        x_clf = self.clf_bn_1(x_clf)\n",
    "        x_clf = self.act_func(x_clf)\n",
    "        # x_clf = self.dropout(x_clf)\n",
    "        x_clf = self.clf_2(x_clf)\n",
    "        \n",
    "        return x_reg, x_clf\n",
    "    \n",
    "    def weights_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
    "                nn.init.normal_(m.bias, std=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model, opt_name, learning_rate=None, l2_param=None):\n",
    "    lower_opt_name = opt_name.lower()\n",
    "    if lower_opt_name == 'momentum':\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate, \n",
    "                         momentum=momentum, weight_decay=l2_param)\n",
    "    elif lower_opt_name == 'nag':\n",
    "        return optim.SGD(model.parameters(), lr=learning_rate, \n",
    "                         momentum=momentum, weight_decay=l2_param, nesterov=True)\n",
    "    elif lower_opt_name == 'adam':\n",
    "        return optim.Adam(model.parameters(), lr=learning_rate, \n",
    "                          weight_decay=l2_param)\n",
    "    else:\n",
    "        sys.exit(\"Illegal arguement for optimizer type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation_function(act_func_name):\n",
    "    act_func_name = act_func_name.lower()\n",
    "    if act_func_name == 'relu':\n",
    "        return nn.ReLU()\n",
    "    elif act_func_name == 'prelu':\n",
    "        return nn.PReLU()\n",
    "    elif act_func_name == 'elu':\n",
    "        return nn.ELU()\n",
    "    elif act_func_name == 'silu':\n",
    "        return nn.SiLU()\n",
    "    elif act_func_name == 'leakyrelu':\n",
    "        return nn.LeakyReLU()\n",
    "    elif act_func_name == 'tanh':\n",
    "        return nn.Tanh()\n",
    "    else:\n",
    "        sys.exit(\"Illegal arguement for activation function type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_hsp(n_wsc, epochs):\n",
    "    hsp_val = np.zeros(n_wsc)\n",
    "    beta_val = hsp_val.copy()\n",
    "    hsp_list = np.zeros((n_wsc, epochs))\n",
    "    beta_list = np.zeros((n_wsc, epochs))\n",
    "    \n",
    "    return hsp_val, beta_val, hsp_list, beta_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight sparsity control with Hoyer's sparsness (Layer wise)\n",
    "def calc_hsp(w, beta, max_beta, beta_lr, tg_hsp):\n",
    "    \n",
    "    # Get value of weight\n",
    "    [dim, n_nodes] = w.shape\n",
    "    num_elements = dim * n_nodes\n",
    "    norm_ratio = torch.norm(w, 1) / torch.norm(w, 2)\n",
    "\n",
    "    # Calculate hoyer's sparsity level\n",
    "    num = np.sqrt(num_elements) - norm_ratio.item()\n",
    "    den = np.sqrt(num_elements) - 1\n",
    "    hsp = num / den\n",
    "\n",
    "    # Update beta\n",
    "    beta = beta + beta_lr * np.sign(tg_hsp - hsp)\n",
    "    \n",
    "    # Trim value\n",
    "    beta = 0.0 if beta < 0.0 else beta\n",
    "    beta = max_beta if beta > max_beta else beta\n",
    "\n",
    "    return [hsp, beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_penalty(model, epoch, hsp_val, beta_val, hsp_list, beta_list, tg_hsp):\n",
    "    l1_reg = None\n",
    "    layer_idx = 0\n",
    "    wsc_idx = 0\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name and \"bn\" not in name:\n",
    "            if \"ext\" in name or \"reg_1\" in name or \"clf_1\" in name:\n",
    "                temp_w = param\n",
    "                \n",
    "                if wsc_flag[layer_idx] != 0:\n",
    "                    hsp_val[wsc_idx], beta_val[wsc_idx] = calc_hsp(\n",
    "                        temp_w, beta_val[wsc_idx], max_beta[wsc_idx], \n",
    "                        beta_lr[wsc_idx], tg_hsp[wsc_idx]\n",
    "                    )\n",
    "                    hsp_list[wsc_idx, epoch - 1] = hsp_val[wsc_idx]\n",
    "                    beta_list[wsc_idx, epoch - 1] = beta_val[wsc_idx]\n",
    "                    layer_reg = torch.norm(temp_w, 1) * beta_val[wsc_idx]\n",
    "                    wsc_idx += 1\n",
    "                else:\n",
    "                    layer_reg = torch.norm(temp_w, 1).item() * l1_param\n",
    "\n",
    "                if l1_reg is None:\n",
    "                    l1_reg = layer_reg\n",
    "                else:\n",
    "                    l1_reg = l1_reg + layer_reg\n",
    "                layer_idx += 1\n",
    "        \n",
    "    return l1_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearsonr(x, y):\n",
    "    x_mean = torch.mean(x)\n",
    "    y_mean = torch.mean(y)\n",
    "    xx = x.sub(x_mean)\n",
    "    yy = y.sub(y_mean)\n",
    "    num = xx.dot(yy)\n",
    "    den = torch.norm(xx, 2) * torch.norm(yy, 2)\n",
    "    corr = num / den\n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, train_loader, optimizer, criterion_clf, criterion_reg, \n",
    "          hsp_val, beta_val, hsp_list, beta_list, tg_hsp):\n",
    "    model.train()\n",
    "    reg_loss = 0\n",
    "    clf_loss = 0\n",
    "    clf_acc = 0\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    y_train_true = []\n",
    "    y_train_pred = []\n",
    "    \n",
    "    for batch_idx, (input, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        input, target = input.to(DEVICE), target.to(DEVICE)\n",
    "        output_reg, output_clf = model(input)\n",
    "        target_clf = target[:, scanner_idx].long().view(-1)\n",
    "        target_reg = target[:, p_factor_idx].view(-1, 1)\n",
    "        running_clf_loss = criterion_clf(output_clf, target_clf)\n",
    "        running_reg_loss = criterion_reg(output_reg, target_reg)\n",
    "        l1_term = l1_penalty(model, epoch, hsp_val, beta_val, hsp_list, beta_list, tg_hsp)\n",
    "        running_loss = running_clf_loss + running_reg_loss + l1_term\n",
    "        cost = running_loss\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "        clf_loss += running_clf_loss.item()\n",
    "        reg_loss += running_reg_loss.item()\n",
    "        total += output_reg.size(0)\n",
    "        _, pred = torch.max(output_clf.data, 1)\n",
    "        correct += (pred.view(-1, 1) == target).sum().item()\n",
    "        true_batch = torch.flatten(target_reg.detach())\n",
    "        pred_batch = torch.flatten(output_reg.detach())\n",
    "        y_train_true.append(true_batch)\n",
    "        y_train_pred.append(pred_batch)\n",
    "        \n",
    "    reg_loss /= total\n",
    "    clf_loss /= total\n",
    "    clf_acc = 100 * correct / total\n",
    "    y_train_true = torch.flatten(torch.stack(y_train_true))\n",
    "    y_train_pred = torch.flatten(torch.stack(y_train_pred))\n",
    "    train_corr = pearsonr(y_train_true, y_train_pred)\n",
    "    return clf_loss, reg_loss, clf_acc, train_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, epoch, valid_loader, criterion_clf, criterion_reg):\n",
    "    model.eval()\n",
    "    reg_loss = 0\n",
    "    clf_loss = 0\n",
    "    clf_acc = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    y_valid_true = []\n",
    "    y_valid_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input, target in valid_loader:\n",
    "            input, target = input.to(DEVICE), target.to(DEVICE)\n",
    "            output_reg, output_clf = model(input)\n",
    "            target_clf = target[:, scanner_idx].long().view(-1)\n",
    "            target_reg = target[:, p_factor_idx].view(-1, 1)\n",
    "            running_clf_loss = criterion_clf(output_clf, target_clf)\n",
    "            running_reg_loss = criterion_reg(output_reg, target_reg)\n",
    "            clf_loss += running_clf_loss.item()\n",
    "            reg_loss += running_reg_loss.item()\n",
    "            total += output_reg.size(0)\n",
    "            _, pred = torch.max(output_clf.data, 1)\n",
    "            correct += (pred.view(-1, 1) == target).sum().item()\n",
    "            true_batch = torch.flatten(target_reg.detach())\n",
    "            pred_batch = torch.flatten(output_reg.detach())\n",
    "            y_valid_true.append(true_batch)\n",
    "            y_valid_pred.append(pred_batch)\n",
    "\n",
    "    clf_acc = 100 * correct / total\n",
    "    y_valid_true = torch.flatten(torch.stack(y_valid_true))\n",
    "    y_valid_pred = torch.flatten(torch.stack(y_valid_pred))\n",
    "    valid_corr = pearsonr(y_valid_true, y_valid_pred)\n",
    "    return clf_loss, reg_loss, clf_acc, valid_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, epoch, test_loader, criterion_clf, criterion_reg):\n",
    "    model.eval()\n",
    "    reg_loss = 0\n",
    "    total = 0\n",
    "    y_test_true = []\n",
    "    y_test_pred = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input, target in test_loader:\n",
    "            input, target = input.to(DEVICE), target.to(DEVICE)\n",
    "            output_reg, output_clf = model(input)\n",
    "            target_reg = target[:, p_factor_idx].view(-1, 1)\n",
    "            running_reg_loss = criterion_reg(output_reg, target_reg)\n",
    "            reg_loss += running_reg_loss.item()\n",
    "            total += output_reg.size(0)\n",
    "            true_batch = torch.flatten(target_reg.detach())\n",
    "            pred_batch = torch.flatten(output_reg.detach())\n",
    "            y_test_true.append(true_batch)\n",
    "            y_test_pred.append(pred_batch)\n",
    "\n",
    "    y_test_true = torch.flatten(torch.stack(y_test_true))\n",
    "    y_test_pred = torch.flatten(torch.stack(y_test_pred))\n",
    "    test_corr = pearsonr(y_test_true, y_test_pred)\n",
    "    return reg_loss, test_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class early_stopping_func:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0, path=None):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_epoch = 0\n",
    "        self.best_corr = 0\n",
    "        self.early_stop = False\n",
    "        self.valid_corr_max = -np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "    \n",
    "    def __call__(self, valid_loss, model, epoch, train_corr, valid_corr, test_corr):\n",
    "        if self.best_corr is None:\n",
    "            self.best_corr = valid_corr\n",
    "            self.best_corr_list = [train_corr, valid_corr, test_corr]\n",
    "            self.save_checkpoint(valid_loss, model, epoch)\n",
    "        elif valid_corr < self.best_corr + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                print(\"Early Stopping! Best Model at Epoch {}\"\n",
    "                      .format(self.best_epoch), end=\", \")\n",
    "                print(\"valid corr: {:.4f}, test corr: {:.4f}\"\n",
    "                      .format(self.best_corr_list[1], self.best_corr_list[2]))\n",
    "        else:\n",
    "            self.best_corr = valid_corr\n",
    "            self.best_corr_list = [train_corr, valid_corr, test_corr]\n",
    "            self.save_checkpoint(self.best_corr, model, epoch)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, best_corr, model, epoch):\n",
    "        if self.verbose:\n",
    "            print(\"Validation Corr Increased! ({:.4f} --> {:.4f}), Saving the Model!\"\n",
    "                  .format(self.valid_corr_max, best_corr))\n",
    "        torch.save(model.state_dict(), self.path + \"/early_stopped_model.pt\")\n",
    "        self.valid_corr_max = best_corr\n",
    "        self.best_epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(\n",
    "    save_dir, epochs, train_loss, valid_loss, test_loss, \n",
    "    train_corr, valid_corr, test_corr, train_acc, valid_acc, lr,\n",
    "    plot_hsp_list, plot_beta_list, tg_hsp):\n",
    "    \n",
    "    sns.set(style=\"dark\", font_scale=2)\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(28, 10))\n",
    "    ax = ax.flat\n",
    "    lw = 3.5\n",
    "    last_epoch = epochs\n",
    "    \n",
    "    train_loss, valid_loss, test_loss = np.array(train_loss), np.array(valid_loss), np.array(test_loss)\n",
    "    \n",
    "    ax[0].plot(train_loss[:last_epoch, 0], label='train disc loss', lw=lw, color=\"r\")\n",
    "    ax[0].legend()\n",
    "    ax[0].set_title(\"Discriminator Loss Plot\")\n",
    "\n",
    "    # ax[1].plot(train_loss[:last_epoch, 1], label='train pred loss', lw=lw, color=\"r\")\n",
    "    ax[1].plot(valid_loss[:last_epoch, 1], label='valid pred loss', lw=lw, color=\"g\")\n",
    "    ax[1].plot(test_loss[:last_epoch, 1], label='test pred loss', lw=lw, color=\"b\")\n",
    "    ax[1].legend()\n",
    "    ax[1].set_title(\"Predictor Loss Plot\")\n",
    "\n",
    "    ax[2].plot(lr[:last_epoch], label='learning rate', lw=lw, color=\"k\")\n",
    "    ax[2].legend()\n",
    "    ax[2].set_title(\"Learning Rate Plot\")\n",
    "\n",
    "    ax[3].plot(train_corr[:last_epoch], label='train corr', lw=lw, color=\"r\")\n",
    "    ax[3].plot(valid_corr[:last_epoch], label='valid corr', lw=lw, color=\"g\")\n",
    "    ax[3].plot(test_corr[:last_epoch], label='test corr', lw=lw, color=\"b\")\n",
    "    ax[3].legend()\n",
    "    ax[3].set_title(\"Correlation Plot ($r$={:.4f})\".format(test_corr[-1]))\n",
    "\n",
    "    plot_hsp_list, plot_beta_list = np.array(plot_hsp_list).T, np.array(plot_beta_list).T\n",
    "    \n",
    "    for idx, n_layer in enumerate(indices):\n",
    "        ax[4].plot(plot_hsp_list[idx], label='layer{}'.format(n_layer), lw=lw)\n",
    "        ax[5].plot(plot_beta_list[idx], \n",
    "                   label='layer{}'.format(n_layer), lw=lw)\n",
    "        ax[4].legend(); ax[5].legend()\n",
    "        ax[4].set_title(\"HSP plot [{:.3f}/{:.3f}]\".format(plot_hsp_list[0, -1], tg_hsp[0][0]))\n",
    "        ax[5].set_title(\"Beta plot\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(\"{}/Learning_curves.png\".format(save_dir))\n",
    "    \n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_fold(\n",
    "    n_outer_cv=0, output_folder=output_folder, params=None):\n",
    "\n",
    "    start_fold_time = time.time()\n",
    "    act_func_name = params[\"act_func\"]\n",
    "    optimizer_name = params[\"optimizer\"]\n",
    "\n",
    "    extr_hidden = params[\"extr_hidden\"]\n",
    "    pred_hidden = params[\"pred_hidden\"]\n",
    "    disc_hidden = params[\"disc_hidden\"]\n",
    "\n",
    "    learning_rate = params[\"learning_rate\"]\n",
    "    batch_size = params[\"batch_size\"]\n",
    "    dropout_rate = params[\"dropout_rate\"]\n",
    "    dropout_reg = params[\"dropout_reg\"]\n",
    "    lambda_ = params[\"lambda_\"]\n",
    "    \n",
    "    epochs = params[\"epochs\"]\n",
    "    swa_start = params[\"swa_start\"]\n",
    "    \n",
    "    n_outer_cv = n_outer_cv\n",
    "    output_folder = output_folder\n",
    "\n",
    "    print(\"\\n=================================== Outer Fold [{}/{}] ===================================\"\n",
    "          .format(n_outer_cv + 1, outer_n_splits))\n",
    "    outer_save_dir = \"{}/Outer_fold_{}\".format(output_folder, n_outer_cv + 1)\n",
    "    os.makedirs(outer_save_dir, exist_ok=True)\n",
    "    outer_train_idx = train_folds_idx[n_outer_cv]\n",
    "    outer_valid_idx = valid_folds_idx[n_outer_cv]\n",
    "    outer_test_idx = test_folds_idx[n_outer_cv]\n",
    "\n",
    "    X_train, y_train = X[outer_train_idx], y[outer_train_idx]\n",
    "    X_valid, y_valid = X[outer_valid_idx], y[outer_valid_idx]\n",
    "    X_test, y_test = X[outer_test_idx], y[outer_test_idx]\n",
    "    \n",
    "    outer_train_dataset = train_dataset(X_train, y_train)\n",
    "    outer_valid_dataset = valid_dataset(X_valid, y_valid)\n",
    "    outer_test_dataset = test_dataset(X_test, y_test)\n",
    "    \n",
    "    outer_train_loader = DataLoader(\n",
    "        outer_train_dataset, batch_size=batch_size, pin_memory=True,\n",
    "        shuffle=True, num_workers=num_workers, drop_last=True\n",
    "    )\n",
    "    outer_valid_loader = DataLoader(\n",
    "        outer_valid_dataset, batch_size=len(y_valid), pin_memory=True,\n",
    "        shuffle=True, num_workers=num_workers, drop_last=True\n",
    "    )\n",
    "    outer_test_loader = DataLoader(\n",
    "        outer_test_dataset, batch_size=len(y_test), pin_memory=True,\n",
    "        shuffle=True, num_workers=num_workers, drop_last=True\n",
    "    )\n",
    "        \n",
    "    # Assign model \n",
    "    model = DNN(\n",
    "        extr_hidden, disc_hidden, pred_hidden, dropout_rate, dropout_reg, lambda_, act_func_name\n",
    "    ).to(DEVICE)\n",
    "    optimizer = get_optimizer(model, optimizer_name, learning_rate, l2_param)\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer, mode=mode, patience=lr_patience, min_lr=min_lr, factor=lr_factor\n",
    "    )\n",
    "    swa_model_50 = AveragedModel(model).to(DEVICE)\n",
    "    swa_model_75 = AveragedModel(model).to(DEVICE)\n",
    "    swa_model_100 = AveragedModel(model).to(DEVICE)\n",
    "    swa_scheduler = SWALR(optimizer, swa_lr=swa_lr)\n",
    "\n",
    "    criterion_clf = nn.CrossEntropyLoss()\n",
    "    criterion_reg = nn.MSELoss(reduction=\"mean\")\n",
    "    \n",
    "    early_stopping = early_stopping_func(patience=early_stopping_patience, path=outer_save_dir)\n",
    "\n",
    "    # list to save learning parameters\n",
    "    outer_train_loss = []\n",
    "    outer_valid_loss = []\n",
    "    outer_test_loss = []\n",
    "    outer_train_corr = []\n",
    "    outer_valid_corr = []\n",
    "    outer_test_corr = []\n",
    "    outer_train_acc = []\n",
    "    outer_valid_acc = []\n",
    "    outer_lr = []\n",
    "    outer_hsp_list = []\n",
    "    outer_beta_list = []\n",
    "\n",
    "    hsp_val, beta_val, hsp_list, beta_list = init_hsp(n_wsc, epochs)\n",
    "        \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_clf_loss, train_reg_loss, train_acc, train_corr = train(\n",
    "            model, epoch, outer_train_loader, \n",
    "            optimizer, criterion_clf, criterion_reg, \n",
    "            hsp_val, beta_val, hsp_list, beta_list, tg_hsp\n",
    "        )\n",
    "        valid_clf_loss, valid_reg_loss, valid_acc, valid_corr = valid(\n",
    "            model, epoch, outer_valid_loader, criterion_clf, criterion_reg\n",
    "        )\n",
    "        test_reg_loss, test_corr = test(\n",
    "            model, epoch, outer_test_loader, criterion_clf, criterion_reg\n",
    "        )\n",
    "        \n",
    "        if epoch > swa_start:\n",
    "            swa_model_50.update_parameters(model)\n",
    "            swa_model_75.update_parameters(model)\n",
    "            swa_model_100.update_parameters(model)\n",
    "            # swa_scheduler.step()\n",
    "        \n",
    "        scheduler.step(hsp_val[0])\n",
    "        lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        outer_train_loss.append([train_clf_loss, train_reg_loss])\n",
    "        outer_train_corr.append(train_corr)\n",
    "        outer_train_acc.append(train_acc)\n",
    "        outer_valid_loss.append([valid_clf_loss, valid_reg_loss])\n",
    "        outer_valid_corr.append(valid_corr)\n",
    "        outer_valid_acc.append(valid_acc)\n",
    "        outer_test_loss.append([[], test_reg_loss])\n",
    "        outer_test_corr.append(test_corr)\n",
    "        outer_lr.append(lr)\n",
    "        outer_hsp_list.append(list(hsp_val))\n",
    "        outer_beta_list.append(list(beta_val))\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(\"Epoch [{:d}/{:d}]\".format(epoch, epochs), end=\" \")\n",
    "            print(\"Train corr: {:.4f}, Valid corr: {:.4f}, Test corr: {:.4f}\"\n",
    "                  .format(train_corr, valid_corr, test_corr))\n",
    "            for i in range(len(wsc_flag)):\n",
    "                if wsc_flag[i] != 0:\n",
    "                    print(\"Layer {:d}: [{:.4f}/{:.4f}]\".\n",
    "                          format( i + 1, hsp_val[i], tg_hsp[i][0]), end=\" \")\n",
    "            print(\"\\nCurrent learning rate: {:.2e}\".format(Decimal(str(lr))))\n",
    "\n",
    "        early_stopping(valid_reg_loss, model, epoch, train_corr, valid_corr, test_corr)\n",
    "        \n",
    "        if early_stopping.early_stop:\n",
    "            train_corr = early_stopping.best_corr_list[0]\n",
    "            valid_corr = early_stopping.best_corr_list[1]\n",
    "            test_corr = early_stopping.best_corr_list[2]\n",
    "            break\n",
    "\n",
    "        plot_learning_curves(\n",
    "            outer_save_dir, epochs, outer_train_loss, outer_valid_loss, outer_test_loss,  \n",
    "            outer_train_corr, outer_valid_corr, outer_test_corr, \n",
    "            outer_train_acc, outer_valid_acc, outer_lr, outer_hsp_list, outer_beta_list, tg_hsp\n",
    "        )\n",
    "    \n",
    "    if early_stopping.early_stop == False:\n",
    "        print(\"Outer Fold [{}/{}] train corr: {:.4f}, valid corr: {:.4f}, test corr: {:.4f}\"\n",
    "              .format(n_outer_cv + 1, outer_n_splits, train_corr, valid_corr, test_corr))\n",
    "    \n",
    "    swa_model_50 = swa_model_50.cpu()\n",
    "    swa_model_75 = swa_model_75.cpu()\n",
    "    swa_model_100 = swa_model_100.cpu()\n",
    "\n",
    "    torch.optim.swa_utils.update_bn(outer_train_loader, swa_model_50)\n",
    "    torch.optim.swa_utils.update_bn(outer_train_loader, swa_model_75)\n",
    "    torch.optim.swa_utils.update_bn(outer_train_loader, swa_model_100)\n",
    "    \n",
    "    torch.save(swa_model_50.state_dict(), \n",
    "               outer_save_dir + \"/swa_model_50_fold_\" + str(n_outer_cv + 1) + \".pt\")\n",
    "    torch.save(swa_model_75.state_dict(), \n",
    "               outer_save_dir + \"/swa_model_75_fold_\" + str(n_outer_cv + 1) + \".pt\")\n",
    "    torch.save(swa_model_100.state_dict(), \n",
    "               outer_save_dir + \"/swa_model_100_fold_\" + str(n_outer_cv + 1) + \".pt\")\n",
    "\n",
    "    torch.save(model.state_dict(), \n",
    "               outer_save_dir + \"/model_fold_\" + str(n_outer_cv + 1) + \".pt\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    tot_time = time.time() - start_fold_time\n",
    "    print(\"Execution Time for Fold: {:.2f} mins\".format(tot_time / 60))\n",
    "    return train_corr, valid_corr, test_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "/users/hjw/data/sann/optuna/20210702_18290451/arc_1024_1024_1024\n",
      "\n",
      "=================================== Outer Fold [1/18] ===================================\n",
      "Epoch [5/150] Train corr: 0.0092, Valid corr: 0.1171, Test corr: 0.0477\n",
      "Layer 1: [0.4864/0.9000] Layer 2: [0.3069/0.3000] Layer 3: [0.3923/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [10/150] Train corr: 0.0223, Valid corr: 0.1467, Test corr: 0.0654\n",
      "Layer 1: [0.7407/0.9000] Layer 2: [0.3066/0.3000] Layer 3: [0.5026/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [15/150] Train corr: 0.0370, Valid corr: 0.0878, Test corr: 0.1430\n",
      "Layer 1: [0.8072/0.9000] Layer 2: [0.3065/0.3000] Layer 3: [0.5027/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [20/150] Train corr: 0.0473, Valid corr: 0.0623, Test corr: 0.0725\n",
      "Layer 1: [0.5978/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5026/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [25/150] Train corr: 0.0394, Valid corr: 0.1253, Test corr: 0.1807\n",
      "Layer 1: [0.7128/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5023/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [30/150] Train corr: 0.0615, Valid corr: 0.1324, Test corr: 0.1660\n",
      "Layer 1: [0.7914/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 3.12e-6\n",
      "Epoch [35/150] Train corr: 0.0662, Valid corr: 0.1382, Test corr: 0.1691\n",
      "Layer 1: [0.8555/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [40/150] Train corr: 0.1235, Valid corr: 0.1392, Test corr: 0.1554\n",
      "Layer 1: [0.8565/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [45/150] Train corr: 0.1350, Valid corr: 0.1484, Test corr: 0.2131\n",
      "Layer 1: [0.9007/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [50/150] Train corr: 0.1343, Valid corr: 0.1502, Test corr: 0.1889\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [55/150] Train corr: 0.1565, Valid corr: 0.1559, Test corr: 0.1922\n",
      "Layer 1: [0.8994/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [60/150] Train corr: 0.1947, Valid corr: 0.1596, Test corr: 0.1965\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [65/150] Train corr: 0.1929, Valid corr: 0.1600, Test corr: 0.1958\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [70/150] Train corr: 0.2071, Valid corr: 0.1592, Test corr: 0.1957\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [75/150] Train corr: 0.2093, Valid corr: 0.1585, Test corr: 0.1943\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [80/150] Train corr: 0.1972, Valid corr: 0.1573, Test corr: 0.1940\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [85/150] Train corr: 0.2239, Valid corr: 0.1566, Test corr: 0.1887\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [90/150] Train corr: 0.2196, Valid corr: 0.1548, Test corr: 0.1895\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [95/150] Train corr: 0.2043, Valid corr: 0.1546, Test corr: 0.1862\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [100/150] Train corr: 0.2025, Valid corr: 0.1547, Test corr: 0.1865\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [105/150] Train corr: 0.2325, Valid corr: 0.1533, Test corr: 0.1879\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [110/150] Train corr: 0.2455, Valid corr: 0.1533, Test corr: 0.1897\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [115/150] Train corr: 0.2200, Valid corr: 0.1533, Test corr: 0.1883\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [120/150] Train corr: 0.2552, Valid corr: 0.1514, Test corr: 0.1892\n",
      "Layer 1: [0.9003/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [125/150] Train corr: 0.2457, Valid corr: 0.1527, Test corr: 0.1928\n",
      "Layer 1: [0.9003/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [130/150] Train corr: 0.2514, Valid corr: 0.1497, Test corr: 0.1905\n",
      "Layer 1: [0.9003/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [135/150] Train corr: 0.2563, Valid corr: 0.1486, Test corr: 0.1902\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [140/150] Train corr: 0.2599, Valid corr: 0.1471, Test corr: 0.1905\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [145/150] Train corr: 0.2694, Valid corr: 0.1463, Test corr: 0.1891\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [150/150] Train corr: 0.2733, Valid corr: 0.1468, Test corr: 0.1925\n",
      "Layer 1: [0.8998/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Outer Fold [1/18] train corr: 0.2733, valid corr: 0.1468, test corr: 0.1925\n",
      "Execution Time for Fold: 30.68 mins\n",
      "\n",
      "=================================== Outer Fold [2/18] ===================================\n",
      "Epoch [5/150] Train corr: 0.0399, Valid corr: 0.0866, Test corr: 0.1073\n",
      "Layer 1: [0.4752/0.9000] Layer 2: [0.3068/0.3000] Layer 3: [0.3845/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [10/150] Train corr: 0.0208, Valid corr: 0.1006, Test corr: 0.1020\n",
      "Layer 1: [0.7259/0.9000] Layer 2: [0.3066/0.3000] Layer 3: [0.5032/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [15/150] Train corr: 0.0119, Valid corr: 0.1233, Test corr: 0.1128\n",
      "Layer 1: [0.8100/0.9000] Layer 2: [0.3065/0.3000] Layer 3: [0.5025/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [20/150] Train corr: 0.0042, Valid corr: 0.1307, Test corr: 0.1628\n",
      "Layer 1: [0.6535/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5023/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [25/150] Train corr: 0.0476, Valid corr: 0.1211, Test corr: 0.1469\n",
      "Layer 1: [0.7591/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [30/150] Train corr: 0.0797, Valid corr: 0.1763, Test corr: 0.1372\n",
      "Layer 1: [0.8285/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 3.12e-6\n",
      "Epoch [35/150] Train corr: 0.0985, Valid corr: 0.1369, Test corr: 0.1574\n",
      "Layer 1: [0.8823/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [40/150] Train corr: 0.0772, Valid corr: 0.1635, Test corr: 0.1749\n",
      "Layer 1: [0.8831/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [45/150] Train corr: 0.1382, Valid corr: 0.1919, Test corr: 0.1642\n",
      "Layer 1: [0.9023/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [50/150] Train corr: 0.1384, Valid corr: 0.1906, Test corr: 0.1780\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [55/150] Train corr: 0.1408, Valid corr: 0.1923, Test corr: 0.1735\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [60/150] Train corr: 0.1886, Valid corr: 0.1949, Test corr: 0.1718\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [65/150] Train corr: 0.1680, Valid corr: 0.1963, Test corr: 0.1771\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [70/150] Train corr: 0.1744, Valid corr: 0.1965, Test corr: 0.1742\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/150] Train corr: 0.1889, Valid corr: 0.1938, Test corr: 0.1786\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [80/150] Train corr: 0.1595, Valid corr: 0.1937, Test corr: 0.1776\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [85/150] Train corr: 0.1602, Valid corr: 0.1924, Test corr: 0.1786\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [90/150] Train corr: 0.1928, Valid corr: 0.1926, Test corr: 0.1776\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [95/150] Train corr: 0.2014, Valid corr: 0.1916, Test corr: 0.1767\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [100/150] Train corr: 0.2040, Valid corr: 0.1900, Test corr: 0.1761\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [105/150] Train corr: 0.1846, Valid corr: 0.1902, Test corr: 0.1777\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [110/150] Train corr: 0.2039, Valid corr: 0.1888, Test corr: 0.1766\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [115/150] Train corr: 0.1987, Valid corr: 0.1900, Test corr: 0.1780\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [120/150] Train corr: 0.1854, Valid corr: 0.1890, Test corr: 0.1771\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [125/150] Train corr: 0.2427, Valid corr: 0.1865, Test corr: 0.1789\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [130/150] Train corr: 0.1993, Valid corr: 0.1865, Test corr: 0.1764\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [135/150] Train corr: 0.2325, Valid corr: 0.1858, Test corr: 0.1811\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [140/150] Train corr: 0.2154, Valid corr: 0.1843, Test corr: 0.1796\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [145/150] Train corr: 0.2029, Valid corr: 0.1839, Test corr: 0.1783\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [150/150] Train corr: 0.2432, Valid corr: 0.1818, Test corr: 0.1785\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Outer Fold [2/18] train corr: 0.2432, valid corr: 0.1818, test corr: 0.1785\n",
      "Execution Time for Fold: 33.74 mins\n",
      "\n",
      "=================================== Outer Fold [3/18] ===================================\n",
      "Epoch [5/150] Train corr: 0.0178, Valid corr: 0.1014, Test corr: 0.0476\n",
      "Layer 1: [0.4769/0.9000] Layer 2: [0.3068/0.3000] Layer 3: [0.3862/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [10/150] Train corr: 0.0450, Valid corr: 0.0970, Test corr: 0.0367\n",
      "Layer 1: [0.7278/0.9000] Layer 2: [0.3065/0.3000] Layer 3: [0.5035/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [15/150] Train corr: 0.0304, Valid corr: 0.1002, Test corr: 0.1286\n",
      "Layer 1: [0.8106/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5017/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [20/150] Train corr: 0.0593, Valid corr: 0.0842, Test corr: 0.0922\n",
      "Layer 1: [0.6305/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5024/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [25/150] Train corr: 0.0530, Valid corr: 0.1486, Test corr: 0.1474\n",
      "Layer 1: [0.7650/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5023/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [30/150] Train corr: 0.0788, Valid corr: 0.1109, Test corr: 0.1433\n",
      "Layer 1: [0.8287/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 3.12e-6\n",
      "Epoch [35/150] Train corr: 0.0927, Valid corr: 0.1291, Test corr: 0.1420\n",
      "Layer 1: [0.8816/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [40/150] Train corr: 0.1318, Valid corr: 0.1518, Test corr: 0.1570\n",
      "Layer 1: [0.8826/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [45/150] Train corr: 0.1440, Valid corr: 0.1356, Test corr: 0.1615\n",
      "Layer 1: [0.9011/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [50/150] Train corr: 0.1906, Valid corr: 0.1389, Test corr: 0.1462\n",
      "Layer 1: [0.9017/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [55/150] Train corr: 0.2256, Valid corr: 0.1372, Test corr: 0.1467\n",
      "Layer 1: [0.8997/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [60/150] Train corr: 0.2277, Valid corr: 0.1377, Test corr: 0.1432\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [65/150] Train corr: 0.2399, Valid corr: 0.1373, Test corr: 0.1438\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [70/150] Train corr: 0.2438, Valid corr: 0.1373, Test corr: 0.1439\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [75/150] Train corr: 0.2606, Valid corr: 0.1373, Test corr: 0.1425\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [80/150] Train corr: 0.2805, Valid corr: 0.1382, Test corr: 0.1439\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [85/150] Train corr: 0.2605, Valid corr: 0.1379, Test corr: 0.1426\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [90/150] Train corr: 0.2736, Valid corr: 0.1378, Test corr: 0.1419\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [95/150] Train corr: 0.2833, Valid corr: 0.1375, Test corr: 0.1443\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [100/150] Train corr: 0.2836, Valid corr: 0.1382, Test corr: 0.1453\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [105/150] Train corr: 0.3019, Valid corr: 0.1372, Test corr: 0.1467\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [110/150] Train corr: 0.2958, Valid corr: 0.1366, Test corr: 0.1442\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [115/150] Train corr: 0.2914, Valid corr: 0.1372, Test corr: 0.1458\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [120/150] Train corr: 0.2922, Valid corr: 0.1352, Test corr: 0.1441\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [125/150] Train corr: 0.3046, Valid corr: 0.1338, Test corr: 0.1452\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [130/150] Train corr: 0.3281, Valid corr: 0.1357, Test corr: 0.1444\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [135/150] Train corr: 0.3239, Valid corr: 0.1321, Test corr: 0.1429\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [140/150] Train corr: 0.3199, Valid corr: 0.1313, Test corr: 0.1448\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [145/150] Train corr: 0.3218, Valid corr: 0.1319, Test corr: 0.1463\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/150] Train corr: 0.3253, Valid corr: 0.1304, Test corr: 0.1451\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Outer Fold [3/18] train corr: 0.3253, valid corr: 0.1304, test corr: 0.1451\n",
      "Execution Time for Fold: 35.78 mins\n",
      "\n",
      "=================================== Outer Fold [4/18] ===================================\n",
      "Epoch [5/150] Train corr: 0.0312, Valid corr: 0.0980, Test corr: -0.0493\n",
      "Layer 1: [0.4847/0.9000] Layer 2: [0.3069/0.3000] Layer 3: [0.3910/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [10/150] Train corr: 0.0307, Valid corr: 0.1298, Test corr: -0.0442\n",
      "Layer 1: [0.7386/0.9000] Layer 2: [0.3066/0.3000] Layer 3: [0.5024/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [15/150] Train corr: 0.0387, Valid corr: 0.0860, Test corr: -0.0039\n",
      "Layer 1: [0.8141/0.9000] Layer 2: [0.3065/0.3000] Layer 3: [0.5025/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [20/150] Train corr: 0.0231, Valid corr: 0.1152, Test corr: -0.0902\n",
      "Layer 1: [0.6083/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5024/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [25/150] Train corr: 0.0488, Valid corr: 0.2206, Test corr: 0.0280\n",
      "Layer 1: [0.7165/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [30/150] Train corr: 0.0695, Valid corr: 0.1952, Test corr: -0.0143\n",
      "Layer 1: [0.7959/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 3.12e-6\n",
      "Epoch [35/150] Train corr: 0.0922, Valid corr: 0.2182, Test corr: 0.0197\n",
      "Layer 1: [0.8642/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [40/150] Train corr: 0.1208, Valid corr: 0.1995, Test corr: -0.0175\n",
      "Layer 1: [0.8536/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [45/150] Train corr: 0.1450, Valid corr: 0.2069, Test corr: -0.0204\n",
      "Layer 1: [0.8989/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [50/150] Train corr: 0.1513, Valid corr: 0.1999, Test corr: -0.0112\n",
      "Layer 1: [0.8988/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [55/150] Train corr: 0.1606, Valid corr: 0.1992, Test corr: -0.0043\n",
      "Layer 1: [0.8993/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [60/150] Train corr: 0.1772, Valid corr: 0.2088, Test corr: -0.0134\n",
      "Layer 1: [0.9008/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [65/150] Train corr: 0.1979, Valid corr: 0.2131, Test corr: -0.0191\n",
      "Layer 1: [0.9014/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [70/150] Train corr: 0.2070, Valid corr: 0.2027, Test corr: -0.0126\n",
      "Layer 1: [0.9012/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [75/150] Train corr: 0.2344, Valid corr: 0.2044, Test corr: -0.0158\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [80/150] Train corr: 0.2385, Valid corr: 0.2055, Test corr: -0.0136\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [85/150] Train corr: 0.2457, Valid corr: 0.2024, Test corr: -0.0125\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [90/150] Train corr: 0.2442, Valid corr: 0.2019, Test corr: -0.0119\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [95/150] Train corr: 0.2631, Valid corr: 0.1994, Test corr: -0.0084\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [100/150] Train corr: 0.2644, Valid corr: 0.1997, Test corr: -0.0094\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [105/150] Train corr: 0.2619, Valid corr: 0.1992, Test corr: -0.0069\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [110/150] Train corr: 0.2614, Valid corr: 0.1986, Test corr: -0.0090\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [115/150] Train corr: 0.2678, Valid corr: 0.2006, Test corr: -0.0095\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [120/150] Train corr: 0.2777, Valid corr: 0.1990, Test corr: -0.0055\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [125/150] Train corr: 0.2707, Valid corr: 0.1976, Test corr: -0.0025\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [130/150] Train corr: 0.2791, Valid corr: 0.1947, Test corr: -0.0018\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [135/150] Train corr: 0.2770, Valid corr: 0.1935, Test corr: -0.0016\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [140/150] Train corr: 0.2909, Valid corr: 0.1944, Test corr: -0.0050\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [145/150] Train corr: 0.2862, Valid corr: 0.1935, Test corr: 0.0012\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [150/150] Train corr: 0.3046, Valid corr: 0.1927, Test corr: 0.0033\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Outer Fold [4/18] train corr: 0.3046, valid corr: 0.1927, test corr: 0.0033\n",
      "Execution Time for Fold: 37.15 mins\n",
      "\n",
      "=================================== Outer Fold [5/18] ===================================\n",
      "Epoch [5/150] Train corr: 0.0068, Valid corr: 0.1235, Test corr: 0.0825\n",
      "Layer 1: [0.4768/0.9000] Layer 2: [0.3068/0.3000] Layer 3: [0.3854/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [10/150] Train corr: 0.0146, Valid corr: 0.0830, Test corr: 0.1212\n",
      "Layer 1: [0.7281/0.9000] Layer 2: [0.3065/0.3000] Layer 3: [0.5026/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [15/150] Train corr: 0.0254, Valid corr: 0.0642, Test corr: 0.1341\n",
      "Layer 1: [0.8098/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5025/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [20/150] Train corr: 0.0364, Valid corr: 0.0863, Test corr: 0.0850\n",
      "Layer 1: [0.6342/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [25/150] Train corr: 0.0367, Valid corr: 0.1022, Test corr: 0.0819\n",
      "Layer 1: [0.7537/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [30/150] Train corr: 0.0593, Valid corr: 0.1194, Test corr: 0.0862\n",
      "Layer 1: [0.8164/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 3.12e-6\n",
      "Epoch [35/150] Train corr: 0.1044, Valid corr: 0.1604, Test corr: 0.1422\n",
      "Layer 1: [0.8784/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [40/150] Train corr: 0.1023, Valid corr: 0.1460, Test corr: 0.1187\n",
      "Layer 1: [0.8768/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [45/150] Train corr: 0.1327, Valid corr: 0.1455, Test corr: 0.1254\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [50/150] Train corr: 0.1417, Valid corr: 0.1568, Test corr: 0.1212\n",
      "Layer 1: [0.9013/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [55/150] Train corr: 0.1790, Valid corr: 0.1632, Test corr: 0.1214\n",
      "Layer 1: [0.9008/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [60/150] Train corr: 0.1814, Valid corr: 0.1633, Test corr: 0.1201\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [65/150] Train corr: 0.1641, Valid corr: 0.1617, Test corr: 0.1199\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/150] Train corr: 0.1675, Valid corr: 0.1606, Test corr: 0.1208\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [75/150] Train corr: 0.1914, Valid corr: 0.1568, Test corr: 0.1204\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [80/150] Train corr: 0.1465, Valid corr: 0.1560, Test corr: 0.1212\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [85/150] Train corr: 0.1963, Valid corr: 0.1574, Test corr: 0.1229\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [90/150] Train corr: 0.2228, Valid corr: 0.1561, Test corr: 0.1212\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [95/150] Train corr: 0.2022, Valid corr: 0.1545, Test corr: 0.1221\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [100/150] Train corr: 0.1962, Valid corr: 0.1528, Test corr: 0.1223\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [105/150] Train corr: 0.2176, Valid corr: 0.1538, Test corr: 0.1229\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [110/150] Train corr: 0.2210, Valid corr: 0.1530, Test corr: 0.1259\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [115/150] Train corr: 0.2213, Valid corr: 0.1547, Test corr: 0.1236\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [120/150] Train corr: 0.2358, Valid corr: 0.1532, Test corr: 0.1236\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [125/150] Train corr: 0.2261, Valid corr: 0.1537, Test corr: 0.1233\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [130/150] Train corr: 0.2176, Valid corr: 0.1517, Test corr: 0.1239\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [135/150] Train corr: 0.2329, Valid corr: 0.1496, Test corr: 0.1229\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [140/150] Train corr: 0.2272, Valid corr: 0.1488, Test corr: 0.1230\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [145/150] Train corr: 0.2422, Valid corr: 0.1502, Test corr: 0.1212\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [150/150] Train corr: 0.2521, Valid corr: 0.1497, Test corr: 0.1236\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Outer Fold [5/18] train corr: 0.2521, valid corr: 0.1497, test corr: 0.1236\n",
      "Execution Time for Fold: 42.46 mins\n",
      "\n",
      "=================================== Outer Fold [6/18] ===================================\n",
      "Epoch [5/150] Train corr: 0.0041, Valid corr: 0.0272, Test corr: 0.1549\n",
      "Layer 1: [0.4879/0.9000] Layer 2: [0.3068/0.3000] Layer 3: [0.3929/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [10/150] Train corr: -0.0060, Valid corr: 0.0762, Test corr: 0.1297\n",
      "Layer 1: [0.7427/0.9000] Layer 2: [0.3066/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [15/150] Train corr: 0.0491, Valid corr: 0.0824, Test corr: 0.0066\n",
      "Layer 1: [0.8040/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5025/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [20/150] Train corr: 0.0533, Valid corr: 0.0753, Test corr: 0.0787\n",
      "Layer 1: [0.5947/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5023/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [25/150] Train corr: 0.0340, Valid corr: 0.0817, Test corr: 0.1413\n",
      "Layer 1: [0.7068/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [30/150] Train corr: 0.0651, Valid corr: 0.1298, Test corr: 0.1275\n",
      "Layer 1: [0.7777/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 3.12e-6\n",
      "Epoch [35/150] Train corr: 0.1107, Valid corr: 0.1280, Test corr: 0.0960\n",
      "Layer 1: [0.8536/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [40/150] Train corr: 0.1052, Valid corr: 0.1154, Test corr: 0.1281\n",
      "Layer 1: [0.8485/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [45/150] Train corr: 0.1502, Valid corr: 0.1213, Test corr: 0.1199\n",
      "Layer 1: [0.8958/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [50/150] Train corr: 0.1345, Valid corr: 0.1452, Test corr: 0.1208\n",
      "Layer 1: [0.8996/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [55/150] Train corr: 0.1503, Valid corr: 0.1338, Test corr: 0.1231\n",
      "Layer 1: [0.8993/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [60/150] Train corr: 0.1934, Valid corr: 0.1394, Test corr: 0.1310\n",
      "Layer 1: [0.9011/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [65/150] Train corr: 0.2087, Valid corr: 0.1372, Test corr: 0.1281\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [70/150] Train corr: 0.2086, Valid corr: 0.1376, Test corr: 0.1261\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [75/150] Train corr: 0.2327, Valid corr: 0.1399, Test corr: 0.1220\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [80/150] Train corr: 0.2494, Valid corr: 0.1408, Test corr: 0.1238\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [85/150] Train corr: 0.2387, Valid corr: 0.1403, Test corr: 0.1238\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [90/150] Train corr: 0.2269, Valid corr: 0.1394, Test corr: 0.1194\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [95/150] Train corr: 0.2414, Valid corr: 0.1415, Test corr: 0.1182\n",
      "Layer 1: [0.9003/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [100/150] Train corr: 0.2674, Valid corr: 0.1415, Test corr: 0.1194\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [105/150] Train corr: 0.2422, Valid corr: 0.1451, Test corr: 0.1131\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [110/150] Train corr: 0.2756, Valid corr: 0.1444, Test corr: 0.1145\n",
      "Layer 1: [0.9003/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [115/150] Train corr: 0.2828, Valid corr: 0.1455, Test corr: 0.1153\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [120/150] Train corr: 0.2760, Valid corr: 0.1424, Test corr: 0.1113\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [125/150] Train corr: 0.2952, Valid corr: 0.1420, Test corr: 0.1074\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [130/150] Train corr: 0.2986, Valid corr: 0.1431, Test corr: 0.1059\n",
      "Layer 1: [0.8998/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [135/150] Train corr: 0.3047, Valid corr: 0.1424, Test corr: 0.1042\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [140/150] Train corr: 0.3204, Valid corr: 0.1419, Test corr: 0.1035\n",
      "Layer 1: [0.9003/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [145/150] Train corr: 0.3006, Valid corr: 0.1426, Test corr: 0.1070\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [150/150] Train corr: 0.3114, Valid corr: 0.1439, Test corr: 0.1075\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Outer Fold [6/18] train corr: 0.3114, valid corr: 0.1439, test corr: 0.1075\n",
      "Execution Time for Fold: 54.02 mins\n",
      "\n",
      "=================================== Outer Fold [7/18] ===================================\n",
      "Epoch [5/150] Train corr: 0.0263, Valid corr: 0.0745, Test corr: 0.0337\n",
      "Layer 1: [0.4881/0.9000] Layer 2: [0.3069/0.3000] Layer 3: [0.3932/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [10/150] Train corr: 0.0221, Valid corr: 0.1324, Test corr: 0.0417\n",
      "Layer 1: [0.7426/0.9000] Layer 2: [0.3066/0.3000] Layer 3: [0.5026/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [15/150] Train corr: 0.0259, Valid corr: 0.1205, Test corr: 0.0438\n",
      "Layer 1: [0.8088/0.9000] Layer 2: [0.3065/0.3000] Layer 3: [0.5023/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [20/150] Train corr: 0.0272, Valid corr: 0.0196, Test corr: 0.0367\n",
      "Layer 1: [0.6014/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5026/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [25/150] Train corr: 0.0444, Valid corr: 0.0653, Test corr: 0.0427\n",
      "Layer 1: [0.7131/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5023/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [30/150] Train corr: 0.0682, Valid corr: 0.1235, Test corr: 0.0832\n",
      "Layer 1: [0.7946/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 3.12e-6\n",
      "Epoch [35/150] Train corr: 0.1009, Valid corr: 0.1347, Test corr: 0.0673\n",
      "Layer 1: [0.8595/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [40/150] Train corr: 0.0664, Valid corr: 0.1366, Test corr: 0.0456\n",
      "Layer 1: [0.8529/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [45/150] Train corr: 0.1362, Valid corr: 0.1498, Test corr: 0.0804\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [50/150] Train corr: 0.1596, Valid corr: 0.1323, Test corr: 0.0961\n",
      "Layer 1: [0.8995/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [55/150] Train corr: 0.1870, Valid corr: 0.1413, Test corr: 0.0952\n",
      "Layer 1: [0.9004/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [60/150] Train corr: 0.2014, Valid corr: 0.1504, Test corr: 0.0953\n",
      "Layer 1: [0.9014/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [65/150] Train corr: 0.2184, Valid corr: 0.1621, Test corr: 0.1026\n",
      "Layer 1: [0.9005/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [70/150] Train corr: 0.2552, Valid corr: 0.1618, Test corr: 0.1078\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [75/150] Train corr: 0.2544, Valid corr: 0.1632, Test corr: 0.1056\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [80/150] Train corr: 0.2531, Valid corr: 0.1608, Test corr: 0.1131\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [85/150] Train corr: 0.2520, Valid corr: 0.1605, Test corr: 0.1101\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [90/150] Train corr: 0.2611, Valid corr: 0.1620, Test corr: 0.1128\n",
      "Layer 1: [0.9003/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [95/150] Train corr: 0.2682, Valid corr: 0.1631, Test corr: 0.1098\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [100/150] Train corr: 0.2644, Valid corr: 0.1647, Test corr: 0.1069\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [105/150] Train corr: 0.2787, Valid corr: 0.1656, Test corr: 0.1055\n",
      "Layer 1: [0.9003/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [110/150] Train corr: 0.2845, Valid corr: 0.1639, Test corr: 0.1090\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [115/150] Train corr: 0.2977, Valid corr: 0.1672, Test corr: 0.1038\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [120/150] Train corr: 0.3143, Valid corr: 0.1647, Test corr: 0.1036\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [125/150] Train corr: 0.3149, Valid corr: 0.1650, Test corr: 0.1070\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [130/150] Train corr: 0.3067, Valid corr: 0.1661, Test corr: 0.1039\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [135/150] Train corr: 0.3038, Valid corr: 0.1663, Test corr: 0.1028\n",
      "Layer 1: [0.8998/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [140/150] Train corr: 0.3032, Valid corr: 0.1671, Test corr: 0.0994\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [145/150] Train corr: 0.3304, Valid corr: 0.1701, Test corr: 0.1026\n",
      "Layer 1: [0.9003/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [150/150] Train corr: 0.3313, Valid corr: 0.1684, Test corr: 0.1042\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Outer Fold [7/18] train corr: 0.3313, valid corr: 0.1684, test corr: 0.1042\n",
      "Execution Time for Fold: 49.79 mins\n",
      "\n",
      "=================================== Outer Fold [8/18] ===================================\n",
      "Epoch [5/150] Train corr: 0.0068, Valid corr: 0.1722, Test corr: 0.1594\n",
      "Layer 1: [0.4815/0.9000] Layer 2: [0.3068/0.3000] Layer 3: [0.3893/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [10/150] Train corr: 0.0390, Valid corr: 0.0786, Test corr: 0.0662\n",
      "Layer 1: [0.7343/0.9000] Layer 2: [0.3066/0.3000] Layer 3: [0.5026/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [15/150] Train corr: 0.0265, Valid corr: 0.1382, Test corr: 0.1844\n",
      "Layer 1: [0.8125/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5027/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [20/150] Train corr: 0.0499, Valid corr: 0.1328, Test corr: 0.1087\n",
      "Layer 1: [0.6119/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5023/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [25/150] Train corr: 0.0332, Valid corr: 0.2048, Test corr: 0.1408\n",
      "Layer 1: [0.7269/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [30/150] Train corr: 0.0475, Valid corr: 0.2356, Test corr: 0.1764\n",
      "Layer 1: [0.7915/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5019/0.5000] \n",
      "Current learning rate: 3.12e-6\n",
      "Epoch [35/150] Train corr: 0.0781, Valid corr: 0.1975, Test corr: 0.2224\n",
      "Layer 1: [0.8637/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [40/150] Train corr: 0.0751, Valid corr: 0.2121, Test corr: 0.2271\n",
      "Layer 1: [0.8608/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [45/150] Train corr: 0.1163, Valid corr: 0.2012, Test corr: 0.2341\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [50/150] Train corr: 0.1201, Valid corr: 0.2323, Test corr: 0.2338\n",
      "Layer 1: [0.9005/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [55/150] Train corr: 0.1007, Valid corr: 0.2102, Test corr: 0.2354\n",
      "Layer 1: [0.8991/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [60/150] Train corr: 0.1523, Valid corr: 0.2068, Test corr: 0.2398\n",
      "Layer 1: [0.9005/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 4.88e-8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/150] Train corr: 0.1699, Valid corr: 0.2123, Test corr: 0.2389\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [70/150] Train corr: 0.1788, Valid corr: 0.2058, Test corr: 0.2377\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [75/150] Train corr: 0.1826, Valid corr: 0.2028, Test corr: 0.2387\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [80/150] Train corr: 0.1753, Valid corr: 0.2036, Test corr: 0.2390\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [85/150] Train corr: 0.1846, Valid corr: 0.2024, Test corr: 0.2386\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [90/150] Train corr: 0.1931, Valid corr: 0.2027, Test corr: 0.2391\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [95/150] Train corr: 0.1946, Valid corr: 0.2073, Test corr: 0.2388\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [100/150] Train corr: 0.1982, Valid corr: 0.2052, Test corr: 0.2398\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [105/150] Train corr: 0.1988, Valid corr: 0.2014, Test corr: 0.2402\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [110/150] Train corr: 0.2123, Valid corr: 0.2020, Test corr: 0.2417\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [115/150] Train corr: 0.2440, Valid corr: 0.2028, Test corr: 0.2403\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [120/150] Train corr: 0.2103, Valid corr: 0.2010, Test corr: 0.2446\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [125/150] Train corr: 0.2389, Valid corr: 0.2002, Test corr: 0.2409\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [130/150] Train corr: 0.2352, Valid corr: 0.2016, Test corr: 0.2407\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [135/150] Train corr: 0.2213, Valid corr: 0.2027, Test corr: 0.2421\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [140/150] Train corr: 0.2361, Valid corr: 0.2015, Test corr: 0.2426\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [145/150] Train corr: 0.2346, Valid corr: 0.2038, Test corr: 0.2435\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [150/150] Train corr: 0.2413, Valid corr: 0.2017, Test corr: 0.2417\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Outer Fold [8/18] train corr: 0.2413, valid corr: 0.2017, test corr: 0.2417\n",
      "Execution Time for Fold: 47.27 mins\n",
      "\n",
      "=================================== Outer Fold [9/18] ===================================\n",
      "Epoch [5/150] Train corr: 0.0195, Valid corr: 0.0600, Test corr: 0.1256\n",
      "Layer 1: [0.4799/0.9000] Layer 2: [0.3069/0.3000] Layer 3: [0.3874/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [10/150] Train corr: 0.0226, Valid corr: 0.0643, Test corr: 0.0904\n",
      "Layer 1: [0.7321/0.9000] Layer 2: [0.3067/0.3000] Layer 3: [0.5031/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [15/150] Train corr: 0.0146, Valid corr: 0.0130, Test corr: 0.1231\n",
      "Layer 1: [0.8124/0.9000] Layer 2: [0.3065/0.3000] Layer 3: [0.5017/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [20/150] Train corr: 0.0446, Valid corr: 0.0612, Test corr: 0.1136\n",
      "Layer 1: [0.6356/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5024/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [25/150] Train corr: 0.0655, Valid corr: 0.0858, Test corr: 0.1158\n",
      "Layer 1: [0.7517/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [30/150] Train corr: 0.0689, Valid corr: 0.0642, Test corr: 0.1090\n",
      "Layer 1: [0.8286/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 3.12e-6\n",
      "Epoch [35/150] Train corr: 0.0866, Valid corr: 0.0818, Test corr: 0.0962\n",
      "Layer 1: [0.8159/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 3.12e-6\n",
      "Epoch [40/150] Train corr: 0.0944, Valid corr: 0.0864, Test corr: 0.1032\n",
      "Layer 1: [0.8712/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [45/150] Train corr: 0.1267, Valid corr: 0.0993, Test corr: 0.1029\n",
      "Layer 1: [0.8674/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [50/150] Train corr: 0.1657, Valid corr: 0.0958, Test corr: 0.1430\n",
      "Layer 1: [0.9013/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [55/150] Train corr: 0.1949, Valid corr: 0.0982, Test corr: 0.1355\n",
      "Layer 1: [0.8986/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [60/150] Train corr: 0.2143, Valid corr: 0.0997, Test corr: 0.1371\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [65/150] Train corr: 0.2179, Valid corr: 0.0986, Test corr: 0.1402\n",
      "Layer 1: [0.9007/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [70/150] Train corr: 0.2405, Valid corr: 0.0974, Test corr: 0.1428\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [75/150] Train corr: 0.2527, Valid corr: 0.0984, Test corr: 0.1422\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [80/150] Train corr: 0.2542, Valid corr: 0.0984, Test corr: 0.1438\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [85/150] Train corr: 0.2686, Valid corr: 0.0973, Test corr: 0.1452\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [90/150] Train corr: 0.2769, Valid corr: 0.0973, Test corr: 0.1456\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [95/150] Train corr: 0.2648, Valid corr: 0.0949, Test corr: 0.1468\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [100/150] Train corr: 0.2838, Valid corr: 0.0971, Test corr: 0.1436\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [105/150] Train corr: 0.2810, Valid corr: 0.0952, Test corr: 0.1465\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [110/150] Train corr: 0.3031, Valid corr: 0.0956, Test corr: 0.1446\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [115/150] Train corr: 0.3002, Valid corr: 0.0964, Test corr: 0.1419\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [120/150] Train corr: 0.3051, Valid corr: 0.0960, Test corr: 0.1448\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [125/150] Train corr: 0.3102, Valid corr: 0.0947, Test corr: 0.1423\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [130/150] Train corr: 0.3067, Valid corr: 0.0949, Test corr: 0.1407\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [135/150] Train corr: 0.3369, Valid corr: 0.0961, Test corr: 0.1396\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/150] Train corr: 0.3358, Valid corr: 0.0939, Test corr: 0.1405\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [145/150] Train corr: 0.3309, Valid corr: 0.0932, Test corr: 0.1411\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [150/150] Train corr: 0.3315, Valid corr: 0.0980, Test corr: 0.1381\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Outer Fold [9/18] train corr: 0.3315, valid corr: 0.0980, test corr: 0.1381\n",
      "Execution Time for Fold: 46.16 mins\n",
      "\n",
      "=================================== Outer Fold [10/18] ===================================\n",
      "Epoch [5/150] Train corr: 0.0129, Valid corr: 0.1411, Test corr: 0.1175\n",
      "Layer 1: [0.4832/0.9000] Layer 2: [0.3068/0.3000] Layer 3: [0.3899/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [10/150] Train corr: 0.0222, Valid corr: 0.1191, Test corr: 0.1804\n",
      "Layer 1: [0.7367/0.9000] Layer 2: [0.3065/0.3000] Layer 3: [0.5027/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [15/150] Train corr: 0.0472, Valid corr: 0.1354, Test corr: 0.1203\n",
      "Layer 1: [0.8098/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5026/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [20/150] Train corr: 0.0486, Valid corr: 0.1410, Test corr: 0.1005\n",
      "Layer 1: [0.6108/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5026/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [25/150] Train corr: 0.0327, Valid corr: 0.1323, Test corr: 0.2166\n",
      "Layer 1: [0.7220/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5023/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [30/150] Train corr: 0.0568, Valid corr: 0.1207, Test corr: 0.1279\n",
      "Layer 1: [0.8011/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5023/0.5000] \n",
      "Current learning rate: 3.12e-6\n",
      "Epoch [35/150] Train corr: 0.0926, Valid corr: 0.1462, Test corr: 0.1610\n",
      "Layer 1: [0.8702/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5023/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [40/150] Train corr: 0.1097, Valid corr: 0.1643, Test corr: 0.2069\n",
      "Layer 1: [0.8699/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [45/150] Train corr: 0.1526, Valid corr: 0.1639, Test corr: 0.2002\n",
      "Layer 1: [0.9018/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [50/150] Train corr: 0.1617, Valid corr: 0.1694, Test corr: 0.2128\n",
      "Layer 1: [0.8981/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [55/150] Train corr: 0.1833, Valid corr: 0.1638, Test corr: 0.2060\n",
      "Layer 1: [0.9003/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [60/150] Train corr: 0.1927, Valid corr: 0.1613, Test corr: 0.2050\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [65/150] Train corr: 0.2163, Valid corr: 0.1590, Test corr: 0.2049\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [70/150] Train corr: 0.2380, Valid corr: 0.1562, Test corr: 0.2047\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [75/150] Train corr: 0.2448, Valid corr: 0.1546, Test corr: 0.2051\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [80/150] Train corr: 0.2242, Valid corr: 0.1530, Test corr: 0.2050\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [85/150] Train corr: 0.2558, Valid corr: 0.1519, Test corr: 0.2069\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [90/150] Train corr: 0.2406, Valid corr: 0.1523, Test corr: 0.2059\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [95/150] Train corr: 0.2338, Valid corr: 0.1519, Test corr: 0.2031\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [100/150] Train corr: 0.2522, Valid corr: 0.1498, Test corr: 0.2049\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [105/150] Train corr: 0.2339, Valid corr: 0.1485, Test corr: 0.2082\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [110/150] Train corr: 0.2548, Valid corr: 0.1496, Test corr: 0.2044\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [115/150] Train corr: 0.2400, Valid corr: 0.1487, Test corr: 0.2028\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [120/150] Train corr: 0.2683, Valid corr: 0.1468, Test corr: 0.2002\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [125/150] Train corr: 0.2604, Valid corr: 0.1465, Test corr: 0.2022\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [130/150] Train corr: 0.2855, Valid corr: 0.1475, Test corr: 0.2019\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [135/150] Train corr: 0.2909, Valid corr: 0.1480, Test corr: 0.2054\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [140/150] Train corr: 0.2716, Valid corr: 0.1469, Test corr: 0.2038\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [145/150] Train corr: 0.3066, Valid corr: 0.1446, Test corr: 0.2051\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [150/150] Train corr: 0.3029, Valid corr: 0.1453, Test corr: 0.2029\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Outer Fold [10/18] train corr: 0.3029, valid corr: 0.1453, test corr: 0.2029\n",
      "Execution Time for Fold: 50.01 mins\n",
      "\n",
      "=================================== Outer Fold [11/18] ===================================\n",
      "Epoch [5/150] Train corr: -0.0084, Valid corr: 0.1575, Test corr: 0.1857\n",
      "Layer 1: [0.4927/0.9000] Layer 2: [0.3068/0.3000] Layer 3: [0.3965/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [10/150] Train corr: 0.0073, Valid corr: 0.1650, Test corr: 0.1525\n",
      "Layer 1: [0.7479/0.9000] Layer 2: [0.3066/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [15/150] Train corr: 0.0174, Valid corr: 0.1355, Test corr: 0.1979\n",
      "Layer 1: [0.8001/0.9000] Layer 2: [0.3065/0.3000] Layer 3: [0.5027/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [20/150] Train corr: 0.0214, Valid corr: 0.1014, Test corr: 0.1254\n",
      "Layer 1: [0.5536/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5025/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [25/150] Train corr: 0.0521, Valid corr: 0.1199, Test corr: 0.2123\n",
      "Layer 1: [0.6646/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [30/150] Train corr: 0.0544, Valid corr: 0.1696, Test corr: 0.2227\n",
      "Layer 1: [0.7487/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 3.12e-6\n",
      "Epoch [35/150] Train corr: 0.0903, Valid corr: 0.1528, Test corr: 0.1951\n",
      "Layer 1: [0.8291/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [40/150] Train corr: 0.1050, Valid corr: 0.1558, Test corr: 0.2030\n",
      "Layer 1: [0.8282/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [45/150] Train corr: 0.1234, Valid corr: 0.1656, Test corr: 0.2086\n",
      "Layer 1: [0.8840/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [50/150] Train corr: 0.1526, Valid corr: 0.1761, Test corr: 0.2018\n",
      "Layer 1: [0.8956/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [55/150] Train corr: 0.1545, Valid corr: 0.1697, Test corr: 0.2159\n",
      "Layer 1: [0.8992/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 4.88e-8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/150] Train corr: 0.1516, Valid corr: 0.1725, Test corr: 0.2201\n",
      "Layer 1: [0.9007/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [65/150] Train corr: 0.2034, Valid corr: 0.1733, Test corr: 0.2335\n",
      "Layer 1: [0.9010/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [70/150] Train corr: 0.1672, Valid corr: 0.1687, Test corr: 0.2424\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [75/150] Train corr: 0.1965, Valid corr: 0.1673, Test corr: 0.2452\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [80/150] Train corr: 0.2138, Valid corr: 0.1704, Test corr: 0.2446\n",
      "Layer 1: [0.9003/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [85/150] Train corr: 0.2022, Valid corr: 0.1681, Test corr: 0.2501\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [90/150] Train corr: 0.2368, Valid corr: 0.1670, Test corr: 0.2481\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [95/150] Train corr: 0.2473, Valid corr: 0.1645, Test corr: 0.2430\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [100/150] Train corr: 0.2093, Valid corr: 0.1646, Test corr: 0.2515\n",
      "Layer 1: [0.9004/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [105/150] Train corr: 0.2474, Valid corr: 0.1595, Test corr: 0.2475\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [110/150] Train corr: 0.2337, Valid corr: 0.1579, Test corr: 0.2386\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [115/150] Train corr: 0.2446, Valid corr: 0.1512, Test corr: 0.2364\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [120/150] Train corr: 0.2685, Valid corr: 0.1493, Test corr: 0.2401\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [125/150] Train corr: 0.2544, Valid corr: 0.1470, Test corr: 0.2415\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [130/150] Train corr: 0.2599, Valid corr: 0.1443, Test corr: 0.2355\n",
      "Layer 1: [0.9005/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [135/150] Train corr: 0.2774, Valid corr: 0.1436, Test corr: 0.2346\n",
      "Layer 1: [0.9003/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [140/150] Train corr: 0.2626, Valid corr: 0.1494, Test corr: 0.2405\n",
      "Layer 1: [0.8998/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [145/150] Train corr: 0.2594, Valid corr: 0.1427, Test corr: 0.2413\n",
      "Layer 1: [0.8997/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [150/150] Train corr: 0.2842, Valid corr: 0.1397, Test corr: 0.2380\n",
      "Layer 1: [0.9005/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Outer Fold [11/18] train corr: 0.2842, valid corr: 0.1397, test corr: 0.2380\n",
      "Execution Time for Fold: 45.86 mins\n",
      "\n",
      "=================================== Outer Fold [12/18] ===================================\n",
      "Epoch [5/150] Train corr: 0.0248, Valid corr: 0.1191, Test corr: 0.0831\n",
      "Layer 1: [0.4800/0.9000] Layer 2: [0.3068/0.3000] Layer 3: [0.3881/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [10/150] Train corr: 0.0333, Valid corr: 0.0891, Test corr: 0.0897\n",
      "Layer 1: [0.7329/0.9000] Layer 2: [0.3066/0.3000] Layer 3: [0.5031/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [15/150] Train corr: 0.0360, Valid corr: 0.0802, Test corr: 0.0199\n",
      "Layer 1: [0.8130/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [20/150] Train corr: 0.0443, Valid corr: 0.0840, Test corr: 0.0233\n",
      "Layer 1: [0.6246/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5026/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [25/150] Train corr: 0.0544, Valid corr: 0.1311, Test corr: 0.1378\n",
      "Layer 1: [0.7472/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [30/150] Train corr: 0.0672, Valid corr: 0.1112, Test corr: 0.0892\n",
      "Layer 1: [0.8197/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5023/0.5000] \n",
      "Current learning rate: 3.12e-6\n",
      "Epoch [35/150] Train corr: 0.1185, Valid corr: 0.1378, Test corr: 0.0735\n",
      "Layer 1: [0.8740/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5023/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [40/150] Train corr: 0.1170, Valid corr: 0.1516, Test corr: 0.0812\n",
      "Layer 1: [0.8756/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5023/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [45/150] Train corr: 0.1156, Valid corr: 0.1322, Test corr: 0.0831\n",
      "Layer 1: [0.8695/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5023/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [50/150] Train corr: 0.1634, Valid corr: 0.1534, Test corr: 0.1132\n",
      "Layer 1: [0.8998/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5023/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [55/150] Train corr: 0.1860, Valid corr: 0.1609, Test corr: 0.1131\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5023/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [60/150] Train corr: 0.2285, Valid corr: 0.1637, Test corr: 0.0981\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [65/150] Train corr: 0.2295, Valid corr: 0.1636, Test corr: 0.0957\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [70/150] Train corr: 0.2254, Valid corr: 0.1637, Test corr: 0.0959\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [75/150] Train corr: 0.2341, Valid corr: 0.1604, Test corr: 0.0952\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [80/150] Train corr: 0.2451, Valid corr: 0.1590, Test corr: 0.0959\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [85/150] Train corr: 0.2425, Valid corr: 0.1587, Test corr: 0.0939\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [90/150] Train corr: 0.2486, Valid corr: 0.1579, Test corr: 0.0943\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [95/150] Train corr: 0.2511, Valid corr: 0.1557, Test corr: 0.0912\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [100/150] Train corr: 0.2490, Valid corr: 0.1572, Test corr: 0.0910\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [105/150] Train corr: 0.2578, Valid corr: 0.1586, Test corr: 0.0899\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [110/150] Train corr: 0.2688, Valid corr: 0.1583, Test corr: 0.0862\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [115/150] Train corr: 0.2987, Valid corr: 0.1548, Test corr: 0.0863\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [120/150] Train corr: 0.2914, Valid corr: 0.1559, Test corr: 0.0840\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [125/150] Train corr: 0.2847, Valid corr: 0.1576, Test corr: 0.0893\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [130/150] Train corr: 0.2904, Valid corr: 0.1581, Test corr: 0.0882\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [135/150] Train corr: 0.3056, Valid corr: 0.1590, Test corr: 0.0870\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [140/150] Train corr: 0.2963, Valid corr: 0.1590, Test corr: 0.0870\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [145/150] Train corr: 0.2941, Valid corr: 0.1579, Test corr: 0.0858\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [150/150] Train corr: 0.3091, Valid corr: 0.1575, Test corr: 0.0852\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Outer Fold [12/18] train corr: 0.3091, valid corr: 0.1575, test corr: 0.0852\n",
      "Execution Time for Fold: 49.22 mins\n",
      "\n",
      "=================================== Outer Fold [13/18] ===================================\n",
      "Epoch [5/150] Train corr: 0.0347, Valid corr: 0.1399, Test corr: 0.0204\n",
      "Layer 1: [0.4832/0.9000] Layer 2: [0.3069/0.3000] Layer 3: [0.3896/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [10/150] Train corr: 0.0211, Valid corr: 0.1739, Test corr: 0.0626\n",
      "Layer 1: [0.7367/0.9000] Layer 2: [0.3067/0.3000] Layer 3: [0.5025/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [15/150] Train corr: 0.0267, Valid corr: 0.1135, Test corr: 0.0131\n",
      "Layer 1: [0.8123/0.9000] Layer 2: [0.3065/0.3000] Layer 3: [0.5023/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [20/150] Train corr: 0.0336, Valid corr: 0.0470, Test corr: -0.0385\n",
      "Layer 1: [0.6000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5022/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [25/150] Train corr: 0.0399, Valid corr: 0.1430, Test corr: 0.0518\n",
      "Layer 1: [0.7248/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [30/150] Train corr: 0.0882, Valid corr: 0.1556, Test corr: 0.0705\n",
      "Layer 1: [0.8052/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 3.12e-6\n",
      "Epoch [35/150] Train corr: 0.0864, Valid corr: 0.1900, Test corr: 0.0456\n",
      "Layer 1: [0.8690/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [40/150] Train corr: 0.1127, Valid corr: 0.1712, Test corr: 0.0413\n",
      "Layer 1: [0.8667/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [45/150] Train corr: 0.1412, Valid corr: 0.1796, Test corr: 0.0652\n",
      "Layer 1: [0.9004/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [50/150] Train corr: 0.1456, Valid corr: 0.1838, Test corr: 0.0510\n",
      "Layer 1: [0.8988/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [55/150] Train corr: 0.1568, Valid corr: 0.1890, Test corr: 0.0623\n",
      "Layer 1: [0.8997/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [60/150] Train corr: 0.1883, Valid corr: 0.1926, Test corr: 0.0421\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [65/150] Train corr: 0.2120, Valid corr: 0.1938, Test corr: 0.0406\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [70/150] Train corr: 0.1815, Valid corr: 0.1983, Test corr: 0.0391\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [75/150] Train corr: 0.1989, Valid corr: 0.2007, Test corr: 0.0361\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [80/150] Train corr: 0.2087, Valid corr: 0.2000, Test corr: 0.0351\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [85/150] Train corr: 0.2197, Valid corr: 0.2004, Test corr: 0.0315\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [90/150] Train corr: 0.2207, Valid corr: 0.2004, Test corr: 0.0318\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [95/150] Train corr: 0.2167, Valid corr: 0.2010, Test corr: 0.0327\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [100/150] Train corr: 0.2486, Valid corr: 0.2030, Test corr: 0.0308\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [105/150] Train corr: 0.2269, Valid corr: 0.2053, Test corr: 0.0275\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [110/150] Train corr: 0.2248, Valid corr: 0.2052, Test corr: 0.0321\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [115/150] Train corr: 0.2454, Valid corr: 0.2047, Test corr: 0.0274\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [120/150] Train corr: 0.2372, Valid corr: 0.2075, Test corr: 0.0296\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [125/150] Train corr: 0.2552, Valid corr: 0.2079, Test corr: 0.0289\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [130/150] Train corr: 0.2552, Valid corr: 0.2063, Test corr: 0.0304\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [135/150] Train corr: 0.2497, Valid corr: 0.2077, Test corr: 0.0275\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [140/150] Train corr: 0.2710, Valid corr: 0.2082, Test corr: 0.0292\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [145/150] Train corr: 0.2819, Valid corr: 0.2107, Test corr: 0.0286\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [150/150] Train corr: 0.2718, Valid corr: 0.2078, Test corr: 0.0308\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5018/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Outer Fold [13/18] train corr: 0.2718, valid corr: 0.2078, test corr: 0.0308\n",
      "Execution Time for Fold: 57.77 mins\n",
      "\n",
      "=================================== Outer Fold [14/18] ===================================\n",
      "Epoch [5/150] Train corr: 0.0084, Valid corr: 0.1280, Test corr: 0.1577\n",
      "Layer 1: [0.4832/0.9000] Layer 2: [0.3067/0.3000] Layer 3: [0.3897/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [10/150] Train corr: 0.0155, Valid corr: 0.0712, Test corr: 0.1242\n",
      "Layer 1: [0.7362/0.9000] Layer 2: [0.3065/0.3000] Layer 3: [0.5028/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [15/150] Train corr: 0.0220, Valid corr: 0.1065, Test corr: 0.1666\n",
      "Layer 1: [0.8129/0.9000] Layer 2: [0.3064/0.3000] Layer 3: [0.5026/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [20/150] Train corr: 0.0314, Valid corr: 0.1213, Test corr: 0.1058\n",
      "Layer 1: [0.6151/0.9000] Layer 2: [0.3063/0.3000] Layer 3: [0.5024/0.5000] \n",
      "Current learning rate: 5.00e-5\n",
      "Epoch [25/150] Train corr: 0.0225, Valid corr: 0.1206, Test corr: 0.1509\n",
      "Layer 1: [0.6756/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.25e-5\n",
      "Epoch [30/150] Train corr: 0.0700, Valid corr: 0.1773, Test corr: 0.2219\n",
      "Layer 1: [0.7403/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 3.12e-6\n",
      "Epoch [35/150] Train corr: 0.1046, Valid corr: 0.1409, Test corr: 0.1704\n",
      "Layer 1: [0.8265/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [40/150] Train corr: 0.0879, Valid corr: 0.1720, Test corr: 0.1263\n",
      "Layer 1: [0.8187/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 7.81e-7\n",
      "Epoch [45/150] Train corr: 0.1109, Valid corr: 0.1489, Test corr: 0.0908\n",
      "Layer 1: [0.8800/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5020/0.5000] \n",
      "Current learning rate: 1.95e-7\n",
      "Epoch [50/150] Train corr: 0.1337, Valid corr: 0.1676, Test corr: 0.1325\n",
      "Layer 1: [0.8787/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.95e-7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/150] Train corr: 0.1576, Valid corr: 0.1779, Test corr: 0.1478\n",
      "Layer 1: [0.9006/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 4.88e-8\n",
      "Epoch [60/150] Train corr: 0.2017, Valid corr: 0.1823, Test corr: 0.1469\n",
      "Layer 1: [0.9002/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [65/150] Train corr: 0.1905, Valid corr: 0.1835, Test corr: 0.1474\n",
      "Layer 1: [0.9003/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [70/150] Train corr: 0.2123, Valid corr: 0.1813, Test corr: 0.1427\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [75/150] Train corr: 0.2234, Valid corr: 0.1781, Test corr: 0.1297\n",
      "Layer 1: [0.9004/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [80/150] Train corr: 0.2341, Valid corr: 0.1770, Test corr: 0.1285\n",
      "Layer 1: [0.8998/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [85/150] Train corr: 0.2080, Valid corr: 0.1743, Test corr: 0.1229\n",
      "Layer 1: [0.9003/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [90/150] Train corr: 0.2409, Valid corr: 0.1726, Test corr: 0.1186\n",
      "Layer 1: [0.9004/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [95/150] Train corr: 0.2384, Valid corr: 0.1717, Test corr: 0.1089\n",
      "Layer 1: [0.9001/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [100/150] Train corr: 0.2401, Valid corr: 0.1637, Test corr: 0.1023\n",
      "Layer 1: [0.9000/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [105/150] Train corr: 0.2374, Valid corr: 0.1641, Test corr: 0.0912\n",
      "Layer 1: [0.8998/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [110/150] Train corr: 0.2810, Valid corr: 0.1648, Test corr: 0.0915\n",
      "Layer 1: [0.8997/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [115/150] Train corr: 0.2705, Valid corr: 0.1664, Test corr: 0.0900\n",
      "Layer 1: [0.8999/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [120/150] Train corr: 0.2717, Valid corr: 0.1627, Test corr: 0.0879\n",
      "Layer 1: [0.9004/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n",
      "Epoch [125/150] Train corr: 0.2753, Valid corr: 0.1658, Test corr: 0.0776\n",
      "Layer 1: [0.9005/0.9000] Layer 2: [0.3062/0.3000] Layer 3: [0.5021/0.5000] \n",
      "Current learning rate: 1.22e-8\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 12] Cannot allocate memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-31590cc6769b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn_outer_cv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouter_n_splits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     train_corr, valid_corr, test_corr = run_fold(\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mn_outer_cv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_outer_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp_output_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     )\n\u001b[1;32m     34\u001b[0m     \u001b[0mcorr_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_corr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_corr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-8ee1b196da39>\u001b[0m in \u001b[0;36mrun_fold\u001b[0;34m(n_outer_cv, output_folder, params)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouter_train_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_reg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m             \u001b[0mhsp_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhsp_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtg_hsp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         )\n\u001b[1;32m     92\u001b[0m         valid_clf_loss, valid_reg_loss, valid_acc, valid_corr = valid(\n",
      "\u001b[0;32m<ipython-input-30-659f45ddeaa7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epoch, train_loader, optimizer, criterion_clf, criterion_reg, hsp_val, beta_val, hsp_list, beta_list, tg_hsp)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0my_train_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf-gpu/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf-gpu/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_SingleProcessDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf-gpu/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    799\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf-gpu/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf-gpu/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf-gpu/lib/python3.6/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_fork\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mSpawnProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf-gpu/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush_std_streams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tf-gpu/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mparent_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 12] Cannot allocate memory"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    \"act_func\": \"elu\",\n",
    "    \"optimizer\": \"nag\",\n",
    "    \"extr_hidden\": 1024, \n",
    "    \"pred_hidden\": 1024,\n",
    "    \"disc_hidden\": 1024,\n",
    "    \"dropout_rate\": 0.9,\n",
    "    \"dropout_reg\": 0.95,\n",
    "    \"learning_rate\": 5e-05,\n",
    "    \"batch_size\": 32,\n",
    "    \"lambda_\": 1.25,\n",
    "    \"epochs\": 150,\n",
    "    \"swa_start\": 75\n",
    "}\n",
    "\n",
    "temp_param = \"arc_{}_{}_{}\".format(\n",
    "    params[\"extr_hidden\"], params[\"disc_hidden\"], params[\"pred_hidden\"],\n",
    "    params[\"learning_rate\"], params[\"batch_size\"], \n",
    "    params[\"dropout_rate\"], params[\"dropout_reg\"],\n",
    "    params[\"lambda_\"], params[\"epochs\"]\n",
    ")\n",
    "\n",
    "temp_output_folder = os.path.join(output_folder, temp_param)\n",
    "\n",
    "code_start_time = time.time()\n",
    "corr_list = []\n",
    "\n",
    "for n_outer_cv in range(outer_n_splits):\n",
    "    train_corr, valid_corr, test_corr = run_fold(\n",
    "        n_outer_cv=n_outer_cv, output_folder=temp_output_folder, params=params\n",
    "    )\n",
    "    corr_list.append([train_corr, valid_corr, test_corr])\n",
    "    corr_df = pd.DataFrame(corr_list, columns=[\"train\", \"valid\", \"test\"])\n",
    "    corr_df.to_csv(\"{}/corr_df.csv\".format(temp_output_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time for the training: 53.69 hours\n"
     ]
    }
   ],
   "source": [
    "code_tot_time = time.time() - code_start_time \n",
    "print(\"Execution Time for the training: {:.2f} hours\".format(code_tot_time / 60 / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSOCV results --> valid corr: 0.1588, test corr: 0.1378\n"
     ]
    }
   ],
   "source": [
    "new_corr_list = []\n",
    "for i in range(len(corr_df)):\n",
    "    new_corr_list.append([\n",
    "        corr_df.iloc[i, 0].detach().cpu().numpy(),\n",
    "        corr_df.iloc[i, 1].detach().cpu().numpy(),\n",
    "        corr_df.iloc[i, 2].detach().cpu().numpy(),\n",
    "    ])\n",
    "new_corr_df = pd.DataFrame(new_corr_list, columns=[\"train\", \"valid\", \"test\"])\n",
    "print(\"LOSOCV results --> valid corr: {:.4f}, test corr: {:.4f}\"\n",
    "      .format(new_corr_df.valid.values.mean(), new_corr_df.test.values.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
